<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Asymptotic analysis and consistency &#8212; econometrics 0.1 documentation</title>
    
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="econometrics 0.1 documentation" href="../index.html" />
    <link rel="prev" title="Estimators as Statistical Decision Functions" href="../Notebook_01_Wald/Statistical_decision_functions.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="asymptotic-analysis-and-consistency">
<h1>Asymptotic analysis and consistency<a class="headerlink" href="#asymptotic-analysis-and-consistency" title="Permalink to this headline">¶</a></h1>
<p><strong>Date: November 2016</strong></p>
<p>In this notebook we discuss the consistency of decision rules. After a
general introduction we restrict attention to empirical loss minimizing
decision rules, establish sufficient conditions for consistency to hold
and discuss the rate at which the convergence takes place.</p>
<ul class="simple">
<li>Consistency of a decision rule states that the corresponding
empirical loss converges to the true loss in probability as the
sample size grows to infinity.</li>
<li>In the case of empirical loss minimization, consistency is tightly
linked to the uniform law of large numbers that we present in detail.</li>
<li>We introduce tail and concentration bounds to establish the uniform
law of large numbers and analyze the (non-asymptotic) rate at which
the convergence takes place.</li>
</ul>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.patches</span> <span class="k">import</span> <span class="n">ConnectionPatch</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="k">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">FloatSlider</span>

<span class="n">colors</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;Blues&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="n">usetex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">&#39;font&#39;</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="s1">&#39;serif&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Knowing the true mechanism <span class="math">\(P\)</span>, while one faces a decision problem
<span class="math">\((\mathcal{H},L,\mathcal{A})\)</span>, naturally leads to the best in
class action <span class="math">\(a^{*}_{L, P, \mathcal{A}}\)</span> as the optimal action
within the prespecified class <span class="math">\(\mathcal{A}\)</span>. Although in reality
we do not know <span class="math">\(P\)</span>, it is illuminating to consider this case first
as we naturally require that at least with a sufficiently large (in
fact, infinite) sample, the true mechanism&#8212;from a statistical point of
view&#8212;can be revealed. The property that guarantees this outcome is the
ergodicity of the assumed statistical models.</p>
<p>This argument suggests to use the best in class action,
<span class="math">\(a^{*}_{L, P, \mathcal{A}}\)</span>, as the optimal large sample solution.
In other words, from any sensible decision rule a minimal property to
require is that as the sample size grows, the distribution of the
decision rule converges to a degenerate distribution concentrated at the
point <span class="math">\(a^{*}_{L, P, \mathcal{A}}\)</span>.</p>
<p>As an example, consider again the MLE estimator in the coin tossing
problem for which the above property is satisfied. The following figure
represents how the different confidence bands associated with the action
distribution evolve as the sample size goes to infinity. Apparently, for
sufficiently large sample sizes, the confidence bands concentrate around
the true value, which, due to the correct specification, is equivalent
with the best in class action.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">theta0</span> <span class="o">=</span> <span class="o">.</span><span class="mi">7</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="n">sample_sizes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="n">perc5</span> <span class="o">=</span> <span class="p">[</span><span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">theta0</span><span class="p">)</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="o">.</span><span class="mi">05</span><span class="p">)</span><span class="o">/</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">]</span>
<span class="n">perc25</span> <span class="o">=</span> <span class="p">[</span><span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">theta0</span><span class="p">)</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="o">.</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">]</span>
<span class="n">perc75</span> <span class="o">=</span> <span class="p">[</span><span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">theta0</span><span class="p">)</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="o">.</span><span class="mi">8</span><span class="p">)</span><span class="o">/</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">]</span>
<span class="n">perc95</span> <span class="o">=</span> <span class="p">[</span><span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">theta0</span><span class="p">)</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="o">.</span><span class="mi">95</span><span class="p">)</span><span class="o">/</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Evolution of the maximumum likelihood estimator&#39;s distribution in the coin tossing example&quot;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">17</span><span class="p">)</span>
<span class="n">small_sample</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">[:</span><span class="n">small_sample</span><span class="p">],</span> <span class="n">perc5</span><span class="p">[:</span><span class="n">small_sample</span><span class="p">],</span> <span class="n">perc25</span><span class="p">[:</span><span class="n">small_sample</span><span class="p">],</span>
                   <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">alpha</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;$5\%$-$95\%$ percentiles&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">[:</span><span class="n">small_sample</span><span class="p">],</span> <span class="n">perc25</span><span class="p">[:</span><span class="n">small_sample</span><span class="p">],</span> <span class="n">perc75</span><span class="p">[:</span><span class="n">small_sample</span><span class="p">],</span>
                   <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">alpha</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;$20\%$-$80\%$ percentiles&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">[:</span><span class="n">small_sample</span><span class="p">],</span> <span class="n">perc75</span><span class="p">[:</span><span class="n">small_sample</span><span class="p">],</span> <span class="n">perc95</span><span class="p">[:</span><span class="n">small_sample</span><span class="p">],</span>
                   <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">alpha</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">r&#39;True $\theta_0$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">300</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mf">0.4</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sample size&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">,</span> <span class="n">perc5</span><span class="p">,</span> <span class="n">perc25</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">alpha</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;$5\%$-$95\%$ percentiles&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">,</span> <span class="n">perc25</span><span class="p">,</span> <span class="n">perc75</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">alpha</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;$20\%$-$80\%$ percentiles&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">,</span> <span class="n">perc75</span><span class="p">,</span> <span class="n">perc95</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">alpha</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">r&#39;True $\theta_0$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mf">0.4</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sample size&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">r&#39;True $\theta_0$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../_images/Asymptotic_analysis_consistency_3_0.png" src="../_images/Asymptotic_analysis_consistency_3_0.png" />
<p>This property of the decision rule is called <strong>consistency</strong>. One of the
main objectives of this notebook is to investigate the conditions under
which it can be established.</p>
<p>Given that asymptotically the true <span class="math">\(P\)</span> can be learned, a natural
question to ask is why not to set <span class="math">\(\mathcal{A}\)</span> large enough so as
to guarantee <span class="math">\(\gamma(P)\in\mathcal{A}\)</span>, i.e. a correctly specified
model. We will see the sense in which requiring consisteny ties our
hands in terms of the &#8220;size&#8221; of <span class="math">\(\mathcal{A}\)</span>.</p>
<p>Although it is hard to provide generally applicable sufficient
conditions for consistency, roughly speaking, we can identify two big
classes of decision rules for which powerful results are available.</p>
<ul class="simple">
<li>Bayes decision rules</li>
<li>Frequentist decision rules building on the empirical distribution
<span class="math">\(P_n\)</span>, where</li>
</ul>
<div class="math">
\[P_n(z) : = \frac{1}{n}\sum_{i=1}^{n} \mathbf{1}\{Z_i \leq z\}\]</div>
<p>and <span class="math">\(\mathbf{1}\{ A \}\)</span> is the indicator function of the set
<span class="math">\(A\)</span>.</p>
<p>In this notebook we will focus exclusively on the latter approach
considering decision rules which assign actions based on minimizing the
empirical analog of the population loss. Hence, the procedure is
labelled by the name: <em>empirical loss minimization</em> or <em>analog
estimation</em>.</p>
</div>
<div class="section" id="consistency-of-decision-rules">
<h2>Consistency of decision rules<a class="headerlink" href="#consistency-of-decision-rules" title="Permalink to this headline">¶</a></h2>
<p>To start out our asymptotic inquiry, we first define consistency in a
more precise manner than we did before. There are two, slightly
different notions depending on the tractability and objectives of the
decision problem at hand.</p>
<ul>
<li><dl class="first docutils">
<dt><strong>Consistency in terms of the loss function:</strong> a decision rule,</dt>
<dd><p class="first last"><span class="math">\(d: \mathcal{S} \mapsto \mathcal{A}\)</span>, is consistent in terms of
the loss function relative to <span class="math">\((\mathcal{H}, \mathcal{A})\)</span>,
if for all <span class="math">\(P \in \mathcal{H}\)</span> the following holds</p>
</dd>
</dl>
</li>
</ul>
<blockquote>
<div><div class="math">
\[L(P, d(Z^n)) \ \ \underset{n \to \infty}{\overset{\text{P}}{\rightarrow}} \ \ \inf_{a \in \mathcal{A}}L(P, a) = L\left(P, a^{*}_{L, P,\mathcal{A}}\right)\]</div>
</div></blockquote>
<ul>
<li><dl class="first docutils">
<dt><strong>Consistency in terms of the action:</strong> a decision rule,</dt>
<dd><p class="first"><span class="math">\(d: \mathcal{S} \mapsto \mathcal{A}\)</span>, is consistent in terms of
the action relative to <span class="math">\((\mathcal{H}, \mathcal{A})\)</span>, if for
all <span class="math">\(P \in \mathcal{H}\)</span> the following holds</p>
<div class="last math">
\[P\left\{z^{\infty} \big| \lim_{n \to \infty} m\left(d(z_n), a^*_{L, P, \mathcal{A}}\right) &gt; \epsilon\right\} = 0 \quad \text{for} \quad \forall\epsilon&gt;0,\]</div>
</dd>
</dl>
</li>
</ul>
<p>where <span class="math">\(m(\cdot, \cdot)\)</span> is a metric on the action space
<span class="math">\(\mathcal{A}\)</span>. The necessary condition&#8212;in the case of analog
estimators&#8212;for this notion of consistency is the identifiability of
<span class="math">\(a^*_{L, P, \mathcal{A}}\)</span> that we define as follows:</p>
<p><strong>Identification:</strong> <span class="math">\(a^*_{L, P, \mathcal{A}}\)</span> is identified
relative to <span class="math">\((P, \mathcal{A})\)</span> if <span class="math">\(a^*_{L, P, \mathcal{A}}\)</span>
is the unique minimizer of <span class="math">\(L(P, \cdot)\)</span> over <span class="math">\(\mathcal{A}\)</span>.</p>
<p>Under identification, the two notions are equivalent. As the above
definitions suggest, however, the former is more general to the extent
that it also allows for partially identified statistical models. Unless
otherwise noted, we will work with consistency in terms of the loss
function and call it simply <em>consistency</em>.</p>
<p>In the above definitions, the set of assumed statistical models
<span class="math">\(\mathcal{H}\)</span> plays a key role: it outlines the set of
distributions under which the particular notion of convergence is
required. Ideally, we want convergence under the true distribution,
however, because we do not know <span class="math">\(P\)</span>, the &#8220;robust&#8221; approach is to
be as agnostic as possible regarding <span class="math">\(\mathcal{H}\)</span>. The central
insight of statstical decision theory is to highlight that this approach
is not limitless: one needs to find a balance between
<span class="math">\(\mathcal{H}\)</span> and <span class="math">\(\mathcal{A}\)</span> to obtain decision rules
with good properties.</p>
<p>Consistency has strong implications for the risk functional in large
samples: the degenerate limiting distribution of the decision rule
implies that asymptotically the variability of the decision rule
vanishes and so</p>
<div class="math">
\[R_{\infty}(P,d) \overset{a.s.}{=} L\left(P, a^{*}_{L, P,\mathcal{A}}\right).\]</div>
</div>
<div class="section" id="uniform-law-of-large-numbers">
<h2>Uniform law of large numbers<a class="headerlink" href="#uniform-law-of-large-numbers" title="Permalink to this headline">¶</a></h2>
<p>As mentioned before, in this notebook we are focusing on decision rules
arising from empirical loss minimization. The basic idea behind this
approach is to utilize the consistency definitions directly, but instead
of using the quantity <span class="math">\(L(P, a)\)</span> that we cannot actually evaluate,
subsitute the empirical distribution into the loss function and work
with the empricial loss <span class="math">\(L(P_n, a)\)</span> instead.</p>
<p><em>Remark:</em> The empirical loss is not defined, if <span class="math">\(P_n\)</span> is not in
the domain of <span class="math">\(L\)</span>&#8212;as is the case, for example, with
non-parametric density estimation. Then, one has to either extend the
domain of <span class="math">\(L\)</span> or map <span class="math">\(P_n\)</span> to the original domain of the
loss function <span class="math">\(L\)</span>. An excellent discussion can be found in Manski
(1988).</p>
<p>Given that the loss function is continuous in its first argument, the
law of large numbers (LLN), accompanied with the continuous mapping
theorem, implies</p>
<div class="math">
\[L(P_n, a) \quad  \underset{n \to \infty}{\to} \quad L(P, a)\]</div>
<p>that is, for a <em>fixed action</em> <span class="math">\(a\)</span>, the emprical loss converges to
the true loss as the sample size goes to infinity. Notice, however, that
consistency</p>
<ul class="simple">
<li>is not a property of a given action, but a whole decision rule, more
precisely, it is about the convergence of the sequence
<span class="math">\(\{d(z^n)\}_{n\geq 1} \subseteq \mathcal{A}\)</span></li>
<li>requires convergence to the <em>minimum</em> loss (within
<span class="math">\(\mathcal{A}\)</span>) that we ought to take into account while
generating decision rules</li>
</ul>
<p>In the spirit of the analogy principle, decision rules minimizing the
<em>empirical loss</em> can be written as follows</p>
<div class="math">
\[d(z^n) := \arg\min_{a\in\mathcal{A}} L(P_n, a),\]</div>
<p>where the dependence on the sample is embodied by the empirical
distribution <span class="math">\(P_n\)</span>. The heuristic idea is that because
<span class="math">\(d(z^n)\)</span> minimizes <span class="math">\(L(P_n, \cdot)\)</span>, and <span class="math">\(P_n\)</span>
converges to <span class="math">\(P\)</span>, then &#8220;ideally&#8221; <span class="math">\(d(z^n)\)</span> should go to
<span class="math">\(a^{*}_{L, P, \mathcal{A}}\)</span>, the minimizer of <span class="math">\(L(P, a)\)</span>, as
the sample size grows.</p>
<p>What do we need to ensure this argument to hold? First, notice that the
standard law of large numbers is inadequate for this purpose. In order
to illustrate this, we turn now to a concept closely related to
empirical loss minimization: generalization.</p>
<div class="section" id="generalization">
<h3>Generalization<a class="headerlink" href="#generalization" title="Permalink to this headline">¶</a></h3>
<p>Using the empirical loss as a substitute for the true loss while
determining the decision rule, a critical question is how much error do
we introduce with this approximation. The object of interest regarding
this question is the <em>excess loss</em> for a given action <span class="math">\(a\)</span> and
sample size <span class="math">\(n\)</span></p>
<div class="math">
\[\left| L(P_n, a) - L(P, a) \right|.\]</div>
<p>We say that the action <span class="math">\(a\)</span> <strong>generalizes</strong> well from a given
sample <span class="math">\(z^n\)</span>, if the corresponding excess loss is small. Taking
the absolute value is important: <span class="math">\(L(P_n, a)\)</span> can easily be smaller
than <span class="math">\(L(P, a)\)</span> as it is in the case of overfitting.</p>
<p>Due to <span class="math">\(P_n\)</span>&#8216;s dependence on the particular sample, however, the
excess loss (for a given <span class="math">\(a\)</span>) is a random variable. It might be
small for a given sample <span class="math">\(z^n\)</span>, but what we really need in order
to justify the method of empricial loss minimization is that the excess
loss is small for &#8220;most samples&#8221;, or more precisely</p>
<div class="math">
\[P\left(z^n : \left| L(P_n, a) - L(P, a) \right| &gt; \delta \right) \quad \text{is small.}\]</div>
<p>Since <span class="math">\(L\)</span> is continuous in its first argument and
<span class="math">\(P_n \to P\)</span>, this so called <strong>tail probability</strong> converges to zero
for any fixed <span class="math">\(a\in\mathcal{A}\)</span>. The figure below displays such
tail probabilities for the MLE estimator in the coin tossing example
(with quadratic loss) for different actions as functions of the sample
size.</p>
<p>One can see that even though the tail probabilities converge to
<span class="math">\(0\)</span> for all <span class="math">\(a\in\mathcal{A}\)</span>, the <em>rate of convergence</em>
depends on the particular <span class="math">\(a\)</span>. In other words,
<span class="math">\(\forall \varepsilon&gt;0\)</span> and <span class="math">\(\forall a\in\mathcal{A}\)</span>, there
is an <em>action dependent</em> minimum sample size
<span class="math">\(N(a, \varepsilon, \delta)\)</span>, that we need in order to guarantee
that the corresponding tail probability falls below <span class="math">\(\varepsilon\)</span>.</p>
<p>Suppose now that we have a decision rule and look at its induced
sequence of actions <span class="math">\(\{a_n\}_{n\geq 1}\)</span> for a fixed realization
<span class="math">\(z^{\infty}\)</span>. Again, for consistency, we need the associated
sequence of tail probabilities converging to zero for &#8220;most
<span class="math">\(z^{\infty}\)</span>&#8221;.</p>
<ul>
<li><p class="first">When the set <span class="math">\(\mathcal{A}\)</span> has finitely many elements, this is
not a problem: we can simply define</p>
<div class="math">
\[N(\varepsilon, \delta) : = \max_{a\in\mathcal{A}} N(a, \varepsilon, \delta)\]</div>
<p>and observe that for all <span class="math">\(n\geq N(\varepsilon, \delta)\)</span>, the
tail probabilities are smaller than <span class="math">\(\varepsilon\)</span>,
<span class="math">\(\forall a\in \{a_n\}_{n\geq 1} \subseteq \mathcal{A}\)</span>.</p>
</li>
<li><p class="first">However, because the critical sample size
<span class="math">\(N(a, \varepsilon, \delta)\)</span> depends on the action, if
<span class="math">\(\mathcal{A}\)</span> is &#8220;too big&#8221;, it is possible that there is no
<span class="math">\(n\)</span> so that <span class="math">\(n &gt; N(a_n, \varepsilon, \delta)\)</span>, hence
<span class="math">\(L(P_n, a_n)\)</span> may never approach <span class="math">\(L(P, a_n)\)</span>.</p>
</li>
</ul>
<p>A possible approach to avoid this complication is to require the
existence of an integer <span class="math">\(N(\varepsilon, \delta)\)</span> independent of
<span class="math">\(a\)</span>, such that for all sample sizes larger than
<span class="math">\(N(\epsilon, \delta)\)</span></p>
<div class="math">
\[\lim_{n\to \infty} P\left\{\sup_{a \in\mathcal{A}} \left| L(P_n, a) - L(P, a) \right| &gt; \delta\right\} = 0.\]</div>
<p>This notion is called the <strong>uniform law of large numbers</strong> referring to
the fact that the convergence is guaranteed simultaneously for all
actions in <span class="math">\(\mathcal{A}\)</span>.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">relative_entropy</span><span class="p">(</span><span class="n">prob0</span><span class="p">,</span> <span class="n">theta0</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">prob0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta0</span><span class="o">/</span><span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">prob0</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta0</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">loss_distribution</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">aa</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">true_dist</span><span class="p">,</span> <span class="n">upper</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Uses the formula for the change of discrete random variable. It takes care of the</span>
<span class="sd">    fact that the loss is not monotone.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">10</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">theta0</span> <span class="o">=</span> <span class="n">true_dist</span><span class="o">.</span><span class="n">args</span>

    <span class="k">if</span> <span class="n">loss_func</span> <span class="o">==</span> <span class="s1">&#39;relative_entropy&#39;</span><span class="p">:</span>
        <span class="n">frac</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">bisect</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">relative_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta0</span><span class="p">,</span> <span class="n">aa</span><span class="p">)</span> <span class="o">-</span> <span class="n">l</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">eps</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">eps</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">loss_func</span> <span class="o">==</span> <span class="s1">&#39;quadratic&#39;</span><span class="p">:</span>
        <span class="n">frac</span> <span class="o">=</span> <span class="p">(</span><span class="n">l</span> <span class="o">-</span> <span class="n">aa</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">aa</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">aa</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">aa</span> <span class="o">&lt;</span> <span class="o">.</span><span class="mi">5</span><span class="p">:</span> <span class="c1">#theta0:</span>
        <span class="k">if</span> <span class="n">upper</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">true_dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">frac</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">true_dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">frac</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">upper</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">true_dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">frac</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">true_dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">frac</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">prob</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">true_dist</span><span class="p">):</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">theta0</span> <span class="o">=</span> <span class="n">true_dist</span><span class="o">.</span><span class="n">args</span>
    <span class="c1">#true = relative_entropy(theta0, theta0, a)</span>
    <span class="n">true</span> <span class="o">=</span> <span class="n">theta0</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta0</span><span class="p">)</span> <span class="o">*</span> <span class="n">a</span><span class="o">**</span><span class="mi">2</span>

    <span class="n">upper</span> <span class="o">=</span> <span class="n">true</span> <span class="o">+</span> <span class="n">l</span>
    <span class="n">lower</span> <span class="o">=</span> <span class="n">true</span> <span class="o">-</span> <span class="n">l</span>

    <span class="n">first</span> <span class="o">=</span> <span class="n">loss_distribution</span><span class="p">(</span><span class="n">upper</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="s2">&quot;quadratic&quot;</span><span class="p">,</span> <span class="n">true_dist</span><span class="p">,</span> <span class="n">upper</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">second</span> <span class="o">=</span> <span class="n">loss_distribution</span><span class="p">(</span><span class="n">lower</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="s2">&quot;quadratic&quot;</span><span class="p">,</span> <span class="n">true_dist</span><span class="p">,</span> <span class="n">upper</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">first</span> <span class="o">+</span> <span class="n">second</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">sample_size</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">action_grid1</span> <span class="o">=</span> <span class="p">[</span><span class="o">.</span><span class="mi">01</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">4</span><span class="p">,</span> <span class="o">.</span><span class="mi">6</span><span class="p">]</span>
<span class="n">action_grid2</span> <span class="o">=</span> <span class="p">[</span><span class="o">.</span><span class="mi">99</span><span class="p">,</span> <span class="o">.</span><span class="mi">95</span><span class="p">,</span> <span class="o">.</span><span class="mi">9</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">]</span>
<span class="n">delta</span> <span class="o">=</span> <span class="o">.</span><span class="mi">02</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Tail probabilities for the MLE estimator (evaluated with quadratic loss) in the coin tossing example </span><span class="se">\n</span><span class="s1">&#39;</span> <span class="o">+</span>
             <span class="s2">r&quot;$\delta = </span><span class="si">{:1.2f}</span><span class="s2">$,  $\theta_0 = </span><span class="si">{:1.2f}</span><span class="s2">$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">theta0</span><span class="p">),</span>
            <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">18</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="mf">1.05</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">action_grid1</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sample_size</span><span class="p">,</span> <span class="p">[</span><span class="n">prob</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">theta0</span><span class="p">))</span> <span class="k">for</span> <span class="n">nn</span> <span class="ow">in</span> <span class="n">sample_size</span><span class="p">],</span>
               <span class="n">label</span> <span class="o">=</span> <span class="s1">r&#39;a = </span><span class="si">{:1.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">2</span> <span class="o">+</span> <span class="n">i</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sample size&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">action_grid2</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sample_size</span><span class="p">,</span> <span class="p">[</span><span class="n">prob</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">theta0</span><span class="p">))</span> <span class="k">for</span> <span class="n">nn</span> <span class="ow">in</span> <span class="n">sample_size</span><span class="p">],</span>
               <span class="n">label</span> <span class="o">=</span> <span class="s1">r&#39;a = </span><span class="si">{:1.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">2</span> <span class="o">+</span> <span class="n">i</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sample size&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../_images/Asymptotic_analysis_consistency_9_0.png" src="../_images/Asymptotic_analysis_consistency_9_0.png" />
<div class="section" id="a-special-case-plug-in-estimators">
<h4>A special case (plug-in estimators)<a class="headerlink" href="#a-special-case-plug-in-estimators" title="Permalink to this headline">¶</a></h4>
<p>There are special cases where we do not need to worry about uniformity.
When the true loss function has a minimizer
<span class="math">\(a^{*}_{L, P, \mathcal{A}}\)</span> that admits a closed form in the sense
that it can be expressed as a continuous function of <span class="math">\(P\)</span>, the
empirical loss minimizer is simply a sample analog of
<span class="math">\(a^{*}_{L, P, \mathcal{A}}\)</span>. Effectively, there is no need to
minimize the emprical loss, so in spirit, it is as if we kept a
particular action fixed. In these cases the standard LLN is sufficient
to establish consistency.</p>
<ul class="simple">
<li><em>Sample mean:</em> Consider the quadratic loss function with the feature
<span class="math">\(\gamma(P) = E[Z]\)</span>, i.e. the loss is
<span class="math">\(L(P,a) = E[(a - E[Z])^2]\)</span>. Evidently, the minimizer of this
quantity is <span class="math">\(E[Z]\)</span>. Although we could proceed by minimizing the
empricial loss to derive the decision rule, we do not need to do
that, because we know up front that it is equal to the sample analog
of <span class="math">\(E[Z]\)</span>. The decision rule is the plug-in estimator
<span class="math">\(d(z^n) = \frac{1}{n}\sum_{i=1}^{n}z_i\)</span>.</li>
<li><em>OLS estimator:</em> Suppose that <span class="math">\(Z=(Y, X)\)</span> and consider the
quadratic loss <span class="math">\(L(P, a) = E[(Y-a)^2 | X]\)</span>, where
<span class="math">\(E[\cdot | X]\)</span> is the expectation operator conditioned on
<span class="math">\(X\)</span>. Assume that <span class="math">\((Y, X)\)</span> follows a multivariate normal
distribution, then the minimizer of <span class="math">\(L\)</span> is given by
<span class="math">\(a = E\left[(X'X)^{-1}\right]E[X'Y]\)</span>. Consequently, there is no
need for explicit minimization, we can use the least squares
estimator as a plug-in estimator for <span class="math">\(a\)</span>.</li>
</ul>
<p>More generally, as the empirical loss minimizer is itself a functional,
if the &#8220;argmin&#8221; functional can be shown to be continuous on the space of
empirical loss functions, then an application of the continuous mapping
theorem together with the Glivenko-Cantelli theorem (i.e.
<span class="math">\(\lVert P_n - P \rVert_{\infty} \to 0\)</span>) would yield consistency.
For reference, see van der Vaart (2000).</p>
</div>
</div>
</div>
<div class="section" id="uniform-law-of-large-numbers-and-consistency">
<h2>Uniform law of large numbers and consistency<a class="headerlink" href="#uniform-law-of-large-numbers-and-consistency" title="Permalink to this headline">¶</a></h2>
<p>In more general nonlinear models, however,
<span class="math">\(a^{*}_{L, P, \mathcal{A}}\)</span> has no analytical form, so we cannot
avoid minimizing the empirical loss in order to derive the decision
rule. In these cases, guaranteeing the validity of uniform convergence
becomes essential. In fact, it can be shown that consistency of any
decision rule that we derive by minimizing some empirical loss function,
is <em>equivalent with</em> the uniform LLN.</p>
<p><strong>Vapnik and Chernovenkis (1971)</strong>: Uniform convergence</p>
<div class="math">
\[\lim_{n\to \infty} P\left\{\sup_{a \in\mathcal{A}} \left| L(P_n, a) - L(P, a) \right| &lt; \varepsilon\right\}= 1\]</div>
<p>for all <span class="math">\(\varepsilon &gt;0\)</span> is <em>necessary and sufficient</em> for
consistency of decision rules arising from empirical loss minimization
over <span class="math">\(\mathcal{A}\)</span>.</p>
<p>Although this characterization of consistency is theoretically
intriguing, it is not all that useful in practice unless we find
easy-to-check conditions which ensure uniform convergence of empirical
loss over a particular function class <span class="math">\(\mathcal{A}\)</span>. As a
preparation to discuss these conditions, introduce first some notation</p>
<p>Almost all loss functions used in practice can be cast in the structure
taking the following form. There exist</p>
<ul class="simple">
<li><span class="math">\(l : \mathcal{A}\times Z \mapsto R^{m}\)</span> and</li>
<li>a <em>continuous</em> <span class="math">\(r : R^{m} \mapsto R_+\)</span> such that</li>
</ul>
<div class="math">
\[L(P, a) = r\left( \int_Z l(a, z)dP (z)\right)\]</div>
<p>Continuity of <span class="math">\(r\)</span> implies that in order to establish consistency
it is enough to investigate the properties of the limit</p>
<div class="math">
\[P\left( \sup_{a\in\mathcal{A}} \left| \int_Z l(a, z)dP_n (z) - \int_Z l(a, z)dP(z)\right| &gt; \varepsilon \right) \quad  \underset{n \to \infty}{\to} \quad  0 \quad\quad \forall \varepsilon &gt; 0\]</div>
<p>Formally, this requires that the class of functions
<span class="math">\(\left\{l(a, \cdot) : a \in \mathcal{A} \right\}\)</span> is a
<strong>Glivenko-Cantelli class</strong> for <span class="math">\(P\)</span>.</p>
<p><strong>Definition (Glivenko-Cantelli class):</strong> Let <span class="math">\(\mathcal{F}\)</span> be a class of integrable real-valued functions of the random variable <span class="math">\(Z\)</span> having distribution <span class="math">\(P\)</span>. Consider the random variable</p>
<div class="math">
\[\Vert P_n - P \Vert_{\mathcal{F}} : = \sup_{f\in\mathcal{F}} \left| \frac{1}{n}\sum_{i=1}^{n} f(z_i) - E[f] \right|                       |\]</div>
<p>We say that <span class="math">\(\mathcal{F}\)</span> is a Glivenko-Cantelli class for <span class="math">\(P\)</span> if <span class="math">\(\Vert P_n - P \Vert_{\mathcal{F}}\)</span> converges to zero in probability as <span class="math">\(n\to\infty\)</span>.</p>
<p>A useful decomposition highlights the importance of the uniform law of
large numbers in the case of empirical loss minimization. We would like
to know the difference between the true loss of our estimator and the
true loss of the best in class action. Denote the expectation operator
by <span class="math">\(\int_Z l(a, z)dP(z) =: P l_a\)</span>, the analog estimate as
<span class="math">\(\hat{a} = d(z^n)\)</span> and the best in class action as <span class="math">\(a^*\)</span>.</p>
<div class="math">
\[Pl_{\hat{a}} - Pl_{a^*} = (Pl_{\hat{a}} - P_n l_{\hat{a}}) + (P_n l_{\hat{a}} - P_n l_{a^*})  + (P_n l_{a^*} - P l_{a^*})\]</div>
<p>The last term on the right hand side is governed by the Law of Large
Numbers, the middle term is necessarily weakly positive by definition of
the empirical loss minimizing decision rule, and the first term is
governed by the Uniform Law of Large Numbers. Hence, if we want to
control the excess risk &#8211; the left hand side &#8211; then we necessarily
have to control the right hand side.</p>
<div class="section" id="sufficient-conditions">
<h3>Sufficient conditions<a class="headerlink" href="#sufficient-conditions" title="Permalink to this headline">¶</a></h3>
<p>Variables of the form <span class="math">\(\Vert P_n - P \Vert_{\mathcal{F}}\)</span> are
ubiquitos in statistics and econometrics and there are well-known
sufficient conditions that guarantee its convergence.</p>
<ul class="simple">
<li>For parametric estimation, probably the most widely used (at least in
econometrics) sufficient condition includes the following three
assumptions (or some form thereof). Suppose that the action space
<span class="math">\(\mathcal{A}\)</span> is indexed by a finite dimensional vector
<span class="math">\(\theta\in\Theta \subset \mathbb{R}^{p}\)</span>. If</li>
<li><span class="math">\(\Theta\)</span> is <em>compact</em></li>
<li><span class="math">\(l(\theta, z)\)</span> is a function such that <span class="math">\(l(\cdot, z)\)</span> is
<em>continuous</em> on <span class="math">\(\Theta\)</span> with probability one</li>
<li><span class="math">\(l(\theta, z)\)</span> is <em>dominated</em> by a function <span class="math">\(G(z)\)</span>, i.e.
<span class="math">\(|l(\theta, z)|\leq G(z)\)</span> for all <span class="math">\(\theta\in\Theta\)</span>, such
that <span class="math">\(E[G]&lt;\infty\)</span></li>
</ul>
<p>then
<span class="math">\(\mathcal{L} := \left\{l(a, \cdot) : a \in \mathcal{A} \right\}\)</span>
is a Glivenko-Cantelli class for <span class="math">\(P\)</span> and so the estimator that it
represents is consistent. Among others, these assumptions are the bases
for consistency of the (Quasi-)Maximum Likelihood (White, 1994) and the
Generalized Method of Moments estimators (Hansen, 1982).</p>
<ul class="simple">
<li>A somewhat different approach focuses on the &#8220;effective size&#8221; of the
class <span class="math">\(\left\{l(a, \cdot) : a \in \mathcal{A} \right\}\)</span> and
frames sufficient conditions in terms of particular complexity
measures such as the Vapnik-Chernovenkis dimension or Rademacher
complexity. The idea is to find tail bounds for the probability that
<span class="math">\(\Vert P_n - P \Vert_{\mathcal{L}}\)</span> deviates substantially
above the complexity of <span class="math">\(\mathcal{L}\)</span>. In the following section
we present the main ideas behind the derivation of non-asymptotic
tail bounds.</li>
</ul>
<p><strong>An interesting necessary condition:</strong> If <span class="math">\(\mathcal{L}\)</span> is a
collection of indicator functions and the data generating process is
assumed to be i.i.d., a necessary and sufficient condition for
distribution free uniform convergence is that the Vapnik-Chervonenkis
complexity of <span class="math">\(\mathcal{L}\)</span> is finite.</p>
</div>
</div>
<div class="section" id="non-asymptotic-bounds">
<h2>Non-asymptotic bounds<a class="headerlink" href="#non-asymptotic-bounds" title="Permalink to this headline">¶</a></h2>
<p>We saw that in order to establish consistency for analog estimators we
had to establish uniform convergence of the empirical loss to the true
loss over the entire action space. In this section, we present an
approach to the laws of large numbers (uniform or &#8220;standard&#8221;), which
builds on a finite sample perspective.</p>
<p>For a given finite sample size, we study the concentration of <em>empirical
random variables</em> around their means. We use the shorthand &#8220;empirical
random variable&#8221; to refer to the sample average of random variables
highlighting their connection to empirical processes. Ensuring that the
empirical random variable goes to a degenerate distribution on its
mean&#8212;&#8221;perfectly&#8221; concentrates around it&#8212;will establish the law of
large numbers.</p>
<p>Studying concentration and tail bounds for each finite sample size has
the advantage that we will get rates of convergence as a byproduct. The
rate of convergence will give us information regarding the minimum
sample size above which the asymptotic results are going to be good
approximations.</p>
<p>First, we study how the concentration measures work for a single random
variable. This is going to parallel the law of large numbers. Second, we
study concentration measures taking place uniformly over a class of
random variables. This is going to parallel the uniform law of large
numbers.</p>
<div class="section" id="tail-and-concentration-bounds">
<h3>Tail and Concentration Bounds<a class="headerlink" href="#tail-and-concentration-bounds" title="Permalink to this headline">¶</a></h3>
<p>Throughout the section we should keep in mind the characterization of
consistency in the case of analog estimators. This was equivalent to
uniform convergence which we repeat here</p>
<div class="math">
\[P\left( \sup_{a\in\mathcal{A}} \left| \int_Z l(a, z)dP_n (z) - \int_Z l(a, z)dP(z)\right| &gt; \varepsilon \right) \quad  \underset{n \to \infty}{\to} \quad  0 \quad\quad \forall \varepsilon &gt; 0.\]</div>
<p>Denote the functions of the random variable <span class="math">\(Z\)</span> as
<span class="math">\(l_a(Z):=l(a, Z)\)</span> and refer to the transformed random variable as
<span class="math">\(l_a\)</span>. Then, the centered empirical random variable for a given
action is</p>
<div class="math">
\[(P_n l_a - P l_a) := \int_Z l(a, z)dP_n (z) - \int_Z l(a, z)dP(z).\]</div>
<p>Note, that this is a random variable because the empirical distribution
is a function of the random sample. <span class="math">\(Pl_a\)</span> is just the expectation
of <span class="math">\(l_a(Z)\)</span> and hence it is a non-random scalar. Next we study the
tail probabilities of these centered random variables.</p>
</div>
<div class="section" id="markov-s-inequality">
<h3>Markov&#8217;s inequality<a class="headerlink" href="#markov-s-inequality" title="Permalink to this headline">¶</a></h3>
<p>Markov&#8217;s inequality states that for any non-negative scalar random
variable <span class="math">\(Z\)</span> and <span class="math">\(t&gt;0\)</span> we have</p>
<div class="math">
\[P\{Z \geq t\}\leq \frac{\mathbb{E}[Z]}{t}.\]</div>
<p>The idea behind this inequality is fairly simple and in fact it follows
from the following obvious inequality</p>
<div class="math">
\[\mathbf{1}\{Z\geq t\}\leq \frac{Z}{t}\]</div>
<p>illustrated by the figure below. Clearly, for any probability measure
<span class="math">\(P\)</span> over <span class="math">\(Z\)</span> with a finite first moment Markov&#8217;s inequality
can be established.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">f1</span> <span class="o">=</span> <span class="p">(</span><span class="n">z</span> <span class="o">&gt;=</span> <span class="n">t</span><span class="p">)</span>
<span class="n">f2</span> <span class="o">=</span> <span class="n">z</span> <span class="o">/</span> <span class="n">t</span>

<span class="n">colors</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">ConnectionPatch</span><span class="p">(</span><span class="n">xyA</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">xyB</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">coordsA</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">coordsB</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span>
                                 <span class="n">arrowstyle</span><span class="o">=</span><span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span> <span class="n">mutation_scale</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">ConnectionPatch</span><span class="p">(</span><span class="n">xyA</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">xyB</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">),</span> <span class="n">coordsA</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">coordsB</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span>
                                 <span class="n">arrowstyle</span><span class="o">=</span><span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span> <span class="n">mutation_scale</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">f1</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">r&#39;$\mathbf</span><span class="si">{1}</span><span class="s1">\{Z \geq t\}$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">f2</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">r&#39;$\frac</span><span class="si">{Z}{t}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">9.3</span><span class="p">,</span> <span class="o">-.</span><span class="mi">25</span><span class="p">,</span> <span class="s1">r&#39;$Z$&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">17</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">r&#39;$\frac</span><span class="si">{Z}{t}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">7</span><span class="p">,</span> <span class="s1">&#39;$\mathbf</span><span class="si">{1}</span><span class="s1">\{Z \geq t\}$&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">t</span> <span class="o">-</span> <span class="o">.</span><span class="mi">05</span><span class="p">,</span> <span class="o">-.</span><span class="mi">25</span><span class="p">,</span> <span class="s1">&#39;$t$&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-.</span><span class="mi">25</span><span class="p">,</span> <span class="s1">&#39;$0$&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span> <span class="o">-.</span><span class="mi">6</span><span class="p">,</span> <span class="o">.</span><span class="mi">93</span><span class="p">,</span> <span class="s1">r&#39;$1$&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Logic behind Markov</span><span class="se">\&#39;</span><span class="s1">s inequality&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">t</span> <span class="o">=</span> <span class="mf">2.5</span>
<span class="n">mu</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">mu</span> <span class="o">-</span> <span class="mi">6</span><span class="p">,</span> <span class="n">mu</span> <span class="o">+</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">g1</span> <span class="o">=</span> <span class="p">(</span><span class="n">z</span> <span class="o">&gt;=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">t</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">z</span> <span class="o">&lt;=</span> <span class="n">mu</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span>
<span class="n">g2</span> <span class="o">=</span> <span class="p">((</span><span class="n">z</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="n">t</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">ConnectionPatch</span><span class="p">(</span><span class="n">xyA</span><span class="o">=</span><span class="p">(</span><span class="n">mu</span> <span class="o">-</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">xyB</span><span class="o">=</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">coordsA</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">coordsB</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span>
                                 <span class="n">arrowstyle</span><span class="o">=</span><span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span> <span class="n">mutation_scale</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">r&#39;$\mathbf</span><span class="si">{1}</span><span class="s1">\{Z \geq t\}$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">g2</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">r&#39;$Z/t$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="mf">5.3</span><span class="p">,</span> <span class="o">-.</span><span class="mi">25</span><span class="p">,</span> <span class="s1">r&#39;$Z$&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">17</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">4.7</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">r&#39;$\frac{|Z-\mu|^2}{t^2}$&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">4.7</span><span class="p">,</span> <span class="o">.</span><span class="mi">7</span><span class="p">,</span> <span class="s1">r&#39;$\mathbf</span><span class="si">{1}</span><span class="s1">\{|Z -\mu |\geq t\}$&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">mu</span> <span class="o">-</span> <span class="o">.</span><span class="mi">05</span><span class="p">,</span> <span class="o">-.</span><span class="mi">25</span><span class="p">,</span> <span class="s1">r&#39;$\mu$&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="n">t</span> <span class="o">-</span> <span class="o">.</span><span class="mi">15</span><span class="p">,</span> <span class="o">-.</span><span class="mi">25</span><span class="p">,</span> <span class="s1">r&#39;$\mu + t$&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">mu</span> <span class="o">-</span> <span class="n">t</span> <span class="o">-</span> <span class="o">.</span><span class="mi">15</span><span class="p">,</span> <span class="o">-.</span><span class="mi">25</span><span class="p">,</span> <span class="s1">r&#39;$\mu - t$&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">mu</span> <span class="o">-</span> <span class="mi">6</span> <span class="o">-</span> <span class="o">.</span><span class="mi">6</span><span class="p">,</span> <span class="o">.</span><span class="mi">93</span><span class="p">,</span> <span class="s1">r&#39;$1$&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Logic behind Chebyshev</span><span class="se">\&#39;</span><span class="s1">s inequality&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../_images/Asymptotic_analysis_consistency_16_0.png" src="../_images/Asymptotic_analysis_consistency_16_0.png" />
<p>It follows from Markov&#8217;s inequality that for any strictly monotonically
increasing non-negative function <span class="math">\(\phi\)</span> and any random variable
<span class="math">\(Z\)</span> (not necessarily non-negative) we have that</p>
<div class="math">
\[P\{Z \geq t\} = P\{\phi(Z) \geq \phi(t)\} \leq \frac{\mathbb{E}[\phi(Z)]}{\phi(t)}.\]</div>
<p>Taking the centered random variable <span class="math">\(| Z - \mathbb{E}[Z]|\)</span> and
<span class="math">\(\phi(x) = x^q\)</span> for some <span class="math">\(q&gt;0\)</span> leads to tail bounds
expressed in terms of the moments of the random variable <span class="math">\(Z\)</span>.</p>
<div class="math">
\[P\{| Z - \mathbb{E}[Z]| \geq t\} \leq \frac{\mathbb{E}[| Z - \mathbb{E}[Z]|^q]}{t^q}.\]</div>
<p>Note that for <span class="math">\(q = 2\)</span>, this form delivers the Chebyshev inequality
(see the right panel of the Figure). This approach to bounding tail
probabilities is quite general. Controlling higher order moments leads
to (weakly) tighter bounds on the tail probabilities.</p>
</div>
<div class="section" id="chernoff-bounds">
<h3>Chernoff bounds<a class="headerlink" href="#chernoff-bounds" title="Permalink to this headline">¶</a></h3>
<p>A related idea is at the core of the so called Chernoff bound. For that
one takes the transformation <span class="math">\(\phi(x) = e^{\lambda x}\)</span> applied to
the centered random variable <span class="math">\((Z - \mathbb{E}[Z])\)</span> which yields</p>
<div class="math">
\[P\{(Z - \mathbb{E}[Z]) \geq t\} \leq \frac{\mathbb{E}\left[e^{\lambda(Z - \mathbb{E}[Z])}\right]}{e^{\lambda t}}.\]</div>
<p>Minimizing the bound over <span class="math">\(\lambda\)</span> (provided the moments exist)
would lead us to the Chernoff bound</p>
<div class="math">
\[\log P\{(Z - \mathbb{E}[Z]) \geq t\} \leq - \sup_{\lambda} \left\{\lambda t - \log \mathbb{E}\left[e^{\lambda(Z - \mathbb{E}[Z])}\right]\right\}\]</div>
<p>Before applying directly the developed tail bounds to the random
variable defined above&#8212;<span class="math">\((P_n l_a - P l_a)\)</span>&#8212;we exploit the
fact that it is a sum of random variables.</p>
</div>
<div class="section" id="hoeffding-bounds">
<h3>Hoeffding bounds<a class="headerlink" href="#hoeffding-bounds" title="Permalink to this headline">¶</a></h3>
<p>Suppose first that the sample <span class="math">\(z^n\)</span> is generated by an iid process
and recall that
<span class="math">\((P_n l_a - P l_a) = \frac{1}{n}\sum_{i=1}^n l_a(Z_i) - \mathbb{E}[l_a(Z)]\)</span>.
Bounding the tails via Chernoff&#8217;s method yields</p>
<div class="math">
\[P\left\{\left(\frac{1}{n}\sum_{i=1}^n l_a(Z_i) - \mathbb{E}[l_a(Z)]\right) \geq t\right\} \leq \frac{\mathbb{E}\left[e^{\lambda \frac{1}{n}\sum_{i=1}^n \left(l_a(Z_i) - \mathbb{E}[l_a(Z)]\right)}\right]}{e^{\lambda t}} = e^{-\lambda t} \prod_{i=1}^n \mathbb{E}\left[e^{\lambda \frac{1}{n}\left(l_a(Z_i) - \mathbb{E}[l_a(Z)] \right)} \right]\]</div>
<p>where the equality follows from independece. Hence, the problem of
deriving a tight bound boils down to bounding the moment generating
function of</p>
<div class="math">
\[\frac{1}{n}\left(l_a(Z_i) - \mathbb{E}[l_a(Z)] \right).\]</div>
<p>For our purposes a particularly important class of random variables are
the so called sub-Gaussian variables.</p>
<p><strong>Definition.</strong> A random variable <span class="math">\(Z\)</span> is called sub-Gaussian if
there is a positive number <span class="math">\(\sigma\)</span> such that</p>
<div class="math">
\[\mathbb{E}\left[e^{\lambda \left(Z - \mathbb{E}[Z] \right)} \right] \leq e^{\sigma^2\lambda^2/2}\]</div>
<p>for all <span class="math">\(\lambda \in \mathbb{R}\)</span>.</p>
<p>Remark: A Guassian variable with variance <span class="math">\(\sigma^2\)</span> is
sub-Gaussian with parameter <span class="math">\(\sigma\)</span>. There are other non-Guassian
random variables which are sub-Gaussian &#8211; for example all bounded
random variables, Rademacher variables</p>
<p><strong>Theorem.</strong> (Hoeffding) Let the variables <span class="math">\(l_a(Z_i)\)</span> be iid and
sub-Gaussian with parameter <span class="math">\(\sigma\)</span>. Then, for all <span class="math">\(t&gt;0\)</span> we
have that</p>
<div class="math">
\[P\left\{\left|\frac{1}{n}\sum_{i=1}^n l_a(Z_i) - \mathbb{E}[l_a(Z)]\right| \geq t\right\} \leq 2\exp\left\{-\frac{t^2 n}{2\sigma^2}\right\}.\]</div>
<p>For a <em>single action :math:`a`</em> we see how the Hoeffding inequality is
at the core of the law of large numbers. In fact it gives an exponential
rate of convergence.</p>
<p>However, in order to talk about concentration properties of decision
rules, <span class="math">\(d: \mathcal{S}\mapsto\mathcal{A}\)</span> we would like to make
statements about tail probabilities uniformly over a class of actions.
This leads us to uniform bounds.</p>
</div>
<div class="section" id="uniform-tail-and-concentration-bounds-for-classes-of-finite-cardinality">
<h3>Uniform Tail and Concentration Bounds for classes of finite cardinality<a class="headerlink" href="#uniform-tail-and-concentration-bounds-for-classes-of-finite-cardinality" title="Permalink to this headline">¶</a></h3>
<p>As a first simple example we consider the case when there are only
finitely many actions in the set <span class="math">\(\mathcal{A}\)</span>. Denote the
cardinality by <span class="math">\(\#\mathcal{A}=A_k\)</span>. A conservative estimate of the
uniform tail bound in this case is the union bound.</p>
<p><strong>Corollary.</strong> (Hoeffding union bound) Let the variables
<span class="math">\(\{l_a(Z_i): a=1,\ldots, A_k\}\)</span> be iid and sub-Gaussian with
common parameter <span class="math">\(\sigma\)</span>. Then, for all <span class="math">\(t &gt; 0\)</span> we have
that</p>
<div class="math">
\[P\left\{\sup_{a\in\mathcal{A}}\left| \frac{1}{n}\sum_{i=1}^n l_a(Z_i) - \mathbb{E}[l_a(Z)]\right| \geq t \right\} \leq \sum_{a = 1}^{A_k} P\left\{\left|\frac{1}{n}\sum_{i=1}^n l_a(Z_i) - \mathbb{E}[l_a(Z)]\right| \geq t \right\} \leq A_k 2\exp\left\{-\frac{t^2 n}{2\sigma^2}\right\}.\]</div>
<p>The difference between the uniform and individual Hoeffding bounds is
just a scaling factor, <span class="math">\(A_k\)</span>. In the following, our objective is
to extend the above analysis to action spaces with infinite cardinality.
To this end, we need to find a better &#8220;measure&#8221; of the size of the
action space than its cardinality. Next, we introduce one such
complexity measure which proves to be extremely useful in the case of
characterizing tail bounds of analog estimators.</p>
</div>
</div>
<div class="section" id="uniform-tail-bounds-for-classes-of-infinite-cardinality-rademacher-complexity">
<h2>Uniform tail bounds for classes of infinite cardinality - Rademacher complexity<a class="headerlink" href="#uniform-tail-bounds-for-classes-of-infinite-cardinality-rademacher-complexity" title="Permalink to this headline">¶</a></h2>
<p>In order to work with sets of infinitely many functions we would like to
capture the size of these infinite sets for the purpose of statistical
analysis. One such measure of statistical size is the Rademacher
complexity of a class of real-valued functions.</p>
<p>For a given realization of a sample, <span class="math">\(z^n\)</span>, of size <span class="math">\(n\)</span>
consider the the set of vectors</p>
<div class="math">
\[\mathcal{L}(z^n) := \left\{ (l(z_1), \ldots, l(z_n)) \in \mathbb{R}^n \mid l \in \mathcal{L} \right\}.\]</div>
<p>This is the number of ways one can label points of a sample using
functions in the class <span class="math">\(\mathcal{F}\)</span>. The <strong>empirical Rademacher
complexity</strong> of <span class="math">\(\mathcal{L}\)</span> for fixed <span class="math">\(z^n\)</span> is defined as</p>
<div class="math">
\[\mathcal{R}\left(\frac{\mathcal{L}(z^n)}{n} \right) := \mathbb{E}_{\epsilon}\left[\sup_{l\in \mathcal{L}}\Big| \frac{1}{n}\sum_{i=1}^n \epsilon_i l(z_i) \Big| \right]\]</div>
<p>where the expectation is taken with respect to the iid Rademacher random
variables, <span class="math">\(\epsilon_i\)</span> which take value in <span class="math">\(\{-1, 1\}\)</span> with
equal probability.</p>
<p>The <strong>Rademacher complexity</strong> of <span class="math">\(\mathcal{L}\)</span> at sample size
<span class="math">\(n\)</span> is defined as</p>
<div class="math">
\[\mathcal{R}_n\left(\mathcal{L}\right) := \mathbb{E}_{Z^n} \Big[\mathcal{R}\left(\frac{\mathcal{L}(z^n)}{n} \right)\Big]\]</div>
<p>.</p>
<p>The Rademacher complexity has an intuitive interpretation. It is the
average of the maximum correlations between the vectors
<span class="math">\(\big(l(z_1), \ldots, l(z_n)\big)\)</span> and the pure noise vector
<span class="math">\(\big(\epsilon(z_1), \ldots, \epsilon(z_n)\big)\)</span>. The function
class <span class="math">\(\mathcal{L}\)</span> is too &#8220;large&#8221; for statistical purposes, if we
can always choose a function, <span class="math">\(l\in\mathcal{L}\)</span> that has high
correlation with a randomly drawn noise vector (Wainwright, 2015).</p>
<div class="section" id="illustration-of-rademacher-complexity">
<h3>Illustration of Rademacher complexity<a class="headerlink" href="#illustration-of-rademacher-complexity" title="Permalink to this headline">¶</a></h3>
<p>In the following we consider two classes of functions. Although both
classes can be parametrized by a <em>single</em> free parameter, we will see
that one of them has drastically smaller Rademacher complexity than the
other.</p>
<div class="section" id="coin-tossing-example-revisited">
<h4>Coin tossing example revisited<a class="headerlink" href="#coin-tossing-example-revisited" title="Permalink to this headline">¶</a></h4>
<p>Consider again the coin tossing example with the quadratic loss function
discussed before. In this case <span class="math">\(Z^n\)</span> is an iid sample from a
Binomial distribution parametrized with <span class="math">\(p\in[0, 1]\)</span>. The class of
functions is given by the quadratic class</p>
<div class="math">
\[\mathcal{L} := \{l_a : l_a(z) = (z - a)^2, \ \  a \in [0, 1]\subseteq \mathbb{R}\}\]</div>
<p>Below we see how the Rademacher compexity of <span class="math">\(\mathcal{L}\)</span>
converges to zero as the sample size grows to infinity.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1"># Compute the Rademacher compelxity for the coin tossing example</span>
<span class="n">n</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">6</span>

<span class="k">def</span> <span class="nf">given_epsilon_path</span><span class="p">(</span><span class="n">n3</span><span class="p">,</span> <span class="n">n4</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">n1</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">k</span> <span class="o">-</span> <span class="n">n3</span>
    <span class="n">n2</span> <span class="o">=</span> <span class="n">k</span> <span class="o">-</span> <span class="n">n4</span>

    <span class="n">diff1</span> <span class="o">=</span> <span class="n">n1</span> <span class="o">-</span> <span class="n">n3</span>
    <span class="n">diff2</span> <span class="o">=</span> <span class="n">n2</span> <span class="o">-</span> <span class="n">n4</span>

    <span class="n">term1</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">diff1</span><span class="o">/</span><span class="n">n</span><span class="p">)</span>
    <span class="n">term2</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">diff2</span><span class="o">/</span><span class="n">n</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">diff1</span> <span class="o">+</span> <span class="n">diff2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">term3</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">((</span><span class="n">diff2</span><span class="o">/</span><span class="p">(</span><span class="n">diff1</span> <span class="o">+</span> <span class="n">diff2</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">diff1</span><span class="o">/</span><span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">diff1</span><span class="o">/</span><span class="p">(</span><span class="n">diff1</span> <span class="o">+</span> <span class="n">diff2</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">diff2</span><span class="o">/</span><span class="n">n</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">term3</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">term1</span><span class="p">,</span> <span class="n">term2</span><span class="p">,</span> <span class="n">term3</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">empirical_rademacher</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">prob_row</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">prob_col</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">k</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">prob_matrix</span> <span class="o">=</span> <span class="n">prob_row</span> <span class="o">*</span> <span class="n">prob_col</span>

    <span class="n">emp_rad</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>               <span class="c1"># loop for n4</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>       <span class="c1"># loop for n3</span>
            <span class="n">emp_rad</span> <span class="o">+=</span> <span class="n">given_epsilon_path</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="n">prob_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">emp_rad</span>

<span class="k">def</span> <span class="nf">rademacher_complexity</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">true_prob</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">theta0</span><span class="p">)</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">rademacher</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">rademacher</span> <span class="o">+=</span> <span class="n">true_prob</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">empirical_rademacher</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">rademacher</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1">#def rademacher_bound(n, delta):</span>
<span class="c1">#    B = 1</span>
<span class="c1">#    arg = -(n * (delta - 2 * rademacher_complexity(n))**2/(8 * B**2))</span>

<span class="c1">#    return 2 * np.exp(arg)</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">RC</span> <span class="o">=</span> <span class="p">[</span><span class="n">rademacher_complexity</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">RC</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Rademacher complexity of $\mathcal</span><span class="si">{L}</span><span class="s2">$ in the coin tossing example&quot;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">17</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sample size&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">25</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../_images/Asymptotic_analysis_consistency_25_1.png" src="../_images/Asymptotic_analysis_consistency_25_1.png" />
</div>
</div>
<div class="section" id="sinusoid-classification">
<h3>Sinusoid classification<a class="headerlink" href="#sinusoid-classification" title="Permalink to this headline">¶</a></h3>
<p>Next, we consider a set of classifier functions where the classification
boundary is given by a sine function.</p>
<div class="math">
\[\mathcal{L} := \Big\{ \mathbb{1}\{sin(az) \geq 0\} - \mathbb{1}\{sin(az) &lt; 0\} : a \in \mathbb{R}_+ \Big\}\]</div>
<p>For better illustration, in this case we consider the empirical
Rademacher complexity for a fixed realization of the sample,
<span class="math">\(z^n\)</span>.</p>
<p><em>Remark:</em> In order to have a closed form solution for the optimal
classifier we are selecting the sample at convenient points. This is
without loss of generality and useful for illustrative purposes.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">n_slider</span> <span class="o">=</span> <span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="nb">max</span> <span class="o">=</span> <span class="mi">20</span> <span class="p">,</span> <span class="n">step</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">rademacher</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Generates N Rademcaher random variables.&#39;&#39;&#39;</span>
    <span class="n">aux</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="o">.</span><span class="mi">5</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">aux</span><span class="p">])</span>

<span class="nd">@interact</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="n">n_slider</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">rademacher_example</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="p">))])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">rademacher</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>

    <span class="n">y_aux</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span> <span class="k">if</span> <span class="n">r</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">y</span><span class="p">])</span>
    <span class="n">x_aux</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="o">**</span><span class="p">((</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="p">))])</span>

    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">y_aux</span><span class="p">)</span><span class="nd">@x_aux</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">a</span><span class="o">*</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">c</span> <span class="o">@</span> <span class="n">y</span> <span class="o">/</span> <span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>


    <span class="n">x_axis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">a</span><span class="o">*</span><span class="n">x_axis</span><span class="p">),</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-.</span><span class="mi">2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="s1">r&#39;$\mathcal</span><span class="si">{R}</span><span class="s1">&#39;</span> <span class="o">+</span> <span class="s1">r&#39;_</span><span class="si">{}</span><span class="s1">(x^</span><span class="si">{}</span><span class="s1">, \epsilon^</span><span class="si">{}</span><span class="s1">) = $&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">({</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="p">)},</span> <span class="p">{</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="p">)},</span> <span class="p">{</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="p">)})</span> <span class="o">+</span> <span class="s1">&#39; </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">R</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Evolution of maximum attainable correlation</span><span class="se">\n</span><span class="s1"> for given sample and noise&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../_images/Asymptotic_analysis_consistency_27_0.png" src="../_images/Asymptotic_analysis_consistency_27_0.png" />
<p>As we can see, by choosing a sufficiently high frequency we can always
find a curve which classifies the data perfectly. Consequently, the
Ramdemacher complexity always takes its maximum irrespective of the
sample size. For statistical purposes the family of sine curves is too
complex.</p>
</div>
</div>
<div class="section" id="uniform-bounds-using-the-rademacher-complexity">
<h2>Uniform bounds using the Rademacher complexity<a class="headerlink" href="#uniform-bounds-using-the-rademacher-complexity" title="Permalink to this headline">¶</a></h2>
<p>With the introduced concepts we are in the position to present an
important concentration inequality for classes of infinite cardinality.</p>
<p><strong>Theorem:</strong> For a uniformly bounded set of functions,
<span class="math">\(\lVert l \rVert_{\infty} &lt; 1\)</span>, for all <span class="math">\(l \in \mathcal{L}\)</span>
and <span class="math">\(\delta &gt; 0\)</span> we have that</p>
<div class="math">
\[P\Big\{ \Vert P_n - P \Vert_{\mathcal{L}} \geq 2 \mathcal{R}_n(\mathcal{L}) + \delta \Big\} \leq 2 \exp\Big\{-n 2 \delta^2 \Big\}.\]</div>
<p>and for a given <span class="math">\(z^n\)</span></p>
<div class="math">
\[P\Big\{ \Vert P_n - P \Vert_{\mathcal{L}} \geq 2 \mathcal{R}_n\left(\frac{\mathcal{L}(z^n)}{n}\right) + \delta \Big\} \leq 2 \exp\Big\{-n \frac{\big(\delta - 2 \mathcal{R}_n(\mathcal{L})\big)^2}{4} \Big\}.\]</div>
<p>Rewriting slightly makes it a bit clearer. If
<span class="math">\(\delta - 2 \mathcal{R}_n(\mathcal{L}) &gt; 0\)</span> then</p>
<div class="math">
\[P\Big\{ \Vert P_n - P \Vert_{\mathcal{L}} \geq  \delta \Big\} \leq 2 \exp\Big\{- n 2 \big(\delta - 2 \mathcal{R}_n(\mathcal{L})\big)^2 \Big\}.\]</div>
<p>Hence, if <span class="math">\(\mathcal{R}_n(\mathcal{L}) = o(1)\)</span> then
<span class="math">\(\Vert P_n - P \Vert_{\mathcal{L}} \to 0\)</span> or put it differently,
<span class="math">\(\mathcal{L}\)</span> is Glivenko-Cantelli.</p>
<p>It is apparent from the above inequality that the tightness of the
finite sample uniform bounds will be (partly) determined by the
Rademacher complexity. The change in Rademacher complexity as the sample
size grows will determine the (non-asymptotic) rate of convergence.</p>
<p>Unfortunately, computing the Rademacher complexity directly is only
feasible in rare special cases. There are various techniques however,
which give bounds on the Rademacher complexity. For different classes of
functions different techniques prove useful, so one usually proceeds on
a case-by-case basis.</p>
<p>The most common ways of bounding the Rademacher complexity are via the
<em>Vapnik-Chervonenkis dimension</em> for classification and via <em>metric
entropy</em> for bounded real valued functions.</p>
<p><strong>TO DO: compare these bounds with the true tail probabilities in the
coin tossing example (see figure above)</strong></p>
</div>
<hr class="docutils" />
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>Bousquet, O., Boucheron, S., &amp; Lugosi, G. (2004). Introduction to
statistical learning theory. In Advanced lectures on machine learning
(pp. 169-207). Springer Berlin Heidelberg.</p>
<p>Chervonenkis, A. and Vapnik, V. (1971). Theory of uniform convergence of
frequencies of events to their probabilities and problems of search for
an optimal solution from empirical data. Automation and Remote Control,
32, 207-217.</p>
<p>Hansen, L. P. (1982). Large sample properties of generalized method of
moments estimators. Econometrica: Journal of the Econometric Society,
1029-1054.</p>
<p>Manski, C. F. (1988). Analog estimation methods in econometrics. Chapman
and Hall.</p>
<p>Van der Vaart, A. W. (2000). Asymptotic statistics (Vol. 3). Cambridge
University Press.</p>
<p>White, Halbert (1994), Estimation, Inference and Specification Analysis
(Econometric Society Monographs). Cambridge University Press.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Asymptotic analysis and consistency</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#consistency-of-decision-rules">Consistency of decision rules</a></li>
<li><a class="reference internal" href="#uniform-law-of-large-numbers">Uniform law of large numbers</a><ul>
<li><a class="reference internal" href="#generalization">Generalization</a><ul>
<li><a class="reference internal" href="#a-special-case-plug-in-estimators">A special case (plug-in estimators)</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#uniform-law-of-large-numbers-and-consistency">Uniform law of large numbers and consistency</a><ul>
<li><a class="reference internal" href="#sufficient-conditions">Sufficient conditions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#non-asymptotic-bounds">Non-asymptotic bounds</a><ul>
<li><a class="reference internal" href="#tail-and-concentration-bounds">Tail and Concentration Bounds</a></li>
<li><a class="reference internal" href="#markov-s-inequality">Markov&#8217;s inequality</a></li>
<li><a class="reference internal" href="#chernoff-bounds">Chernoff bounds</a></li>
<li><a class="reference internal" href="#hoeffding-bounds">Hoeffding bounds</a></li>
<li><a class="reference internal" href="#uniform-tail-and-concentration-bounds-for-classes-of-finite-cardinality">Uniform Tail and Concentration Bounds for classes of finite cardinality</a></li>
</ul>
</li>
<li><a class="reference internal" href="#uniform-tail-bounds-for-classes-of-infinite-cardinality-rademacher-complexity">Uniform tail bounds for classes of infinite cardinality - Rademacher complexity</a><ul>
<li><a class="reference internal" href="#illustration-of-rademacher-complexity">Illustration of Rademacher complexity</a><ul>
<li><a class="reference internal" href="#coin-tossing-example-revisited">Coin tossing example revisited</a></li>
</ul>
</li>
<li><a class="reference internal" href="#sinusoid-classification">Sinusoid classification</a></li>
</ul>
</li>
<li><a class="reference internal" href="#uniform-bounds-using-the-rademacher-complexity">Uniform bounds using the Rademacher complexity</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="../Notebook_01_Wald/Statistical_decision_functions.html" title="previous chapter">Estimators as Statistical Decision Functions</a></li>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/Notebook_02_Asymptotics/Asymptotic_analysis_consistency.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2016, Daniel Csaba, Balint Szoke.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.4.6</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.9</a>
      
      |
      <a href="../_sources/Notebook_02_Asymptotics/Asymptotic_analysis_consistency.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>