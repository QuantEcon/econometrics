<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Estimators as Statistical Decision Functions &mdash; econometrics 0.1 documentation</title>
    
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="econometrics 0.1 documentation" href="../index.html" />
    <link rel="prev" title="Welcome to econometrics’s documentation!" href="../index.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="estimators-as-statistical-decision-functions">
<h1>Estimators as Statistical Decision Functions<a class="headerlink" href="#estimators-as-statistical-decision-functions" title="Permalink to this headline">¶</a></h1>
<p><strong>Date: November 2016</strong></p>
<p>In this notebook we discuss some key concepts of statistical decision
theory in order to provide a general framework for the comparison of
alternative estimators based on their finite sample performance.</p>
<ul class="simple">
<li>The primitive object is a statistical decision problem containing a
loss function, an action space, and a set of assumed statistical
models. We demonstrate that most estimation problems familiar from
econometrics can be formulated as a statistical decision problem.</li>
<li>We compare estimators based on their (finite sample) risk, where risk
is derived from an unknown true data generating mechanism.</li>
<li>We present some straightforward examples to illustrate the main
ideas.</li>
</ul>
<p><strong>TO DO: Add Notations</strong></p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># For coin tossing</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="k">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">FloatSlider</span>

<span class="c1"># For linear regression</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="k">import</span> <span class="n">multivariate_normal</span>
<span class="kn">from</span> <span class="nn">scipy.integrate</span> <span class="k">import</span> <span class="n">dblquad</span>

<span class="c1"># Shut down warnings for nicer output</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h1>
<p><strong>TO DO: add links to QE lectures (ensemble, ergodicity, LLN)</strong></p>
<div class="section" id="stationarity-and-statistical-models">
<h2>Stationarity and statistical models<a class="headerlink" href="#stationarity-and-statistical-models" title="Permalink to this headline">¶</a></h2>
<p>The observed data are modelled as a partial realization of a stochastic
process <span class="math">\(\{Z_t\}_{t\in\mathbb{Z}}\)</span> taking values in
<span class="math">\(\mathbb{R}^{k}\)</span>. Denote a particular realization as the sequence
<span class="math">\(z^{\infty} \in \mathbb{R}^{k\mathbb{Z}}\)</span> and let the partial
history <span class="math">\(z^{n}\)</span> containing <span class="math">\(n\)</span> consecutive elements of the
realization be the <em>sample</em> of size <span class="math">\(n\)</span>. We assume that there
exists a core mechanism undelying this process that describes the
relationships among the elements of the vector <span class="math">\(Z\)</span>. Our aim is to
draw inference about this mechanism after observing a single partial
realization <span class="math">\(z^{n}\)</span>.</p>
<p>How is this possible without being able to draw different samples under
the exact same conditions? A fruitful approach is to assume that the
underlying mechanism is time invariant with the stochastic process being
strictly stationary and study its statistical properties by taking
long-run time averages of the realization <span class="math">\(z^{\infty}\)</span> (or
functions thereof) like</p>
<div class="math">
\[\lim_{n\to \infty}\frac{1}{n}\sum_{t = 1}^{n} z_t\quad\quad \lim_{n\to \infty}\frac{1}{n} \sum_{t = 1}^{n} z^2_t\quad\quad \lim_{n\to \infty}\frac{1}{n}\sum_{t = k}^{n+k} z_{t}z_{t-k}\]</div>
<p>Since the mechanism is assumed to be stable over time, it does not
matter when we start observing the process.</p>
<p>Notice, however, that strictly speaking these time averages are
properties of the particular realization, the extent to which they can
be generalized to the mechanism itself is not obvious. To address this
question, it is illuminating to bundle realizations that share certain
statistical properties together in order to construct an artificial
universe of (counterfactual) alternative <span class="math">\(z^{\infty}\)</span>-s, the so
called <em>ensemble</em>. Statistical properties of the data generating
mechanism can be summarized by assigning probabilities to (sets of)
these <span class="math">\(z^{\infty}\)</span>-s in an internally consistent manner. These
considerations lead us to the idea of a statistical model.</p>
<p><strong>Statistical models</strong> are probability distributions over sequences
<span class="math">\(z^{\infty}\)</span> that assign probabilities so that the unconditional
moments are consistent with the associated long-run time averages. In
other words, with statistical models the time series and ensemble
averages coincide, which is the property known as <strong>ergodicity</strong>.
Roughly speaking, ergodicity allows us to learn about the ensemble
dimension by using a <em>single</em> realization <span class="math">\(z^{\infty}\)</span>.</p>
<div class="section" id="dependence">
<h3>Dependence<a class="headerlink" href="#dependence" title="Permalink to this headline">¶</a></h3>
<p>In reality, however, being endowed only with a partial history of
<span class="math">\(z^{\infty}\)</span>, we cannot calculate the exact log-run time averages.
Nonetheless, by imposing more structure on the problem and having a
sufficinetly large sample, we can obtain reasonable approximations. To
this end, we need to assume some form of weak independence (&#8220;mixing&#8221;),
or more precisely, the property that on average, the dependence between
the elements of <span class="math">\(\{Z_t\}_{t\in\mathbb{Z}}\)</span> dies out as we increase
the gap between them.</p>
<p>Consequently, if we see a <em>long</em> segment of <span class="math">\(z^{\infty}\)</span> and cut
it up into shorter consecutive pieces, say of length <span class="math">\(l\)</span>, then, we
might consider these pieces (provided that <span class="math">\(l\)</span> is &#8220;large enough&#8221;)
as nearly independent records from the distribution of the
<span class="math">\(l\)</span>-block. To clarify this point, consider a statistical model
<span class="math">\(Q_{Z^{\infty}}\)</span> (joint distribution over sequences
<span class="math">\(z^{\infty}\)</span>) with density function <span class="math">\(q_{z^{\infty}}\)</span> and
denote the implied density of the sample as <span class="math">\(q_{n}\)</span>. Note that
because of strict stationarity, it is enough to use the number of
consecutive elements as indices. Under quite general conditions we can
decompose this density as</p>
<div class="math">
\[q_{n}(z^n) = q_{n-1}(z_n | z^{n-1})q_{n-1}(z^{n-1}) = q_{n-1}(z_n | z^{n-1})q_{n-2}(z_{n-1}|z^{n-2})\dots q_{1}(z_{2}|z_1)q_{1}(z_1)\]</div>
<p>For simplicity, we assume that the stochastic process is Markov so that
the partial histories <span class="math">\(z^{i}\)</span> for <span class="math">\(i=1,\dots, n-1\)</span> in the
conditioning sets can be replaced by the &#8220;right&#8221; number of lags
<span class="math">\(z^{n-1}_{n-l}\)</span> and we can drop the subindex from the conditional
densties</p>
<div class="math">
\[q_{n}(z^n) = q(z_n | z^{n-1}_{n-1-l})q(z_{n-1}|z^{n-2}_{n-2-l})\dots q(z_{l+1}|z_{1}^{l})q_{l}(z^l) \quad\quad (1)\]</div>
<p>This assumption is much stronger than what we really need. First, it is
enough to require the existence of a history-dependent latent state
similar to the Kalman filter (!!link!!). Moreover, we could also relax
the Markov assumption and allow for dependence that dies out only
asymptotically. In practice, however, we often have a stong view about
the dependency structure, or at least we are willing to use economic
theory to guide our choice of <span class="math">\(l\)</span>, in which cases we almost always
assume a Markovian structure. For simplicity, in these lectures, unless
otherwise stated, we will restrict ourselves to the family of Markov
processes.</p>
<p>This assumption allows us to learn about the underlying mechanism
<span class="math">\(Q_{Z^{\infty}}\)</span> via its <span class="math">\(l+1\)</span>-period building blocks. Once
we determine the (ensemble) distribution of the block,
<span class="math">\(Q_{Z^{[l+1]}}\)</span>, we can &#8220;build up&#8221; <span class="math">\(Q_{Z^{\infty}}\)</span> by using
a formula similar to (1). Having said that the block distribution
<span class="math">\(Q_{Z^{[l+1]}}\)</span> carries the same information as
<span class="math">\(Q_{Z^{\infty}}\)</span>. Therefore, from now on, we define <span class="math">\(Z\)</span> as
the minimal block we need to know and treat it as an observation.
Statistical models can be represented by their prediction about the
ensemble distribution <span class="math">\(P\)</span> of this observable.</p>
</div>
<div class="section" id="true-data-generating-mechanism">
<h3>True data generating mechanism<a class="headerlink" href="#true-data-generating-mechanism" title="Permalink to this headline">¶</a></h3>
<p>We assume that the mechanism underlying <span class="math">\(\{Z_t\}_{t\in\mathbb{Z}}\)</span>
can be represented with a statistical model <span class="math">\(P_0\)</span> and it is called
<em>true data generating process</em>. We seek to learn about the features of
this model from the observed data.</p>
</div>
</div>
</div>
<div class="section" id="primitives-of-the-problem">
<h1>Primitives of the problem<a class="headerlink" href="#primitives-of-the-problem" title="Permalink to this headline">¶</a></h1>
<p>Every statistical decision problem that we will consider can be
represented with a triple <span class="math">\((\mathcal{H}, \mathcal{A}, L)\)</span>, where</p>
<ol class="arabic simple">
<li><strong>Assumed statistical models</strong> &#8211; <span class="math">\(\mathcal{H}\subset \mathcal{P}\)</span></li>
</ol>
<blockquote>
<div>A collection of statistical models (ergodic probability measures) over
the observed data, which captures our <em>assumptions</em> about the data
generating mechanism underlying <span class="math">\(\{Z_t\}_{t\in\mathbb{Z}}\)</span>.
Ergodicity implies that with infinite data we could single out one
element from <span class="math">\(\mathcal{H}\)</span>.</div></blockquote>
<ol class="arabic simple" start="2">
<li><strong>Action space</strong> &#8211; <span class="math">\(\mathcal{A}\)</span></li>
</ol>
<blockquote>
<div>The set of allowable actions. It is an abstract set embodying our proposed <em>specification</em> by which
we aim to capture features of the true data generating mechanism.</div></blockquote>
<ol class="arabic simple" start="3">
<li><strong>Loss function</strong> &#8211; <span class="math">\(L: \mathcal{P}\times \mathcal{A} \mapsto \mathbb{R}_+\)</span></li>
</ol>
<blockquote>
<div><p>The loss function measures the performance of alternative actions
<span class="math">\(a\in \mathcal{A}\)</span> under a given distribution
<span class="math">\(P\in \mathcal{P}\)</span>, where <span class="math">\(\mathcal{P}\)</span> denotes the space
of strictly stationary probability distributions over the observed
data. In principle, <span class="math">\(L\)</span> measures the distance between
distributions in <span class="math">\(\mathcal{P}\)</span> along particular dimensions
determined by features of the data generating mechanism that we are
interested in. By assigning zero distance to models that share a
particular set of features (e.g. conditional expectation, set of
moments, etc.), the loss function can &#8216;determine&#8217; the domain of
effective actions.</p>
<p>Given the assumed statistical models, we can restrict the domain of the
loss function without loss in generality such that,
<span class="math">\(L: \mathcal{H}\times\mathcal{A} \mapsto \mathbb{R}_+\)</span>.</p>
</div></blockquote>
<div class="section" id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h2>
<p><strong>Quadratic loss:</strong></p>
<p>The most commonly used loss function is the quadratic</p>
<div class="math">
\[L(P, a) = \int (z - a)^2\mathrm{d}P(z)\]</div>
<p>where the action space is <span class="math">\(\mathcal{A}\subseteq \mathbb{R}^{k}\)</span>.
Another important case is when we can write <span class="math">\(Z = (Y, X)\)</span>, the loss
function is</p>
<div class="math">
\[L(P, a) = \int (y - a(x))^2\mathrm{d}P(y, z)\]</div>
<p>and the action space <span class="math">\(\mathcal{A}\)</span> contains some well behaving
real functions of <span class="math">\(X\)</span>.</p>
<p><strong>Relative entropy loss:</strong></p>
<p>When we specificy a whole distribution and are willing to approximate
<span class="math">\(P\)</span>, one useful measure for comparison of distributions is the
Kullback-Leibler divergence, or relative entropy</p>
<div class="math">
\[L(P, a) = - \int \log \frac{p}{a}(z) \mathrm{d}P(z)\]</div>
<p>in which case the action space is <span class="math">\(\mathcal{A} = \{a: Z \mapsto \mathbb{R}_+ : \int a(z)\mathrm{d}z = 1 \}\)</span>.</p>
<p><strong>Generalized Method of Moments:</strong></p>
<p>Following the exposition of Manski (1994), many econometric problems can
be cast as solving <span class="math">\(T(P, \theta) = \mathbf{0}\)</span> in the parameter
<span class="math">\(\theta\)</span>, for a given function
<span class="math">\(T: \mathcal{P}\times\Theta \mapsto \mathbb{R}^m\)</span> with
<span class="math">\(\Theta\subseteq\mathbb{R}^p\)</span>. By expressing estimation problems
in terms of unconditional moment restrictions, for example, we can write
<span class="math">\(T(P, \theta) = \int g(z; \theta)\mathrm{d}P(z) = \mathbf{0}\)</span> for
some function <span class="math">\(g\)</span>. Taking an <em>origin-preserving continuous
transformation</em> <span class="math">\(r:\mathbb{R}^m \mapsto \mathbb{R}_+\)</span> so that</p>
<div class="math">
\[T(P, \theta) = \mathbf{0} \iff r(T)=0\]</div>
<p>we can present the problem in terms of minimizing a particular loss
function. Define the action space as <span class="math">\(\mathcal{A} = \Theta\)</span>, then
the method of moment estimator minimizes the loss
<span class="math">\(L(P, \theta) = r\circ T(P, \theta)\)</span>. The most common form of
<span class="math">\(L\)</span> is</p>
<div class="math">
\[L(P, \theta) = \left[\int g(z; \theta)\mathrm{d}P(z)\right]' W \left[\int g(z; \theta)\mathrm{d}P(z)\right]\]</div>
<p>where <span class="math">\(W\)</span> is a <span class="math">\(m\times m\)</span> positive-definite weighting
matrix.</p>
</div>
<div class="section" id="best-in-class-action">
<h2>Best in-class action<a class="headerlink" href="#best-in-class-action" title="Permalink to this headline">¶</a></h2>
<p>By using a loss function, we acknowledge that learning about the true
mechanism might be too ambitious, so we better focus our attention only
on certain features of it and try to approximate those with our
specification. The loss function expresses our assessment about which
features are important and how deviations from the true features are
being punished. With a specified triple we can define <strong>best in-class
action</strong> as</p>
<div class="math">
\[a^*_{L,\ P,\ \mathcal{A}} := \arg\min_{a \in \mathcal{A}} L(P,a).\]</div>
</div>
<div class="section" id="features">
<h2>Features<a class="headerlink" href="#features" title="Permalink to this headline">¶</a></h2>
<p>Often, we denote by <span class="math">\(\gamma(P)\)</span>, the &#8220;feature&#8221; of distribution
<span class="math">\(P\)</span> that we are interested in estimating. We find it useful to
make the connection between the feature and the loss function. In
particular, the minimizer of the loss function on the broadest possible
domain determines the &#8220;true&#8221; feature, <span class="math">\(\gamma(P)\)</span>. When the action
space <span class="math">\(\mathcal{A}\)</span> is restricted, however, this feature might
differ from the best-in class action <span class="math">\(a^*_{L,\ P, \mathcal{A}}\)</span>
defined above. We can summarize this scenario compactly with
<span class="math">\(\gamma(P)\notin \mathcal{A}\)</span> and saying that the &#8220;specification&#8221;
<span class="math">\(\mathcal{A}\)</span> is misspecified. In such cases the loss function
plays a critical role by specifying the punishments for deviations from
the true <span class="math">\(\gamma(P)\)</span>. We will talk more about misspecification in
the following sections. A couple of examples should help clarifying the
introduced concepts.</p>
<ul>
<li><p class="first"><strong>Conditional expectation &#8211; regression function estimation</strong></p>
<p>Consider the quadratic loss function over the domain of all square
integrable functions <span class="math">\(a: X \to \mathbb{R}\)</span>, where
<span class="math">\(Z = (Y, X)\)</span> and <span class="math">\(Y\)</span> is a scalar. The corresponding
feature is</p>
<div class="math">
\[\gamma(P) = \arg\min_{a \in L^2(X)} \int\limits_{(Y,X)} (y - a(x))^2\mathrm{d}P(y, x)\]</div>
<p>and it is equal to the conditional expectation
<span class="math">\(\gamma(P) = \mathbb{E}[Y|X]\)</span>. If the action space
<span class="math">\(\mathcal{A}\)</span> does not include all square integrable functions,
but only the set of affine functions, the best in class action, i.e.,
the linear projection of <span class="math">\(Y\)</span> to the space spanned by <span class="math">\(X\)</span>,
will be different from <span class="math">\(\gamma(P)\)</span> in general. In other words,
the linear specification for the conditional expectation <span class="math">\(Y|X\)</span>
is misspecified.</p>
</li>
<li><p class="first"><strong>Density function estimation</strong></p>
<p>Consider the Kullback-Leibler
distance over the set of distributions with existing density
functions. Denote this set by <span class="math">\(D_Z\)</span>. Given that the true
<span class="math">\(P\in D_Z\)</span>, the corresponding feature is</p>
<div class="math">
\[\gamma(P) = \arg\min_{a \in D_Z} \int\limits_{Z}\log\left(\frac{p(z)}{a(z)}\right) \mathrm{d}P(z)\]</div>
<p>which provides the density <span class="math">\(p\in\mathbb{R}_+^Z\)</span> such that
<span class="math">\(\int p(z)\mathrm{d}z =1\)</span> and for any sensible set
<span class="math">\(B\subseteq \mathbb{R}^k\)</span>,
<span class="math">\(\int_B p(z)\mathrm{d}z = P(B)\)</span>. If the action space
<span class="math">\(\mathcal{A}\)</span> is only a parametric subset of <span class="math">\(D_Z\)</span>, the
best in class action will be the best approximation in terms of KLIC.
For an extensive treatment see White (1994).</p>
</li>
</ul>
<p>An important aspect of the statistical decision problem is the
relationship between <span class="math">\(\mathcal{H}\)</span> and <span class="math">\(\mathcal{A}\)</span>. Our
<em>maintained assumptions</em> about the mechanism are embodied in
<span class="math">\(\mathcal{H}\)</span>, so a natural attitude is to be as agnostic as
possible about <span class="math">\(\mathcal{H}\)</span> in order to avoid incredible
assumptions. Once we determined <span class="math">\(\mathcal{H}\)</span>, the next step is to
choose the specification, that is the action space <span class="math">\(\mathcal{A}\)</span>.</p>
<ul class="simple">
<li>One approach is to tie <span class="math">\(\mathcal{H}\)</span> and <span class="math">\(\mathcal{A}\)</span> together.
For example, the assumptions of the standard linear regression model outline
the distributions contained in <span class="math">\(\mathcal{H}\)</span> (normal with zero mean and homoscedasticity),
for which the natural action space is the space of affine functions.</li>
<li>On the other hand, many approaches explicitly disentangle
<span class="math">\(\mathcal{A}\)</span> from <span class="math">\(\mathcal{H}\)</span> and try to be agnostic
about the maintained assumptions <span class="math">\(\mathcal{H}\)</span> and rather
impose restrictions on the action space <span class="math">\(\mathcal{A}\)</span>. At the
cost of giving up some potentially undominated actions this approach
can largely influence the success of the inference problem in finite
samples.</li>
</ul>
<p>By choosing an action space not &#8220;tied&#8221; to the set of assumed statistical
models, the statistician inherently introduces a possibility of bias &#8211;
for some statistical models there could be an action outside of the
action space which would fare better than any other action within
<span class="math">\(\mathcal{A}\)</span>. However, coarsening the action space in this manner
has the benefit of constraining the variability of estimated actions
arising from the randomness of the sample.</p>
<p>In this case, the best-in class action has a special role, namely, it
minimizes the &#8220;distance&#8221; between <span class="math">\(\mathcal{A}\)</span> and the true
statistical model, thus measuring the benchmark bias stemming from
restricting <span class="math">\(\mathcal{A}\)</span>.</p>
</div>
<div class="section" id="example-coin-tossing">
<h2>Example - Coin tossing<a class="headerlink" href="#example-coin-tossing" title="Permalink to this headline">¶</a></h2>
<p>The observable is a binary variable <span class="math">\(Z\in\{0, 1\}\)</span> generated by
some statistical model. One might approach this problem by using the
following triple</p>
<ul>
<li><p class="first"><em>Assumed statistical models</em>, <span class="math">\(\mathcal{H}\)</span>:</p>
<ul class="simple">
<li><span class="math">\(Z\)</span> is generated by an i.i.d. Bernoulli distribution, i.e.
<span class="math">\(\mathcal{H} = \{P(z; \theta): \theta \in[0,1]\}\)</span></li>
<li>The probability mass function associated with the distribution
<span class="math">\(P(z;\theta)\in\mathcal{H}\)</span> has the form</li>
</ul>
<div class="math">
\[p(z; \theta) = \theta^z(1-\theta)^{1-z}\]</div>
</li>
<li><p class="first"><em>Action space</em>, <span class="math">\(\mathcal{A}\)</span>:</p>
<ul class="simple">
<li>Let the action space be equal to <span class="math">\(\mathcal{H}\)</span>, that is
<span class="math">\(\mathcal{A} = \{P(z, a): a\in[0,1]\} = \mathcal{H}\)</span>.</li>
</ul>
</li>
<li><p class="first"><em>Loss function</em>, <span class="math">\(L\)</span>: We entertain two alternative loss functions</p>
<ul class="simple">
<li>Relative entropy</li>
</ul>
<div class="math">
\[L_{RE}(P, a) = \sum_{z\in\{0,1\}} p(z;  \theta)\log \frac{p(z; \theta)}{p(z; a)} = E_{\theta}[\log p(z; \theta)] - E_{\theta}[\log p(z; a)]\]</div>
<ul class="simple">
<li>Quadratic loss</li>
</ul>
<div class="math">
\[L_{MSE}(P, a) = \sum_{z\in\{0,1\}} p(z;  \theta)(\theta - a)^2 = E_{\theta}[(\theta - a)^2]\]</div>
</li>
</ul>
</div>
<div class="section" id="example-linear-regression-function">
<h2>Example - Linear regression function<a class="headerlink" href="#example-linear-regression-function" title="Permalink to this headline">¶</a></h2>
<p>In the basic setup of regression function estimation we write
<span class="math">\(Z=(Y,X)\in\mathbb{R}^2\)</span> and the objective is to predict the value
of <span class="math">\(Y\)</span> as a function of <span class="math">\(X\)</span> using the quadratic loss
function. Let <span class="math">\(\mathcal{F}:= \{f:X \mapsto Y\}\)</span> be the family of
all functions mapping from <span class="math">\(X\)</span> to <span class="math">\(Y\)</span>. The following is an
example for a triple</p>
<ul class="simple">
<li><em>Assumed statistical models</em>, <span class="math">\(\mathcal{H}\)</span><ul>
<li><span class="math">\((Y,X)\)</span> is generated by an i.i.d. joint Normal
distribution, <span class="math">\(\mathcal{N}(\mu, \Sigma)\)</span>, implying that the
true regression function, i.e. conditional expectation, is affine</li>
</ul>
</li>
<li><em>Action space</em>, <span class="math">\(\mathcal{A}\)</span><ul>
<li>The action space is the set of affine functions over <span class="math">\(X\)</span>,
i.e. <span class="math">\(\mathcal{A}:= \{a \in \mathcal{F} : a(x) = \beta_0 + \beta_1\cdot x\}\)</span></li>
</ul>
</li>
<li><em>Loss function</em>, <span class="math">\(L\)</span><ul>
<li>Quadratic loss function</li>
</ul>
</li>
</ul>
<div class="math">
\[L(P, f) = \int\limits_{(Y,X)}(y - f(x))^2\mathrm{d}P(y,x)\]</div>
</div>
</div>
<div class="section" id="statistical-decision-functions">
<h1>Statistical Decision Functions<a class="headerlink" href="#statistical-decision-functions" title="Permalink to this headline">¶</a></h1>
<p>As stated before, for each potential statistical model we choose the
optimal best in-class action where optimality is gauged by the loss
function.</p>
<div class="math">
\[a^*_{L, P, \mathcal{A}} = \arg\min_{a\in\mathcal{A}}L(P,a).\]</div>
<p>If one knows the data generating process, there is no need for
statistical inference. What makes the problem statistical is that the
distribution <span class="math">\(P\)</span> describing the environment is not known. The
statistician can only base her action on the available data, which is a
realization of the underlying data generating mechanism. The time
invariant stochastic relationship between the data and the environment
allows the decision maker to carry out statistical inference regarding
the data generating process.</p>
<p>A statistical <strong>decision rule</strong> then is a function mapping samples (of
different sizes) to actions from <span class="math">\(\mathcal{A}\)</span>. In order to
flexibly talk about the behavior of decision rules as the sample size
grows, we define the domain of the decision rule to be the set of
samples of all potential sample sizes,
<span class="math">\(\mathcal{S}:= \bigcup_{n\geq1}Z^n\)</span>. The decision rule is defined
as a sequence of functions</p>
<div class="math">
\[d:\mathcal{S} \mapsto \mathcal{A} \quad \quad \text{that is} \quad \quad \{d(z^n)\}_{n\geq 1}\subseteq \mathcal{A},\quad \forall z^{\infty}\]</div>
<div class="section" id="example-cont-estimator-for-coin-tossing">
<h2>Example (cont) - estimator for coin tossing<a class="headerlink" href="#example-cont-estimator-for-coin-tossing" title="Permalink to this headline">¶</a></h2>
<p>One common way to find a decision rule is to plug the empirical
distribution <span class="math">\(P_{n}\)</span> into the loss function <span class="math">\(L(P, a)\)</span> to
obtain</p>
<div class="math">
\[L_{RE}\left(P_{n}; a\right) = \frac{1}{n}\sum_{i = 1}^{n} \log \frac{p(z_i; \theta)}{p(z_i; a)}\quad\quad\text{and}\quad\quad L_{MSE}\left(P_{n}; a\right) = \frac{1}{n}\sum_{i = 1}^{n} (z_i -a)^2\]</div>
<p>and to look for an action that minimizes this sample analog. In case of
relative entropy loss, it is</p>
<div class="math">
\[d(z^n) := \arg \min_{a} L(P_{n}, a) = \arg\max_{a} \frac{1}{n}\sum_{i=1}^{n} \log f(z_i ,a) = \arg\max_{a}  \frac{1}{n}\underbrace{\left(\sum_{i=1}^{n} z_i\right)}_{:= y}\log a + \left(\frac{n-y}{n}\right)\log(1-a)\]</div>
<p>where we define the random variable <span class="math">\(Y_n := \sum_{i = 1}^{n} Z_i\)</span>
as the number of <span class="math">\(1\)</span>s in the sample of size <span class="math">\(n\)</span>, with
<span class="math">\(y\)</span> denoting a particular realization. The solution of the above
problem is the <em>maximum likelihood estimator</em> taking the following form</p>
<div class="math">
\[\hat{a}(z^n) = \frac{1}{n}\sum_{i=1}^{n} z_i = \frac{y}{n}\]</div>
<p>and hence the <strong>maximum likelihood</strong> decision rule is</p>
<div class="math">
\[d_{mle}(z^n) = P(z, \hat{a}(z^n))\]</div>
<p>It is straightforward to see that if we used the quadratic loss instead
of relative entropy, the decision rule would be identical to
<span class="math">\(d_{mle}(z^n)\)</span>. Nonetheless, the two loss funcions can lead to
very different assessment of the decision rule as will be shown below.</p>
<p>For comparison, we consider another decision rule, a particular Bayes
estimator (posterior mean), which takes the following form</p>
<div class="math">
\[d_{bayes}(z^n) = P(z, \hat{a}_B(z^n))\quad\quad\text{where}\quad\quad \hat{a}_B(z^n) = \frac{\sum^{n}_{i=1} z_i + \alpha}{n + \alpha + \beta} = \frac{y + \alpha}{n + \alpha + \beta}\]</div>
<p>where <span class="math">\(\alpha, \beta &gt; 0\)</span> are given parameters of the Beta prior.
Later, we will see how one can derive such estimators. What is important
for us now is that this is an alternative decision rule arising from the
same triple <span class="math">\((\mathcal{H}, \mathcal{A}, L_{MSE})\)</span> as the maximum
likelihood estimator, with possibly different statistical properties.</p>
</div>
<div class="section" id="example-cont-estimator-for-linear-regression-function">
<h2>Example (cont) - estimator for linear regression function<a class="headerlink" href="#example-cont-estimator-for-linear-regression-function" title="Permalink to this headline">¶</a></h2>
<p>In this case the approach that we used to derive the maximum likelihood
estimator in the coin tossing example leads to the following sample
analog objective function</p>
<div class="math">
\[d_{OLS}(z^n):= \arg\min_{a \in \mathcal{A}}L(P_{n},a) = \arg\min_{\beta_0, \ \beta_1} \sum_{t=1}^n (y_t - \beta_0 - \beta_1\cdot x_t)^2\]</div>
<p>With a bit of an abuse of notation redefine <span class="math">\(X\)</span> to include the
constant for the intercept, i.e. <span class="math">\(\mathbf{X} = (\mathbf{1}, x^n)\)</span>.
Then the solution for the vector of coefficients,
<span class="math">\(\mathbf{\beta}=(\beta_0, \beta_1)\)</span>, in the ordinary least squares
regression is given by</p>
<div class="math">
\[\hat{\mathbf{\beta}}_{OLS} := (\mathbf{X}^T \mathbf{X})^{-1}\mathbf{X}^T \mathbf{Y}\]</div>
<p>Hence, after sample <span class="math">\(z^n\)</span>, the decision rule predicts <span class="math">\(y\)</span> as
an affine function given by <span class="math">\(d_{OLS}(z^n) = \hat{a}_{OLS}\)</span> such
that</p>
<div class="math">
\[\hat{a}_{OLS}(x) := \langle \mathbf{\hat{\beta}}_{OLS}, (1, x) \rangle.\]</div>
<p>Again, for comparison we consider a Bayesian decision rule where the
conditional prior distribution of <span class="math">\(\beta\)</span> is distributed as
<span class="math">\(\beta|\sigma \sim \mathcal{N}(\mu_b, \sigma^2\mathbf{\Lambda_b}^{-1})\)</span>.
(<span class="math">\(\sigma^2=(1-\rho^2)\sigma_Y^2\)</span> in our joint normal
specification.) Then the decision rule is given by</p>
<div class="math">
\[\hat{\mathbf{\beta}}_{bayes} := (\mathbf{X}^T \mathbf{X} + \mathbf{\Lambda_b})^{-1}(\mathbf{\Lambda_b} \mu_b + \mathbf{X}^T \mathbf{Y})\]</div>
<p>Hence, decision rule after sample <span class="math">\(z^n\)</span> is an affine function
given by <span class="math">\(d_{bayes}(z^n) = \hat{a}_{bayes}\)</span> such that</p>
<div class="math">
\[\hat{a}_{bayes}(x) := \langle \mathbf{\hat{\beta}}_{bayes}, (1, x) \rangle.\]</div>
<p>Later we will talk more about Bayes estimators and the idea behind them.</p>
</div>
</div>
<div class="section" id="induced-distributions-over-actions-and-losses">
<h1>Induced distributions over actions and losses<a class="headerlink" href="#induced-distributions-over-actions-and-losses" title="Permalink to this headline">¶</a></h1>
<p>For each realization of the sample, <span class="math">\(Z^n = z^n\)</span>, the decision rule
assigns an action <span class="math">\(d(z^n)\in\mathcal{A}\)</span> which then can be
evaluated with the loss function <span class="math">\(L(P, d(z^n))\)</span> using a particular
distribution <span class="math">\(P\)</span>. However, this does not capture the stochasticity
of the sample. It is important to assess the decision rule in
counterfactual worlds with a different realization of the sample.</p>
<p>For each probability distribution we can characterize the properties of
a decision function by considering the distribution that it induces over
losses. It is instructive to note that the decision rule in fact gives
rise to - an <em>induced distribution over the action space</em>,
<span class="math">\(\mathcal{A}\)</span> and - an <em>induced distribution over losses</em>, i.e.
<span class="math">\(\mathbb{R}_+\)</span>.</p>
<p>This approach proves to be useful as the action space can be an abstract
space with no immediate notion of metric while the range of the loss
function is always the real line (or a subset of it). In other words, a
possible way to compare different decision rules, i.e. estimators, is to
compare the distributions they induce over losses under different data
generating mechanisms for a fixed sample size.</p>
<div class="section" id="evaluating-decision-functions">
<h2>Evaluating Decision Functions<a class="headerlink" href="#evaluating-decision-functions" title="Permalink to this headline">¶</a></h2>
<p>Comparing distributions, however, is often an ambiguous task. A special
case where one could safely claim that one decision rule is better than
another is if the probability of being under a certain loss level is
always greater for one decision rule than the other. For instance, we
could say that <span class="math">\(d_1\)</span> is a better decision rule than <span class="math">\(d_2\)</span>
relative to <span class="math">\(\mathcal{H}\)</span> if for all <span class="math">\(P\in\mathcal{H}\)</span></p>
<div class="math">
\[P\{z^n: L(P, d_1(z^n)) \leq x\} \geq P\{z^n: L(P, d_2(z^n)) \leq x\} \quad \forall \ x\in\mathbb{R}\]</div>
<p>which is equivalent to stating that the induced distribution of
<span class="math">\(d_2\)</span> is first-order stochastically dominating the induced
distribution of <span class="math">\(d_1\)</span> for every <span class="math">\(P\in\mathcal{H}\)</span>. This, of
course, implies that</p>
<div class="math">
\[\mathbb{E}[L(P, d_1(z^n))] \leq \mathbb{E}[L(P, d_2(z^n))]\]</div>
<p>where the expectation is taken with respect to the sample distributed
according to <span class="math">\(P\)</span>.</p>
<p>In fact, the expected value of the induced loss is the most common
measure to evaluate decision rules. Since the loss is defined over the
real line, this measure always gives a single real number which serves
as a basis of comparison for a given data generating process. The
expected value of the loss induced by a decision rule is called <strong>the
risk</strong> of the decision rule and is denoted by</p>
<div class="math">
\[R_n(P, d) = \mathbb{E}[L(P, d(z^n))].\]</div>
<p>This functional now provides a clear and straightforward ordering of
decision rules so that <span class="math">\(d_1\)</span> is preferred to <span class="math">\(d_2\)</span> for a
given sample size <span class="math">\(n\)</span>, if
<span class="math">\(R_n(P, d_1) &lt; R_n\left(P, d_2\right)\)</span>.</p>
<p>The fundamental problem of statistical decision theory is to select a
decision rule which is optimal in terms of its risk no matter what the
true underlying <span class="math">\(P\)</span> is. However, as pointed out by Ferguson (1967)</p>
<blockquote>
<div><em>&#8220;situations in which a best decision rule exists are rare and
uninteresting&#8221;</em> (p. 28).</div></blockquote>
<p>One might use the concept of admissibility to rule out certain decision
rules but in most cases this procedure leaves plenty of competing
options to choose from. Moreover, most commonly used techniques are
based on &#8220;reasonable&#8221;, but <em>ad hoc</em> decision rules without solving
explicitly any statistical decision problem, with a prime example being
the maximum likelihood estimator. In the following, we adopt this view
and instead of focusing on various notions of optimality, we consider
statistical decision theory as a common framework in which different
approaches to constructing decision rules can be analyzed, highlighting
their relative strengths and weaknesses.</p>
<p>According to the above criterion, a good decision rule should entail
relatively small risk <span class="math">\(R_n(P, d)\)</span> for the sample size at hand.
Although the desirability of this rule is hard to deny, the apparent
difficulty is that this criterion hinges on an unknown object,
<span class="math">\(P\)</span>. Later (in another notebook - link!!!) we will consider three
approaches, each of them having alternative ways to handle the ignorance
about the true risk.</p>
<ol class="arabic simple">
<li><strong>Classical approach:</strong> where the main assessment of a decision rule
is based on its asymptotic properties</li>
<li><strong>Bayesian approach:</strong> where the ignorance about <span class="math">\(P\)</span> is
resolved by the use of a prior</li>
<li><strong>Statistical learning (minimax) approach:</strong> where a decision rule is
judged according to its performance under the least favorable
(worst-case) distribution</li>
</ol>
</div>
<div class="section" id="example-cont-induced-distributions-for-coin-tossing">
<h2>Example (cont) - induced distributions for coin tossing<a class="headerlink" href="#example-cont-induced-distributions-for-coin-tossing" title="Permalink to this headline">¶</a></h2>
<p>Take the case when the true data generating process is indeed i.i.d.
Bernoulli (correct specification) with * <span class="math">\(\theta_0 = 0.79\)</span> *
<span class="math">\(n = 25\)</span></p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">theta0</span> <span class="o">=</span> <span class="o">.</span><span class="mi">79</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">25</span>
<span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span>

<span class="k">def</span> <span class="nf">relative_entropy</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">theta0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta0</span><span class="o">/</span><span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta0</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta0</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">quadratic_loss</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">theta0</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span> <span class="nf">loss_distribution</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">dr</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">true_dist</span><span class="p">,</span> <span class="n">theta0</span><span class="p">,</span> <span class="n">y_grid</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Uses the formula for the change of discrete random variable. It takes care of the</span>
<span class="sd">    fact that relative entropy is not monotone.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">16</span>
    <span class="k">if</span> <span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;relative_entropy&#39;</span><span class="p">:</span>
        <span class="n">a1</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">bisect</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">:</span> <span class="n">relative_entropy</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">-</span> <span class="n">l</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">eps</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">theta0</span><span class="p">)</span>
        <span class="n">a2</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">bisect</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">:</span> <span class="n">relative_entropy</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">-</span> <span class="n">l</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">theta0</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">eps</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;quadratic&#39;</span><span class="p">:</span>
        <span class="n">a1</span> <span class="o">=</span> <span class="n">theta0</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
        <span class="n">a2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="o">-</span> <span class="n">theta0</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">dr</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
        <span class="n">y1</span> <span class="o">=</span> <span class="n">y_grid</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">dr</span><span class="p">)][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">prob1</span> <span class="o">=</span> <span class="n">true_dist</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">y1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">prob1</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">a2</span><span class="p">,</span> <span class="n">dr</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
        <span class="n">y2</span> <span class="o">=</span> <span class="n">y_grid</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">a2</span><span class="p">,</span> <span class="n">dr</span><span class="p">)][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">prob2</span> <span class="o">=</span> <span class="n">true_dist</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">y2</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">prob2</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">):</span>
        <span class="c1"># around zero loss, the two sides might find the same a</span>
        <span class="k">return</span> <span class="n">prob1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">prob1</span> <span class="o">+</span> <span class="n">prob2</span>

<span class="k">def</span> <span class="nf">risk_quadratic</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    See Casella and Berger, p.332</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">first_term</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">theta0</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta0</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">+</span> <span class="n">n</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">second_term</span> <span class="o">=</span> <span class="p">((</span><span class="n">n</span> <span class="o">*</span> <span class="n">theta0</span> <span class="o">+</span> <span class="n">alpha</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">+</span> <span class="n">n</span><span class="p">)</span> <span class="o">-</span> <span class="n">theta0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

    <span class="k">return</span> <span class="n">first_term</span> <span class="o">+</span> <span class="n">second_term</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">theta0_slider</span> <span class="o">=</span> <span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="nb">max</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">step</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">theta0</span><span class="p">)</span>
<span class="n">n_slider</span> <span class="o">=</span> <span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="nb">max</span> <span class="o">=</span> <span class="mi">100</span> <span class="p">,</span> <span class="n">step</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">n</span><span class="p">)</span>

<span class="nd">@interact</span><span class="p">(</span><span class="n">theta0</span> <span class="o">=</span> <span class="n">theta0_slider</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">n_slider</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">example1</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">18</span><span class="p">))</span>

    <span class="n">true_dist</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">theta0</span><span class="p">)</span>

    <span class="n">y_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>                       <span class="c1"># sum of ones in a sample</span>
    <span class="n">a_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>                 <span class="c1"># action space represented as [0, 1]</span>
    <span class="n">rel_ent</span> <span class="o">=</span> <span class="n">relative_entropy</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">a_grid</span><span class="p">)</span>      <span class="c1"># form of the loss function</span>
    <span class="n">quadratic</span> <span class="o">=</span> <span class="n">quadratic_loss</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">a_grid</span><span class="p">)</span>      <span class="c1"># form of the loss function</span>

    <span class="c1"># The two decision functions (as a function of Y)</span>
    <span class="n">decision_rule</span> <span class="o">=</span> <span class="n">y_grid</span><span class="o">/</span><span class="n">n</span>
    <span class="n">decision_rule_bayes</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_grid</span> <span class="o">+</span> <span class="n">alpha</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="p">)</span>

    <span class="n">loss_re_mle</span> <span class="o">=</span> <span class="n">relative_entropy</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">decision_rule</span><span class="p">)</span>
    <span class="n">loss_re_bayes</span> <span class="o">=</span> <span class="n">relative_entropy</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">decision_rule_bayes</span><span class="p">)</span>
    <span class="n">loss_quad_mle</span> <span class="o">=</span> <span class="n">quadratic_loss</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">decision_rule</span><span class="p">)</span>
    <span class="n">loss_quad_bayes</span> <span class="o">=</span> <span class="n">quadratic_loss</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">decision_rule_bayes</span><span class="p">)</span>

    <span class="n">loss_dist_re_mle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">loss_distribution</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">decision_rule</span><span class="p">,</span> <span class="s2">&quot;relative_entropy&quot;</span><span class="p">,</span> <span class="n">true_dist</span><span class="p">,</span> <span class="n">theta0</span><span class="p">,</span> <span class="n">y_grid</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">loss_re_mle</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
    <span class="n">loss_dist_re_mle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">true_dist</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">y_grid</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">loss_dist_re_mle</span><span class="p">,</span> <span class="n">true_dist</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">y_grid</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])])</span>
    <span class="n">loss_dist_re_bayes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">loss_distribution</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">decision_rule_bayes</span><span class="p">,</span> <span class="s2">&quot;relative_entropy&quot;</span><span class="p">,</span> <span class="n">true_dist</span><span class="p">,</span> <span class="n">theta0</span><span class="p">,</span> <span class="n">y_grid</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">loss_re_bayes</span><span class="p">])</span>

    <span class="n">loss_dist_quad_mle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">loss_distribution</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">decision_rule</span><span class="p">,</span> <span class="s2">&quot;quadratic&quot;</span><span class="p">,</span> <span class="n">true_dist</span><span class="p">,</span> <span class="n">theta0</span><span class="p">,</span> <span class="n">y_grid</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">loss_quad_mle</span><span class="p">])</span>
    <span class="n">loss_dist_quad_bayes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">loss_distribution</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">decision_rule_bayes</span><span class="p">,</span> <span class="s2">&quot;quadratic&quot;</span><span class="p">,</span> <span class="n">true_dist</span><span class="p">,</span> <span class="n">theta0</span><span class="p">,</span> <span class="n">y_grid</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">loss_quad_bayes</span><span class="p">])</span>

    <span class="n">risk_re_mle</span> <span class="o">=</span> <span class="n">loss_re_mle</span> <span class="o">@</span> <span class="n">loss_dist_re_mle</span>
    <span class="n">risk_re_bayes</span> <span class="o">=</span> <span class="n">loss_re_bayes</span> <span class="o">@</span> <span class="n">loss_dist_re_bayes</span>

    <span class="n">risk_quad_mle</span> <span class="o">=</span> <span class="n">risk_quadratic</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">risk_quad_bayes</span> <span class="o">=</span> <span class="n">risk_quadratic</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;True distribution over Y (number of 1s in the sample)&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_grid</span><span class="p">,</span> <span class="n">true_dist</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">y_grid</span><span class="p">),</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">3</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">y_grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">true_dist</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">y_grid</span><span class="p">),</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">3</span><span class="p">],</span> <span class="n">alpha</span> <span class="o">=</span> <span class="o">.</span><span class="mi">7</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Loss functions over the action space&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">a_grid</span><span class="p">,</span> <span class="n">rel_ent</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;relative entropy loss&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">a_grid</span><span class="p">,</span> <span class="n">quadratic</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;quadratic loss&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">2</span><span class="p">],</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">r&#39;True $\theta_0$&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Distribution of the MLE estimator over the action space&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">decision_rule</span><span class="p">,</span> <span class="n">true_dist</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">y_grid</span><span class="p">),</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">decision_rule</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">true_dist</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">y_grid</span><span class="p">),</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">2</span><span class="p">],</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">r&#39;True $\theta_0$&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Distribution of the Bayes estimator over the action space&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">decision_rule_bayes</span><span class="p">,</span> <span class="n">true_dist</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">y_grid</span><span class="p">),</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">decision_rule_bayes</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">true_dist</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">y_grid</span><span class="p">),</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span>
                    <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">2</span><span class="p">],</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">r&#39;True $\theta_0$&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Distribution of entropy loss (MLE estimator)&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">loss_re_mle</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">loss_dist_re_mle</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">9</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">risk_re_mle</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span>
                     <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;Entropy risk&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Distribution of entropy loss (Bayes estimator)&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">loss_re_bayes</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">loss_dist_re_bayes</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">9</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">risk_re_bayes</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span>
                     <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;muted&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;Entropy risk&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Distribution of quadratic loss (MLE estimator)&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">loss_quad_mle</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">loss_dist_quad_mle</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">9</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">risk_quad_mle</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span>
                     <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;Quadratic risk&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">05</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Distribution of quadratic loss (Bayes estimator)&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">loss_quad_bayes</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">loss_dist_quad_bayes</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">9</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">risk_quad_bayes</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span>
                     <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;muted&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;Quadratic risk&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">05</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">22</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../_images/Statistical_decision_functions_16_0.png" src="../_images/Statistical_decision_functions_16_0.png" />
<p><strong>NOTE</strong>: we should break this figure into separate pieces and compare the objects using fixed parameter values (no slider)</p>
</div>
<div class="section" id="discussion">
<h2>Discussion<a class="headerlink" href="#discussion" title="Permalink to this headline">¶</a></h2>
<p>Role of the loss function</p>
<ul class="simple">
<li>For all sample sizes, the probability mass function of the MLE
estimator assigns positive probability to both <span class="math">\(\theta=0\)</span> and
<span class="math">\(\theta = 1\)</span>, whereas the support of the Bayes estimator lies
always in the interior <span class="math">\((0, 1)\)</span>. This difference has
significant consequences for the relative entropy risk, because
<span class="math">\(L_{RE}\)</span> is not defined (or it takes infinity) at the bounaries
of <span class="math">\([0, 1]\)</span>. As a result, the relative entopy risk of the MLE
estimator does not exist and so the Bayes estimator always wins in
terms of realative entropy. The secret of <span class="math">\(d_{bayes}\)</span> is to
shrink the effective action space.</li>
</ul>
<p>Bias vs. variance</p>
<ul class="simple">
<li>The MLE estimator is unbiased in the sense that its mean always
coincide with the true <span class="math">\(\theta_0\)</span>. In contrast, the Bayes
estimator is biased - the extent of which depends on the relationship
between the prior parameters and the true value. Notice, however,
that <span class="math">\(d_{bayes}\)</span> is less dispered, the values to which it
assigns positive probability are more densely placed in
<span class="math">\([0, 1]\)</span>. Exploiting this trade-off between bias and variance
will be a crucial device in finding decision rules with low risk.</li>
</ul>
<p>Performance of the decision rules depends on the true data generating
mechanism</p>
<ul class="simple">
<li>Comparing the decision rules in terms of the quadratic loss reveals
that the true <span class="math">\(\theta_0\)</span> is a critical factor. It determines
the size of the bias (hence the risk) of the Bayes estimator. Since
<span class="math">\(\theta_0\)</span> is unknown, this naturally introduces a subjective
(not data driven) element into our analysis: when the prior happens
to concentrate around the true <span class="math">\(\theta_0\)</span> the Bayes estimator
performs better thant the MLE, otherwise the bias could be so large
that it flips the ordering of decision rules.</li>
</ul>
</div>
<div class="section" id="example-cont-induced-distributions-for-linear-regression">
<h2>Example (cont) - induced distributions for linear regression<a class="headerlink" href="#example-cont-induced-distributions-for-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>Take the simple case when the data is i.i.d. and</p>
<ul class="simple">
<li><span class="math">\((Y,X) \sim \mathcal{N}(\mu, \Sigma)\)</span> where<ul>
<li><span class="math">\(\mu = (1, 3)\)</span></li>
<li><span class="math">\(\Sigma = \begin{bmatrix} 4 &amp; 1 \\ 1 &amp; 8 \end{bmatrix}\)</span></li>
</ul>
</li>
<li><span class="math">\(n=50\)</span></li>
</ul>
<p>In this case the optimal regression function is affine (correct
specification) and the coefficients are given by</p>
<div class="math">
\[\begin{split}\beta_0 &amp;= \mu_Y - \rho\frac{\sigma_Y}{\sigma_X}\mu_X = 1 - \frac{1}{8} 3 = -0.625 \\
\beta_1 &amp;= \rho\frac{\sigma_Y}{\sigma_X} = \frac{1}{8} = 0.125\end{split}\]</div>
<p>For the Bayes estimator consider</p>
<blockquote>
<div><ul class="simple">
<li>the prior mean being <span class="math">\(\mu_b = (2, 2)\)</span></li>
<li>the precision matrix being <span class="math">\(\Lambda_b = \begin{bmatrix} 6 &amp; -3 \\ -3 &amp; 6 \end{bmatrix}\)</span></li>
</ul>
</div></blockquote>
<p>With the given specification we can simulate the induced distributions
over actions and over losses.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>                    <span class="c1"># mean</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>       <span class="c1"># covariance matrix</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>                                   <span class="c1"># sample size</span>

<span class="c1"># Bayes priors</span>
<span class="n">mu_bayes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">precis_bayes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="c1"># joint normal rv for (Y,X)</span>
<span class="n">mvnorm</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

<span class="c1"># decision rule -- OLS estimator</span>
<span class="k">def</span> <span class="nf">d_OLS</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">Z</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">Z</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Y</span>

<span class="c1"># decision rule -- Bayes</span>
<span class="k">def</span> <span class="nf">d_bayes</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">Z</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">Z</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span> <span class="o">+</span> <span class="n">precis_bayes</span><span class="p">)</span> <span class="o">@</span> <span class="p">(</span><span class="n">precis_bayes</span> <span class="o">@</span> <span class="n">mu_bayes</span> <span class="o">+</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Y</span><span class="p">)</span>

<span class="c1"># loss -- define integrand</span>
<span class="k">def</span> <span class="nf">loss_int</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Defines the integrand under mvnorm distribution.&#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">*</span><span class="n">mvnorm</span><span class="o">.</span><span class="n">pdf</span><span class="p">((</span><span class="n">y</span><span class="p">,</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># simulate distribution over actions and over losses</span>
<span class="n">B_OLS</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">L_OLS</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">B_bayes</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">L_bayes</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="c1"># generate sample</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">mvnorm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

    <span class="c1"># get OLS action corrsponding to realized sample</span>
    <span class="n">b_OLS</span> <span class="o">=</span> <span class="n">d_OLS</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

    <span class="c1"># get Bayes action</span>
    <span class="n">b_bayes</span> <span class="o">=</span> <span class="n">d_bayes</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

    <span class="c1"># get loss through integration</span>
    <span class="n">l_OLS</span> <span class="o">=</span> <span class="n">dblquad</span><span class="p">(</span><span class="n">loss_int</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">b_OLS</span><span class="p">,))</span> <span class="c1"># get loss</span>
    <span class="n">l_bayes</span> <span class="o">=</span> <span class="n">dblquad</span><span class="p">(</span><span class="n">loss_int</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">b_bayes</span><span class="p">,))</span> <span class="c1"># get loss</span>

    <span class="c1"># record action</span>
    <span class="n">B_OLS</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">b_OLS</span><span class="p">)</span>
    <span class="n">B_bayes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">b_bayes</span><span class="p">)</span>

    <span class="c1"># record loss</span>
    <span class="n">L_OLS</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">l_OLS</span><span class="p">)</span>
    <span class="n">L_bayes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">l_bayes</span><span class="p">)</span>

<span class="c1"># take first column if integrating</span>
<span class="n">L_OLS</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">L_OLS</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">L_bayes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">L_bayes</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">B_OLS</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">B_OLS</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">beta_0$&quot;</span><span class="p">,</span> <span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">beta_1$&quot;</span><span class="p">])</span>
<span class="n">B_bayes</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">B_bayes</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">beta_0$&quot;</span><span class="p">,</span> <span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">beta_1$&quot;</span><span class="p">])</span>

<span class="n">g1</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">jointplot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">beta_0$&quot;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">beta_1$&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">B_OLS</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;kde&quot;</span><span class="p">,</span> <span class="n">space</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="p">,</span> <span class="n">xlim</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">ylim</span> <span class="o">=</span> <span class="p">(</span><span class="o">-.</span><span class="mi">2</span><span class="p">,</span> <span class="o">.</span><span class="mi">4</span><span class="p">))</span>
<span class="n">g1</span><span class="o">.</span><span class="n">ax_joint</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">sigma</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">]],[</span><span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">sigma</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;best in class&#39;</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_axis_labels</span><span class="p">(</span><span class="s1">r&#39;$\beta_0$&#39;</span><span class="p">,</span> <span class="s1">r&#39;$\beta_1$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Distribution over the action space -- OLS&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.08</span><span class="p">)</span>

<span class="n">g2</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">jointplot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">beta_0$&quot;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">beta_1$&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">B_bayes</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;kde&quot;</span><span class="p">,</span> <span class="n">space</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="p">,</span> <span class="n">xlim</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">ylim</span> <span class="o">=</span> <span class="p">(</span><span class="o">-.</span><span class="mi">2</span><span class="p">,</span> <span class="o">.</span><span class="mi">4</span><span class="p">))</span>
<span class="n">g2</span><span class="o">.</span><span class="n">ax_joint</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">sigma</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">]],[</span><span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">sigma</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;best in class&#39;</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_axis_labels</span><span class="p">(</span><span class="s1">r&#39;$\beta_0$&#39;</span><span class="p">,</span> <span class="s1">r&#39;$\beta_1$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Distribution over the action space -- Bayes&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.08</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../_images/Statistical_decision_functions_20_0.png"><img alt="../_images/Statistical_decision_functions_20_0.png" src="../_images/Statistical_decision_functions_20_0.png" style="width: 423.3px; height: 468.34999999999997px;" /></a>
<a class="reference internal image-reference" href="../_images/Statistical_decision_functions_20_1.png"><img alt="../_images/Statistical_decision_functions_20_1.png" src="../_images/Statistical_decision_functions_20_1.png" style="width: 423.3px; height: 468.34999999999997px;" /></a>
<p>The best in class action in the normal case is the affine function with
coefficients <span class="math">\((\beta_0, \beta_1) = (-.625, .125)\)</span>. This action is
depicted by a red dot on the graphs above. We can compute the
corresponding loss &#8211; the minimum loss attainable with the actions in
<span class="math">\(\mathcal{A}\)</span> &#8211; as follows.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">b_best</span> <span class="o">=</span> <span class="p">[</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">sigma</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">sigma</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">l_best</span> <span class="o">=</span> <span class="n">dblquad</span><span class="p">(</span><span class="n">loss_int</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">b_best</span><span class="p">,))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">l_best</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mf">3.8749999999509477</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">L_OLS</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;OLS&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">L_bayes</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Bayes&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">L_OLS</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Risk of OLS&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">L_bayes</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">3</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Risk of Bayes&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">l_best</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">4</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Lowest in class loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Distribution over losses&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">18</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../_images/Statistical_decision_functions_23_0.png" src="../_images/Statistical_decision_functions_23_0.png" />
</div>
<div class="section" id="id1">
<h2>Discussion<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>The joint plots for the distributions over actions illustrate that
the Bayes actions have a bigger bias relative to the OLS ones &#8211; in
fact we know that the OLS estimates are unbiased.</li>
</ul>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">beta_0</span> <span class="o">=</span> <span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">sigma</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">beta_1</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">sigma</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Bias of OLS&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;==========================&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{:.4f}</span><span class="s1"> - </span><span class="si">{:.4f}</span><span class="s1"> = </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">beta_0</span><span class="p">,</span> <span class="n">B_OLS</span><span class="o">.</span><span class="n">mean</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">beta_0</span> <span class="o">-</span> <span class="n">B_OLS</span><span class="o">.</span><span class="n">mean</span><span class="p">()[</span><span class="mi">0</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{:.4f}</span><span class="s1"> - </span><span class="si">{:.4f}</span><span class="s1"> = </span><span class="si">{:.4f}</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">beta_1</span><span class="p">,</span> <span class="n">B_OLS</span><span class="o">.</span><span class="n">mean</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">beta_1</span> <span class="o">-</span> <span class="n">B_OLS</span><span class="o">.</span><span class="n">mean</span><span class="p">()[</span><span class="mi">1</span><span class="p">]))</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Bias of Bayes&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;==========================&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{:.4f}</span><span class="s1"> - </span><span class="si">{:.4f}</span><span class="s1"> = </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">beta_0</span><span class="p">,</span> <span class="n">B_bayes</span><span class="o">.</span><span class="n">mean</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">beta_0</span> <span class="o">-</span> <span class="n">B_bayes</span><span class="o">.</span><span class="n">mean</span><span class="p">()[</span><span class="mi">0</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{:.4f}</span><span class="s1"> - </span><span class="si">{:.4f}</span><span class="s1"> = </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">beta_1</span><span class="p">,</span> <span class="n">B_bayes</span><span class="o">.</span><span class="n">mean</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">beta_1</span> <span class="o">-</span> <span class="n">B_bayes</span><span class="o">.</span><span class="n">mean</span><span class="p">()[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">Bias</span> <span class="n">of</span> <span class="n">OLS</span>
<span class="o">==========================</span>
<span class="mf">0.6250</span> <span class="o">-</span> <span class="mf">0.6358</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.0108</span>
<span class="mf">0.1250</span> <span class="o">-</span> <span class="mf">0.1223</span> <span class="o">=</span> <span class="mf">0.0027</span>


<span class="n">Bias</span> <span class="n">of</span> <span class="n">Bayes</span>
<span class="o">==========================</span>
<span class="mf">0.6250</span> <span class="o">-</span> <span class="mf">0.6786</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.0536</span>
<span class="mf">0.1250</span> <span class="o">-</span> <span class="mf">0.1235</span> <span class="o">=</span> <span class="mf">0.0015</span>
</pre></div>
</div>
<ul class="simple">
<li>On the other hand the Bayes actions have smaller variance relative to
the OLS actions.</li>
</ul>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Variance of OLS&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;======================&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">B_OLS</span><span class="o">.</span><span class="n">var</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">Varinace of Bayes&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;======================&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">B_bayes</span><span class="o">.</span><span class="n">var</span><span class="p">())</span>
</pre></div>
</div>
<pre class="literal-block">
Variance of OLS
======================
$beta_0$    0.182442
$beta_1$    0.010429
dtype: float64


Varinace of Bayes
======================
$beta_0$    0.103671
$beta_1$    0.007337
dtype: float64
</pre>
<ul class="simple">
<li>In terms of the expected loss the slightly bigger bias of the Bayes
estimate is more compensated by the lower variance. The risk of the
Bayes decision rule is lower than that of the OLS.</li>
</ul>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Risk of OLS:   </span><span class="si">{:.4f}</span><span class="s1"> </span><span class="se">\n</span><span class="s1">Risk of Bayes: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">L_OLS</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">L_bayes</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">Risk</span> <span class="n">of</span> <span class="n">OLS</span><span class="p">:</span>   <span class="mf">4.0338</span>
<span class="n">Risk</span> <span class="n">of</span> <span class="n">Bayes</span><span class="p">:</span> <span class="mf">4.0007</span>
</pre></div>
</div>
<ul class="simple">
<li>The feature of the true DGP lies within the action space and the
model is very &#8220;simple&#8221;, hence it&#8217;s difficult to beat the OLS (we need
small sample and large noise). With more complex models this might
not be the case.</li>
</ul>
</div>
</div>
<div class="section" id="misspecification-and-the-bias-variance-dilemma">
<h1>Misspecification and the bias-variance dilemma<a class="headerlink" href="#misspecification-and-the-bias-variance-dilemma" title="Permalink to this headline">¶</a></h1>
<p>In the above examples we maintained the assumption that the true feature
of the data generating process (probability mass function of <span class="math">\(Z\)</span>
or conditional expectation of <span class="math">\(Y\)</span> given <span class="math">\(X\)</span>) lies within the
specified action set. In applications hinging on nonexperimental data,
however, it is more reasonable to assume that the action set contains
only approximations of the true feature. We say that the model is
misspecified if the action space does not contain the feature of the
true data generating process, i.e.,
<span class="math">\(\gamma(P) \not \in \mathcal{A}\)</span>.</p>
<p>Nothing in the analysis above prevents us to entertain the possibility
of misspecification. In these instances the best in class action is only
an approximation of the true feature. For example, the true regression
function might not be linear however the exercise of estimating a linear
approximation of the regression function is completely well defined.</p>
<p>One could measure this approximation error via the loss function without
introducing the inference problem. The <strong>approximation error</strong>
quantified via the loss is given by</p>
<div class="math">
\[\min_{a\in\mathcal{A}} L(P,a) - L(P, \gamma(P)) \quad\quad (2)\]</div>
<p>This naturally leads to a dilemma regarding the &#8220;size&#8221; of the action
space. In principle, with a relatively rich <span class="math">\(\mathcal{A}\)</span>, we can
get closer to the truth by making the the difference in (2) small.
Recall, however, that we do not know the best-in class action
<span class="math">\(a^{*}_{L, P, \mathcal{A}}\)</span> either. Although, we will see (in the
next notebook) that by requiring the property of <em>consistency</em> from our
decision rules, we can guarantee to get close to
<span class="math">\(a^{*}_{L, P, \mathcal{A}}\)</span> at least with a sufficiently large
sample, the neccesary sample size will itself depend on the size of
<span class="math">\(\mathcal{A}\)</span>. With a too complex <span class="math">\(\mathcal{A}\)</span>, the
estimation of the best-in class action can be so bad for reasonable
sample sizes that misspecification concerns become secondary.</p>
<p>This trade-off closely resembles the bias-variance dilemma well-known
from classical statistics. To see this, decompose the excessive risk of
a decision rule <span class="math">\(d\)</span> for a given sample size <span class="math">\(n\)</span> (relative to
the value of loss at the truth) as</p>
<div class="math">
\[R_n(P, d) - L\left(P, \gamma(P) \right) = \underbrace{R_n(P, d) - L\left(P, a^{*}_{L,P, \mathcal{A}}\right)}_{\text{estimation error}} + \underbrace{L\left(P, a^{*}_{L, P, \mathcal{A}}\right)- L\left(P, \gamma(P)\right)}_{\text{approximation error}}\]</div>
<p>The <strong>estimation error</strong> stems from the fact that we do not know
<span class="math">\(P\)</span>, so we have to use a finite sample to approximate the best
in-class action. The second term, which is not influenced by any random
object, is the same quantity as the difference in (2).</p>
<p>The estimation error can be viewed as the variance, while the
approximation error is associated with the bias of the decision rule.
One of the ways of balancing the trade-off between bias and variance is
through changing the action space <span class="math">\(\mathcal{A}\)</span>. If
<span class="math">\(\mathcal{A}\)</span> is &#8220;small&#8221;, then the variance (estimation error) is
small, while the bias (approximation error) is large, leading to
underfitting. On the other hand, with a rich <span class="math">\(\mathcal{A}\)</span>, the
estimation error might get &#8220;too&#8221; large, implying overfitting.</p>
<div class="section" id="a-warning">
<h2>A warning<a class="headerlink" href="#a-warning" title="Permalink to this headline">¶</a></h2>
<p>The introduced notion of misspecification is a <em>statistical</em> one. From a
modeler&#8217;s point of view, a natural question to ask is to what extent
misspecification affects the economic interpretation of the parameters
of a fitter statistical model. Intuitively, a necessary condition for
the economic interpretation is to have a correctly specified statistical
model. Because different economic models can give rise to the same
statistical model, this condition is by no means sufficient. From this
angle, a misspecified statistical model can easily invalidate any kind
of economic interpretation of estimated parameters. This issue is more
subtle and it would require an extensive treatment that we cannot
deliver here, but it is worth keeping in mind the list of very strong
assumptions necessary to give well-defined meaning to the parameters
that we seek to estimate. An interesting discussion can be found in
Chapter 4 of White (1994).</p>
</div>
</div>
<div class="section" id="references">
<h1>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h1>
<p>Breiman, Leo (1969). Probability and Stochastic Processes: With a View
Towards Applications. Houghton Mifflin</p>
<p>Ferguson, Thomas S. (1967). Mathematical Statistics: A Decision
Theoretic Approach. Academic Press</p>
<p>Manski, C. (1994). Analog estimation of econometric models. In: Engle
III, R.F.,McFadden, D.F. (Eds.) Handbook of Econometrics, vol. 4.
North-Holland, Amsterdam.</p>
<p>White, Halbert (1994), Estimation, Inference and Specification Analysis
(Econometric Society Monographs). Cambridge University Press</p>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Estimators as Statistical Decision Functions</a></li>
<li><a class="reference internal" href="#introduction">Introduction</a><ul>
<li><a class="reference internal" href="#stationarity-and-statistical-models">Stationarity and statistical models</a><ul>
<li><a class="reference internal" href="#dependence">Dependence</a></li>
<li><a class="reference internal" href="#true-data-generating-mechanism">True data generating mechanism</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#primitives-of-the-problem">Primitives of the problem</a><ul>
<li><a class="reference internal" href="#examples">Examples</a></li>
<li><a class="reference internal" href="#best-in-class-action">Best in-class action</a></li>
<li><a class="reference internal" href="#features">Features</a></li>
<li><a class="reference internal" href="#example-coin-tossing">Example - Coin tossing</a></li>
<li><a class="reference internal" href="#example-linear-regression-function">Example - Linear regression function</a></li>
</ul>
</li>
<li><a class="reference internal" href="#statistical-decision-functions">Statistical Decision Functions</a><ul>
<li><a class="reference internal" href="#example-cont-estimator-for-coin-tossing">Example (cont) - estimator for coin tossing</a></li>
<li><a class="reference internal" href="#example-cont-estimator-for-linear-regression-function">Example (cont) - estimator for linear regression function</a></li>
</ul>
</li>
<li><a class="reference internal" href="#induced-distributions-over-actions-and-losses">Induced distributions over actions and losses</a><ul>
<li><a class="reference internal" href="#evaluating-decision-functions">Evaluating Decision Functions</a></li>
<li><a class="reference internal" href="#example-cont-induced-distributions-for-coin-tossing">Example (cont) - induced distributions for coin tossing</a></li>
<li><a class="reference internal" href="#discussion">Discussion</a></li>
<li><a class="reference internal" href="#example-cont-induced-distributions-for-linear-regression">Example (cont) - induced distributions for linear regression</a></li>
<li><a class="reference internal" href="#id1">Discussion</a></li>
</ul>
</li>
<li><a class="reference internal" href="#misspecification-and-the-bias-variance-dilemma">Misspecification and the bias-variance dilemma</a><ul>
<li><a class="reference internal" href="#a-warning">A warning</a></li>
</ul>
</li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="../index.html" title="previous chapter">Welcome to econometrics&#8217;s documentation!</a></li>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/Notebook_01_Wald/Statistical_decision_functions.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2016, Daniel Csaba, Balint Szoke.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.4.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.8</a>
      
      |
      <a href="../_sources/Notebook_01_Wald/Statistical_decision_functions.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>