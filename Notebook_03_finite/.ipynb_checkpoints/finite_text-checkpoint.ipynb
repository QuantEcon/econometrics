{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coping with finite samples\n",
    "\n",
    "#### December 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From asymptotics to finite samples\n",
    "\n",
    "In notebook {reference asymptotics} we took an asymptotic approach in that we focused on how the decision rule behaves as the sample size grows to infinity. The main features we set as good measures of behavior were consistency and fast rate of convergence.\n",
    "\n",
    "The important conclusion we draw was that both consistency and the rate of convergence are driven by the complexity of the composite loss-action space, $\\mathcal{L}$. Finiteness of model complexity ensures consistency and given consistency, the size of the complexity affects the rate of convergence.\n",
    "\n",
    "This notebook explores implications of dealing with a finite sample size relative to the infinite, asymptotic case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation-approximation decomposition  \n",
    "\n",
    "We saw that a consistent decision rule, $d: \\mathcal{S} \\mapsto \\mathcal{A}$, satisfies $L(P, d(z^n)) \\overset{P}{\\to} L(P, a^*_{L, P, \\mathcal{A}})$, hence, the associated loss has a degenerate asymptotic distribution centered around the loss of the best-in-class action (\"asymptotic risk\"). One of the central objects of the finite sample analysis concerns how the finite sample risk deviates from this (asymptotic) best-in-class loss, the so called **estimation error**\n",
    "\n",
    "$$ R_n(P, d) - L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right) $$\n",
    "\n",
    "More generally, one would like to assess the performance of decision rule $d$ relative to the true feature $\\gamma(P)=a^*_{L, P, \\mathcal{F}}$ of the DGP, with $\\mathcal{F}$ denoting the set of all admissible actions, in which case the variable of interest is the **excess risk** \n",
    "\n",
    "$$ R_n(P, d) - L\\left(P, a^*_{L, P, \\mathcal{F}} \\right) $$\n",
    "\n",
    "Decomposing this excess risk highlights one of the most important tensions underlying finite sample inference problems. We can write\n",
    "\n",
    "$$ R_n(P, d) - L\\left(P, a^*_{L, P, \\mathcal{F}}  \\right) =  \\underbrace{R_n(P, d) - L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)}_{\\substack{\\text{estimation error} \\\\ \\text{random}}} + \\underbrace{L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)- L\\left(P, a^*_{L, P, \\mathcal{F}}  \\right)}_{\\substack{\\text{approximation error} \\\\ \\text{deterministic}}}. $$\n",
    "\n",
    "As we noted earlier the **approximation error** usually referred to as *misspecification* is stemming from the fact that the true feature might lie outside of the action space. Intuitively, as we enlarge the action space $\\mathcal{A}$, the approximation error gets weakly smaller and the estimation error gets weakly larger. An ideal decision rule balances this trade-off.\n",
    "\n",
    "Much of what follows next concerns the behavior of the terms in the above decomposition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-volatility-misspecification decomposition \n",
    "\n",
    "A well known concept in statistics is the bias-variance trade-off concerning the estimation error of a decision rule. Here we give a slightly more general version which accommodates the possibility of misspecification.\n",
    "\n",
    "TAYLOR \n",
    "\n",
    "\n",
    "We arrive at the decomposition of excess risk\n",
    "\n",
    "$$ R_n(P, d) - L\\left(P, a^*_{L, P, \\mathcal{F}} \\right) = \\underbrace{R_n\\left(P, d\\right) - L\\left(P, \\bar d_n \\right)}_{\\text{volatility}} + \\underbrace{L\\left(P, \\bar{d_n}\\right) - L\\left(P, a^{*}_{L,P,  \\mathcal{A}}\\right)}_{\\text{bias}} + \\underbrace{L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)- L\\left(P,a^*_{L, P, \\mathcal{F}}  \\right)}_{\\text{misspecification}} $$\n",
    "\n",
    "Note that while the first two terms depend on the sample size $n$, the misspecification term remains constant given that the action space $\\mathcal{A}$ is kept fixed.\n",
    "* If the decision rule $d$ is consistent relative to $(\\mathcal{H}, \\mathcal{A})$, the *bias* converges to zero as $n$ goes to infinty \n",
    "* If the decision rule $d$ is consistent relative to $(\\mathcal{H}, \\mathcal{A})$, the *volatility* converges to zero as $n$ goes to infinty\n",
    "\n",
    "**TOADD?**: stylized figure from the mostafa book (after bias-variance trade-off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of the bias-volatility-misspecification trade-off\n",
    "\n",
    "### 1. Quadratic loss\n",
    "\n",
    "The elements of the problem are\n",
    "\n",
    "* *Observable:* $Z = (Y,X)$\n",
    "* *Loss function:* $L(P, a) = \\int_Z (y - a(x))^2 \\mathrm{d}P(z)$ \n",
    "* *Admissible space:* $\\mathcal{F}\\equiv \\{ a: x \\mapsto y \\ |  \\ a \\ \\text{is measurable}\\}$ \n",
    "* *True feature:* $\\gamma(P) = \\mathbb{E}_P[Y|X] = \\inf_{a\\in \\mathcal{F}} \\ L(P, a)$ \n",
    "\n",
    "Then the minimal loss can be written as \n",
    "\n",
    "$$L(P, a^*_{P, \\mathcal{F}}) = \\int_Z \\underbrace{(y - \\mathbb{E}_P[Y|X](x))^2}_{=\\sigma^2_x \\ \\ \\text{(noise)}} \\mathrm{d}P(z) = \\mathbb{E}_P[\\sigma^2_x]$$\n",
    "\n",
    "the loss evaluated at the best-in-class action, $a^*_{P, \\mathcal{A}}(x)= \\inf_{a\\in\\mathcal{A}} \\ L(P, a)$, is\n",
    "\n",
    "$$L\\left(P, a^*_{P, \\mathcal{A}}\\right) = \\mathbb{E}_P[\\sigma^2_x] + \\int_Z \\underbrace{\\left[\\mathbb{E}_P[Y|X](x) - a^*_{P, \\mathcal{A}}(x)\\right]^2}_{= \\text{misspecification}^2_x} \\mathrm{d}P(z) = L(P,a^*_{P, \\mathcal{F}}) + \\mathbb{E}_P\\left[\\text{misspecification}^2_x\\right]$$\n",
    "\n",
    "and the loss evaluated at the average action $\\bar d_n(x) := \\int_{Z^n} a_{z^n}(x) \\mathrm{d}P(z^n)$ is\n",
    "\n",
    "$$L\\left(P, \\bar d_n\\right) = L\\left(P, a^*_{P, \\mathcal{A}}\\right)  + \\int_Z \\underbrace{\\left[a^*_{P, \\mathcal{A}}(x) - \\bar d_n(x)\\right]^2}_{= \\text{bias}_x^2} \\mathrm{d}P(z) = L\\left(P, a^*_{P, \\mathcal{A}}\\right)  + \\mathbb{E}_P\\left[ \\text{bias}_x^2 \\right]$$\n",
    "\n",
    "Moreover, since $\\frac{\\partial^2 L}{\\partial a^2} = 2$ and $O(\\lambda^3) = 0$, the volatility term is simply\n",
    "\n",
    "$$R_n(P, d) - L(P, \\bar d_n) = \\int_Z \\left[\\int_{Z^n} \\left(a_{z^n}(x) - \\bar d_n(x)\\mathbf{1}(z^n)\\right)^2\\mathrm{d}P(z^n)\\right]\\mathrm{d}P(z)  = \\mathbb{E}_P\\left[\\text{volatility}_x\\right]$$\n",
    "\n",
    "\n",
    "Therefore, the excess risk of a decision rule $d$ under the quadratic loss is \n",
    "\n",
    "$$R_n(P, d) - L(P, a^*_{P, \\mathcal{F}}) = \\mathbb{E}_P\\left[\\text{misspecification}^2_x\\right] + \\mathbb{E}_P\\left[\\text{bias}_x^2\\right] + \\mathbb{E}_P\\left[\\text{volatility}_x\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Relative entropy loss\n",
    "\n",
    "\n",
    "\n",
    "The elements of the problem are\n",
    "\n",
    "* *Observable:* $Z \\sim P$, where $P$ has the density $p$\n",
    "* *Loss function:* $L(P, a) = \\int_Z p(z)\\log \\frac{p}{a}(z) \\mathrm{d}z$ \n",
    "* *Admissible space:* distributions on $Z$ for which density exists. Denote these densities by $a(z)$\n",
    "* *True feature:* $\\gamma(P) = p(z)$ \n",
    "\n",
    "Then the minimal loss is zero and it is reached by $a(z)=p(z)$, i.e. $L(P, a^*_{P, \\mathcal{F}})=0$. \n",
    "\n",
    "The loss evaluated at the best-in-class action $a^*_{P, \\mathcal{A}}(z)= \\inf_{a\\in\\mathcal{A}} \\ L(P, a)$, is\n",
    "\n",
    "$$L\\left(P, a^*_{P, \\mathcal{A}}\\right) = \\int_Z \\underbrace{\\log\\left(\\frac{p}{a^*_{P, \\mathcal{A}}}\\right)(z)}_{= \\text{misspecification}_z} \\mathrm{d}P(z) = \\mathbb{E}_P\\left[\\text{misspecification}_z\\right]$$\n",
    "\n",
    "and the loss evaluated at the average action $\\bar d_n(z) := \\int_{Z^n} a_{z^n}(z) \\mathrm{d}P(z^n)$ is\n",
    "\n",
    "\\begin{align*}\n",
    "L\\left(P, \\bar d_n\\right) &= \\int_Z \\left[\\log\\left(\\frac{p}{a^*_{P, \\mathcal{A}}}\\right)(z) + \\log\\left(\\frac{a^*_{P, \\mathcal{A}}}{\\bar d_n}\\right)(z)\\right] \\mathrm{d}P(z) \\\\\n",
    "&= L\\left(P, a^*_{P, \\mathcal{A}}\\right)  + \\int_Z \\underbrace{\\log\\left(\\frac{a^*_{P, \\mathcal{A}}}{\\bar d_n}\\right)(z)}_{= \\text{bias}_z}\\mathrm{d}P(z) = L\\left(P, a^*_{P, \\mathcal{A}}\\right) + \\mathbb{E}_P\\left[\\text{bias}_z\\right]\n",
    "\\end{align*}\n",
    "\n",
    "Note also that in this case the higher order terms in the Taylor expansion are not zeros. We might approximate the volatility of the decision rule with the second-order term defined above. However, relative entropy loss allows us to use an alternative (exact) measure for the variation in $d$, the so called **Theil's second entropy** (Theil (1967)), which captures all higher order moments of $d$. We can derive it by writing\n",
    "\n",
    "\\begin{align*}\n",
    "R_n(P, d) - L(P, \\bar d_n) &= \\mathbb{E}_P\\left[\\int_{Z^n} \\log \\left(\\frac{p}{d(z^n)}\\right)(z)\\mathrm{d}P(z^n)\\right]  - \\mathbb{E}_P\\left[\\log\\left(\\frac{p}{\\bar d_n}\\right)(z)\\right] \\\\\n",
    "&= \\mathbb{E}_P\\left[ \\underbrace{\\left(\\log \\bar d_n - \\mathbb{E}_{Z^n}[\\log d(z^n)]\\right)(z)}_{= \\nu(d)_z}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "The volatility term indeed captures the variability of $d(z^n)$ (as the sample varies). For example, $\\mathbb{V}[d(z^n)]=0$ implies $\\nu(d) = 0$. Furthermore, note that Theil's second entropy measure of an arbitrary (integrable) random variable $Y$ is\n",
    "\n",
    "$$\\nu(Y) := \\log \\mathbb{E}Y - \\mathbb{E}\\log Y$$\n",
    "\n",
    "This measure was utilized by Alvarez and Jermann (2006) and Backus et al. (2014) in the asset pricing literature. Essentially, it can be considered as a generalization of variance or more precisely, both variance and $\\nu$ are special cases of the general measure of volatiliy \n",
    "\n",
    "$$f(\\mathbb{E}Y) - \\mathbb{E}f(Y), \\quad\\quad\\text{where}\\quad f'' < 0$$\n",
    "\n",
    "The measure $\\nu$ is obtained by setting $f(y) = \\log(y)$, while the variance follows from $f(y)=-y^2$.\n",
    "\n",
    "Therefore, the excess risk of a decision rule $d$ under the relative entropy loss is \n",
    "\n",
    "$$R_n(P, d) - L(P, a^*_{P, \\mathcal{F}}) = \\mathbb{E}_P\\left[\\text{misspecification}_z\\right] + \\mathbb{E}_P\\left[\\text{bias}_z\\right] + \\mathbb{E}_P\\left[\\nu(d)_z\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------_\n",
    "## Classical approach -- the analogy principle\n",
    "\n",
    "\n",
    "The classical approach builds entirely on the empirical loss minimization principle.\n",
    "Briefly, the underlying justification is the *belief* that the decision rule's finite sample distributions converge fast enough (as $n\\to\\infty$) so that we can approximate them well with the limiting distribution.\n",
    "\n",
    "The decision rule itself is obtained by the empirical loss minimization procedure, with the action space being identical irrespective of the sample size. In terms of the general ELM decision rules we can nest this scenario by taking the index sequence to be constant. I.e. $\\{\\mathcal{A}_n\\}_{n\\geq1} \\equiv \\{\\mathcal{A}\\}_{n\\geq1}$ \n",
    "\n",
    "$$ \\mathcal{D}_{ELM} \\left(\\{\\mathcal{A}\\}_{n\\geq1} \\right) := \\left\\{ d: \\mathcal{S} \\mapsto \\mathcal{F} \\ \\left| \\ \\ \\forall z^{\\infty}, \\ \\ d(z^n) := \\inf_{a\\in\\mathcal{A}} \\ L(P_n, a) ,\\ \\ \\mathcal{A}\\subseteq \\mathcal{F}, \\ \\ \\forall n \\geq 1\\right\\}\\right. $$\n",
    "\n",
    "For ease of notation we denote the decision rules obtained via the classical aprroach as $d^C(z^{n})$.\n",
    "\n",
    "As we saw earlier this procedure leads to a consistent decision rule provided the composite loss-action space has a finite complexity measure. The complexity also plays a crucial role in determining the rate of convergence.\n",
    "\n",
    "However, in general there is no formal justification for the finite sample distribution of the decision rule based on the limiting distribution. Typically, the most accurate estimates can be obtained via simulations, like bootstrapping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Examples of decision rules built on the classical approach\n",
    "\n",
    "\n",
    "Maximum likelihood, (non-linear) least squares and GMM can be all considered as special cases. \n",
    "\n",
    "**Example 1.** In the case of regression function estimation take $\\mathcal{F} = Y^X$ and $L(P, \\mu) = \\int_{(Y,X)} (y - \\mu(x))^2P(\\mathrm{d}(y,x))$, then the **non-linear least squares** estimator can be written as\n",
    "\n",
    "$$\\widehat{\\mu}^C(z^n) = \\text{arg}\\inf\\limits_{\\mu \\in \\mathcal{A}}\\hspace{2mm}  \\frac{1}{n}\\sum_{t=1}^n (y_t - \\mu(x_t))^2. $$\n",
    "\n",
    "**Example 2.** In the case of density function estimation, we can take $\\mathcal{F} = \\{f: Z \\mapsto \\mathbb{R}_+ : \\int_Z f(z)\\mathrm{d}z =  1 \\}$ and $L(p, f) = \\int_{Z} p(z)\\log \\frac{p}{f}(z) \\mathrm{d}z$, then the **maximum likelihood** estimator can be written as\n",
    "\n",
    "$$ \\widehat{f}^C(z^{n}) = \\text{arg}\\inf\\limits_{f \\in \\mathcal{A}}\\hspace{2mm} - \\frac{1}{n}\\sum_{t=1}^n \\log f(z_t) + \\underbrace{H(p)}_{\\text{entropy of }p}. $$\n",
    "\n",
    "**Example 3.** In the case of moment restrictions, we can take $\\mathcal{A} = \\{g(\\cdot; \\theta) : \\theta \\in \\Theta\\}$ and $L(P, \\theta) = \\left[\\int g(z; \\theta)\\mathrm{d}P(z)\\right]' W \\left[\\int g(z; \\theta)\\mathrm{d}P(z)\\right]$ with a positive semi-definite $W$. Then the **GMM** estimator is\n",
    "\n",
    "$$ \\widehat{\\theta}_{W}(z^n) = \\text{arg}\\inf\\limits_{\\theta \\in \\mathcal{A}}\\hspace{2mm} \\left[\\frac{1}{n}\\sum_{t=1}^n g(z_t; \\theta)\\right]' W \\left[\\frac{1}{n}\\sum_{t=1}^n g(z_t; \\theta)\\right] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"heuristic justification\" is that in large samples consistent decision rules become invariant implying that the risk simply boils down to its loss function evaluated at the best-in-class action. Good generalization ensures that the empirical loss and the true loss are approximately the same and the shrinking variance ensures that the risk aprroximates the loss.\n",
    "\n",
    "$$ L(P_n, d^C(z^n)) \\approx L(P, d^C(z^n)) \\approx R_n(P, d^C) $$ \n",
    "\n",
    "The main difficulty, of course, is to find the critical value for the \"sufficiently large\" $n$. This critical value naturally depends on the *rate of convergence* which itself hinges on the complexity properties of the DGP and the composite loss-action space.\n",
    "\n",
    "Although the variation of the decision rule vanishes asymptotically, by employing the \"right\" scaling factor corresponding to the rate of convergence, we can derive a non-degenerate asymptotic distribution for $d^C$. Typically the goal is to establish a limiting distribution via the central limit theorem. For example, when the scaling factor is $\\sqrt{n}$ -- frequently encountered in practice -- $(d^C(z^n) - a^*_{\\mathcal{A}})$ must go to zero fast enough in order to ensure the existence of the following limit\n",
    "\n",
    "$$ \\sqrt{n}\\left(d^C(z^n) -  a^*_{\\mathcal{A}}\\right) \\to \\mathcal{N}(0, \\Sigma_L)\\quad\\quad\\text{as}\\quad n\\to \\infty .$$\n",
    "\n",
    "(Note that here we deal with convergence in the action space and not in the loss space.)\n",
    "\n",
    "To capture the variation stemming from the finiteness of the sample, the classical approach proceeds in the spirit of \"backward induction\": first, derive the asymptotic covariance matrix $\\Sigma_L$ of $\\sqrt{n}d^C$ and then scale it appropriately (by $n$) to obtain the finite sample approximation of the variance of $d^C$.\n",
    "\n",
    "As the notation reveals, alternative loss functions can lead to different estimators and corresponding asymptotic covariance matrices.\n",
    "\n",
    "This provides basis to rank decision rules by invoking the criterion of asymptotic efficiency, defined as the smallest possible asymptotic covariance matrix relative to some well-defined class of estimators. (Examples are maximum likelihood and Cramer-Rao lower bound, GMM can be made efficient by choosing the weighting matrix appropriately...) \n",
    "\n",
    "Of course, this type of \"minimization\" of asymptotic covariance matrices does not necessarily imply small finite sample variance of the decision rule. The estimation-approximation error decomposition helps understanding the inherent trade-off.\n",
    "\n",
    "Since the finite sample estimator variance can be linked with the second order term of the Taylor expansion, it might be tempting to view the quest for asymptotic efficiency as a device to minimize the risk of the decision rule. Notice, however, that this method does not take into account the possible *trade-off* between the different moments of the decision rule (i.e. the leading terms in the Taylor expansion of risk). In particular, the classical approach often restricts attention to unbiased estimators and looks for the minimum variance estimator *among this class*. The unbiasedness is gauged only relative to the---restricted---range of the decision rule $\\mathcal{A}$. Hence, even if the decision rule is unbiased we still have misspecification error. Now, it is difficult to defend the merits of an unbiased but misspecified  decision rule with large variance relative to a misspecified decision rule with some bias and smaller variance. \"Clever\" estimators tipically trade-off bias-misspecification and variance flexibly adjusting to the available sample size and the complexity of the estimation problem at hand.\n",
    "\n",
    "Summing up, in case of no misspecification:\n",
    "\n",
    "$$ \\underbrace{L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)- L\\left(P, a^*_{L, P, \\mathcal{F}} \\right)}_{\\text{misspecification}} = 0 $$\n",
    "\n",
    "In case of the decision rule being unbiased:\n",
    "\n",
    "$$ \\underbrace{L\\left(P, \\bar{d_n}\\right) - L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)}_{\\text{bias}} = 0 $$\n",
    "\n",
    "\n",
    "If both the bias and misspecification is zero for each sample size then the excess risk is determined by the volatility term. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example and analysis\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of the example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example reveals the danger of using asymptotic theory to approximate a decision rule's finite sample performance. A possible measure that can inform us about how well the asymptotic distribution approximates the finite sample properties is the **estimation error** defined as\n",
    "\n",
    "$$\\mathcal{E}_d(P, \\mathcal{A}, n) := R_n(P, d) - \\inf_{a\\in\\mathcal{A}} \\ L(P, a).$$\n",
    "\n",
    "While the risk $R_n(P, d)$ captures the performance of $d$ in samples of size $n$, $L(P, a^*_{P, L, \\mathcal{A}})$ essentially encodes its asymptotic properties and from the consistency of $d$ it follows that \n",
    "\n",
    "$$\\lim_{n\\to \\infty} \\ \\mathcal{E}_d(P, \\mathcal{A}, n) \\overset{P}{=} 0.$$\n",
    "\n",
    "In other words, even if $d$ is consistent relative to $(\\mathcal{A}, \\mathcal{H})$, its finite sample behavior hinges on the range of $d$, i.e. the action space $\\mathcal{A}$.\n",
    "\n",
    "Recall that the origin of estimation error is the fact that we do not know $P$, instead we have to use the information in the (finite) sample to approximate the 'best' action in $\\mathcal{A}$. Intuitively, the smaller the estimation error the better this approximation. This consideration is closely related to the idea of **generalization**: the key issue in learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalization\n",
    "\n",
    "Endowed with a fixed finite sample $z^n$, we say that a particular action $a\\in\\mathcal{A}$ *generalizes well*, if the quantity  \n",
    "\n",
    "$$\\left|L(P, a) - L(P_n, a)\\right|\\quad \\text{is small}$$\n",
    "\n",
    "that is, if the discrepancy between the empricial and true loss is \"relatively small\". Note that this property does *not* require that the empricial loss is itself small, which is the objective function of the classical ELM approach.\n",
    "\n",
    "**ADD ROAD MAP: how to resolve $A$, $z^n$, $P$**\n",
    "\n",
    "This sheds some light on what can go wrong with the ELM approach in finite samples. In practice, one of the worst situations is **overfitting**, that is when the empirical loss is much smaller than the true loss, hence our assessment of the quality of $a\\in\\mathcal{A}$ might be overly optimistic.\n",
    "\n",
    "Extending the generalization property to $\\forall a \\in\\mathcal{A}$ leads to the notion of **generalization error**, defined as\n",
    "\n",
    "$$ \\Delta(P, z^n, \\mathcal{A}) := \\sup_{a\\in\\mathcal{A}} \\ \\left|L(P, a) - L(P_n, a)\\right|$$\n",
    "\n",
    "The decision rule derived through ELM will generalize well from the given sample $z^n$, if the associated $\\Delta$ is small.\n",
    "\n",
    "Notice that this error still hinges on the particular sample $z^n$. To draw uniform inference about the generalization properties of $d$, in notebook {reference asymptotics}, we constructed probabilistic tail bounds for $\\Delta(P, z^n, \\mathcal{A})$, for which the key building block was the complexity of the action space $\\mathcal{A}$.  \n",
    "\n",
    "A somewhat less ambitious approach is to focus on the average generalization error, i.e. \n",
    "\n",
    "$$\\bar \\Delta(P, \\mathcal{A}) := \\int_{Z^n} \\sup_{a\\in\\mathcal{A}} \\ \\left|L(P, a) - L(P_n, a)\\right| \\mathrm{d}P(z^n) = \\mathbb{E}_{Z^n}\\left[ \\Delta(P, z^n)\\right]$$\n",
    "\n",
    "Naturally, by bounding the tail probabilities of $\\Delta$ we control the mean as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimation error and generalization\n",
    "\n",
    "It turns out that the average generalization error of the ELM decision rule $d^C$ is tightly linked to its estimation error. To see this, consider the following decomposition of the excess loss for a given realization of the sample $z^n$   \n",
    "\n",
    "\\begin{align*}\n",
    "L(P, d^C(z^n)) - L(P, a^*_{\\mathcal{A}}) = & \\Big(L(P, d^C(z^n)) - L(P_n, d^C(z^n))\\Big) \\\\\n",
    "+ &\\underbrace{\\Big(L(P_n, d^C(z^n)) - L(P_n, a^*_{\\mathcal{A}}) \\Big)}_{\\leq 0} + \\Big(L(P_n,  a^*_{\\mathcal{A}}) - L(P,a^*_{\\mathcal{A}}) \\Big)\\quad\\quad  (1)\n",
    "\\end{align*}\n",
    "\n",
    "and take the expectations over $Z^n$ to arrive at \n",
    "\n",
    "$$\\mathcal{E}_{d^C}(P, \\mathcal{A}, n) =  R_n(P, d^C) - L(P, a^*_{\\mathcal{A}}) \\leq \\mathbb{E}_{Z^n}\\Big[\\sup_{a\\in\\mathcal{A}}\\{L(P, a) - L(P_n, a)\\} \\Big] = \\bar \\Delta(P, \\mathcal{A})$$\n",
    "\n",
    "\n",
    " * In (1) the second term on the RHS is nonpositive, because the decision rule is based on ELM, so  $L(P, d(z^n)) \\leq L(P_n, a) \\ \\forall a\\in\\mathcal{A}$. \n",
    " * The last term disappears when we take the expectation, as we assumed the form $L(P,a) = \\int_Z l(a, z)\\mathrm{d}P(z)$ of the loss function and $\\mathbb{E}_{Z^n}[P_n]=P$.\n",
    "\n",
    "\n",
    "This suggests that by seeking good generalization performance of the ELM estimator the statistician can efficiently control the estimation error as well, thus making sure that the asymptotic analysis provides a relatively good approximation of the finite sample properties of $d^C$. As we saw in the last notebook, the key device to curb the average generalization error is to choose an action space $\\mathcal{A}$ with small complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### coming from bias-variance-misspecification decomposition...\n",
    "\n",
    "From this somewhat naive perspective, if the \"initial\" action space is \"too small\", there is no way to get rid of misspecification even if the sample size becomes astronomical. The only reasoning that might justify this approach is the *belief* that the model was correctly specified in the first place.   \n",
    "\n",
    "Anticipating the key idea behind modern approaches to inference, it is worth contemplating the possibility that the action space itself depends on the sample size as a result of the statistician's desire to reduce misspecification at least asymptotically. This requires a slight adjustment in the definition of the ELM decision rules. In particular, define the class of decision rules indexed by the sequence $\\{\\mathcal{A}_n\\}_{n\\geq 1}$ as follows\n",
    "\n",
    "$$ \\mathcal{D}_{ELM}\\left(\\{\\mathcal{A}_n\\}_{n\\geq 1}\\right) := \\left\\{ d: \\mathcal{S} \\mapsto \\mathcal{F} \\ \\left| \\ \\ \\forall z^{\\infty}, \\ \\ d(z^n) := \\inf_{a\\in\\mathcal{A}_n} \\ L(P_n, a) ,\\ \\ \\text{s.t.} \\ \\  \\mathcal{A}_n\\subseteq \\mathcal{A}_{n+1}\\subseteq \\mathcal{F}, \\ \\ \\forall n \\geq 1\\right\\}\\right. $$\n",
    "\n",
    "A special case, of course, is the original scenario, when $\\mathcal{A}_n = \\mathcal{A}\\subseteq \\mathcal{F}$, $\\forall n\\geq 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "## Statistical Learning Theory -- the minimax approach\n",
    "\n",
    "\n",
    "As seen above the classical approach does not deal with the generalization problem arising in finite samples. Statistical learning theory takes a somewhat different approach controlling for the estimation error -- while still keeping an eye on the approximation error.   \n",
    "\n",
    "Again, the objective is to minimize the excess risk of the decision rule. As seen earlier the estimation-approximation error decomposition highlights one of the main dilemmas the statistician is facing\n",
    "\n",
    "$$ \\underbrace{R_n(P, d) - L\\left(P, a^{*}_{L, P, \\mathcal{F}} \\right)}_{\\text{excess risk}} =  \\underbrace{R_n(P, d) - L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)}_{\\substack{\\text{estimation error} \\\\ \\text{random}}} + \\underbrace{L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)- L\\left(P, a^{*}_{L, P, \\mathcal{F}} \\right)}_{\\substack{\\text{approximation error} \\\\ \\text{deterministic}}}. $$\n",
    "\n",
    "By controlling the decision rule -- or more precisely the range of the decision rule, the action space $\\mathcal{A}$ -- the statistician can balance the trade-off between the estimation error and the approximation error.\n",
    "\n",
    "* The approximation captures the idea that the true feature of the DGP does not lie in the range of the decision rule, hence there is an inherent error due to this misspecification. Correspondingly, ceteris paribus enlarging the action space -- the range of the decision rule -- the approximation error gets smaller. As $\\mathcal{A}$ approaches $\\mathcal{F}$ the approximation error should vanish.\n",
    "\n",
    "* However, the range of the decision rule also plays a key role in the size of the estimation error. Indeed, it is easy to see that one could always make the estimation error zero by choosing a constant decision rule -- that is, one which range is a singleton and hence assigns the same action to each possible realization of the sample. \n",
    "\n",
    "Typically, this trade-off inherent in all statistical inference problems can be visualized on the following graph.\n",
    "\n",
    "<img src=\"./decomp.png\", width=600, height=440>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theoretical trade-off when dealing with decision rules based on the empirical loss minimization priciple is the following. In terms of the action space,\n",
    "\n",
    "* whenever the gain from smaller approximation error exceeds the loss from greater estimation error one should increase the action space, there is **underfitting**.\n",
    "* whenever the gain from smaller estimation error exceeds the loss from greater approximation error one should decrease the action space, there is **overfitting**.\n",
    "\n",
    "An ideal decision rule based on the analogy principle traces the minimum of the U shaped excess risk.\n",
    "\n",
    "Next, we take a closer look at how the estimation error depends on the decision rule -- in particular its range.\n",
    "\n",
    "As shown above the estimation error and the generalization property are tightly connected.\n",
    "\n",
    "$$ \\underbrace{R_n(P, d) - L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)}_{\\text{estimation error}} \\leq \\mathbb{E}_{Z^n}\\Big[\\sup_{a\\in\\mathcal{A}}\\{L(P, a) - L(P_n, a)\\} \\Big] = \\bar \\Delta(P, \\mathcal{A}) $$\n",
    "\n",
    "The tail bounds for empirical processes discussed in notebook {reference asymptotics} are useful to get a grasp on the generalization error $\\bar \\Delta(P, \\mathcal{A})$. In fact, one can bound it in terms of the Rademacher complexity of the composite loss-action space, $\\mathcal{L}$. \n",
    "\n",
    "$$ \\mathbb{E}_{Z^n}\\Big[\\sup_{a\\in\\mathcal{A}}\\{L(P, a) - L(P_n, a)\\} \\Big] = \\bar \\Delta(P, \\mathcal{A}) \\leq 2 \\mathcal{R}_n\\left(\\mathcal{L}\\right) $$\n",
    "\n",
    "The Rademacher complexity of $\\mathcal{L}:= \\{l(a, \\cdot) : a \\in \\mathcal{A}\\}$ is increasing in the action space, $\\mathcal{A}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling the complexity of the action space\n",
    "\n",
    "Based on the previous discussion on how the size of the action space affects both estimation and approximation errors through the complexity of $\\mathcal{L}$ the approach of statistical learning theory is to explicitly control for complexity through the decision rule's action space.\n",
    "\n",
    "In this sense the corresponding decision rule -- $d_{SLT}$ -- can be defined(?) in terms of a sequence of action spaces\n",
    "\n",
    "$$ \\mathcal{D}_{ELM}\\left(\\{\\mathcal{A}_n\\}_{n\\geq 1}\\right) := \\left\\{ d: \\mathcal{S} \\mapsto \\mathcal{F} \\ \\left| \\ \\ \\forall z^{\\infty}, \\ \\ d(z^n) := \\inf_{a\\in\\mathcal{A}_n} \\ L(P_n, a) ,\\ \\ \\text{s.t.} \\ \\  \\mathcal{A}_n\\subseteq \\mathcal{A}_{n+1}\\subseteq \\mathcal{F}, \\ \\ \\forall n \\geq 1\\right\\}\\right. $$ \n",
    "\n",
    "The rationale behind this procedure is that one should increase the action space and fit more flexible and complex models if the quantity and quality of the data increase. As a result, the effective $\\mathcal{A}_n$ is defined by the quantity of data at hand -- the more the data the more flexible model one can afford to fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalized empirical loss minimization\n",
    "\n",
    "Most often one encounters the above procedure in the following form. At each sample size $n$ the constrained optimization problem\n",
    "\n",
    "$$ d_{SLT}(z^n) := \\inf_{a\\in\\mathcal{A}_n} \\ L(P_n, a) $$\n",
    "\n",
    "is recast as an unconstrained optimization problem via the method of Lagrange multipliers,\n",
    "\n",
    "$$ d_{SLT}(z^n) := \\inf_{a\\in\\mathcal{F}} \\ L(P_n, a) + \\Gamma (a, n). $$\n",
    "\n",
    "The general function $\\Gamma$ is the penalty term determining the effective size of the action space -- and hence determining $\\mathcal{A}_n$ in the sequence.\n",
    "\n",
    "Often $\\Gamma(a, n)$ takes the specific form $\\Gamma(a, n) = \\kappa_n C(a)$ where C is a general measure of the complexity of a single action and $\\kappa_n$ is a tuning parameter determined flexibly with the sample size and the quality of the fit -- usuallyt through cross-validation. (**TODO**: here we should probably have a brief digression on how we can jump from the complexity of sets to the complexity of one single action.)\n",
    "\n",
    "By choosing the action space, the loss function and the penalty term many of the well known machine learning techniques can be treated simultaneously in the introduced framework. The lasso and ridge regressions, support vector machines or regularization networks can be seen as particular specifications of the defined objects. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Examples for the SLT approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----------------------------------------------------\n",
    "# Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A) Taylor-expansion of the risk functional\n",
    "\n",
    "Remember that the risk of a decision rule $d$ is given by the following expression.\n",
    "\n",
    "$$ R_n(P, d) := \\int\\limits_{Z^n} L(P, d(z^n)) \\ \\mathrm{d} P(z^n) $$\n",
    "\n",
    "Consider the Taylor expansion of this functional with respect to the decision rule around a particular $d$. For any alternative decision rule $\\tilde{d}$, we can define the difference\n",
    "\n",
    "$$\\tilde{d} - d := \\lambda \\eta(z^n)\\quad \\quad \\text{where}\\quad \\eta: \\mathcal{S} \\mapsto \\mathcal{A}, \\quad \\lambda\\in\\mathbb{R}_+$$\n",
    "\n",
    "and then the second-order Taylor expansion of the risk functional around $d$ is\n",
    "\n",
    "$$ R_n\\left(P, \\tilde{d}\\right) = R_n\\left(P, d \\right) + \\int_{Z^n} \\frac{\\partial L(P, d(z^n))}{\\partial a}\\lambda\\eta(z^n)\\mathrm{d} P(z^n) + \\int_{Z^n} \\frac{\\partial^2 L(P, d(z^n))}{\\partial a^2}\\frac{\\lambda^2\\eta(z^n)^2}{2}\\mathrm{d} P(z^n) + O(\\lambda^{3})$$\n",
    "\n",
    "where we use the notion of Gateaux differential (generalization of directional derivate) to obtain the marginal change in the loss function as the abstract $a$ changes. \n",
    "\n",
    "An important reference point of any decision rule $d$ is the expected action that it provides for a given sample size $n$,\n",
    "\n",
    "$$\\bar{d}_n := \\int_{Z^n} d(z^n)\\ \\mathrm{d}P(z^n)$$ \n",
    "\n",
    "which does not necessarily belong to $\\mathcal{A}$. In what follows, we imagine a decision rule $\\bar{d}_n\\mathbf{1}(z^n)$ that assigns the value $\\bar{d}_n$ to all sample realization $z^n$ and use the Taylor approximation around this hypothetical decision rule to approximate the risk of $d$. In this case, $d - \\bar{d}_n\\mathbf{1} := \\lambda \\eta(z^n)$ and \n",
    "\n",
    "$$ R_n\\left(P, d\\right) = L\\left(P, \\bar d_n \\right) + \\int_{Z^n} \\frac{\\partial^2 L(P, \\bar d_n)}{\\partial a^2}\\frac{(d - \\bar{d}_n\\mathbf{1})^2}{2}\\mathrm{d} P(z^n) + O(\\lambda^{3})$$\n",
    "\n",
    "where the first-order term vanishes because the partial -- evaluated at $\\bar d_n\\mathbf{1}$ -- is a constant and $\\int_{Z^n}(d - \\bar d_n\\mathbf{1}) \\mathrm{d}P = 0$. Note that in this expression the second-order term encodes the theoretical variation of the action that $d$ assigns to random samples of size $n$. The regular variance formula is altered by (one half of) the second derivative of the loss function (evaluated at $\\bar d_n$), representing the role of the loss functions's curvature in determining the decision rule's volatility. As a result, a reasonable measure for the decision rule's volatility can be defined as $R_n\\left(P, d\\right) - L\\left(P, \\bar d_n \\right)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B) Bias-variance-misspecification decomposition of GMM\n",
    "\n",
    "The elements of the problem are\n",
    "\n",
    "* *Observable:* $Z \\sim P$, with given moment conditions $g: Z \\times \\mathbb{R}^{p+m} \\mapsto \\mathbb{R}^m$ \n",
    "* *Action space:* $\\mathcal{A} = \\Theta \\subseteq \\mathbb{R}^p$\n",
    "* *Admissible space:* $\\mathcal{F} = \\Theta'\\equiv \\Theta \\times \\mathbb{R}^m$, so that we can always set the expectation of $g$ equal to zero by means of the $m$ auxiliary parameters.  \n",
    "* *Loss function:* $L(P, a) = \\left(\\int_Z g(z, a) \\mathrm{d}P(z)\\right)'W\\left(\\int_Z g(z, a) \\mathrm{d}P(z)\\right)$\n",
    "\n",
    "Then the minimal loss is zero (by construction), i.e. $L(P, a^{*}_{P, \\mathcal{F}}) = 0$. \n",
    "\n",
    "The loss evaluated at the best-in-class action $a^*_{P, \\mathcal{A}} = \\inf_{a\\in\\mathcal{A}} \\ L(P, a)$, is\n",
    "\n",
    "$$L\\left(P, a^*_{P, \\mathcal{A}}\\right) = \\mathbb{E}_P\\left[ g\\left(z, a^{*}_{P, \\mathcal{A}}\\right) \\right]' W \\mathbb{E}_P\\left[ g\\left(z, a^{*}_{P, \\mathcal{A}}\\right) \\right] = \\text{misspecification}$$\n",
    "\n",
    "For the bias term we substract this quantity from the loss evaluated at the average action $\\bar d_n(z) := \\int_{Z^n} a_{z^n} \\mathrm{d}P(z^n)$ \n",
    "\n",
    "$$L\\left(P, \\bar d_n\\right) - L\\left(P, a^*_{P, \\mathcal{A}}\\right) = \\mathbb{E}_P\\left[ g\\left(z, \\bar d_n \\right) \\right]' W \\mathbb{E}_P\\left[ g\\left(z, \\bar d_n \\right) \\right]  - \\mathbb{E}_P\\left[ g\\left(z, a^{*}_{P, \\mathcal{A}}\\right) \\right]' W \\mathbb{E}_P\\left[ g\\left(z, a^{*}_{P, \\mathcal{A}}\\right) \\right]  = \\text{bias}$$\n",
    "\n",
    "We approximate the volatility term with the second-order term of the Taylor expansion. For simplicity, make use of the following notation \n",
    "\n",
    "$$D(a) := \\mathbb{E}_P\\left[ \\frac{\\partial g(z, a)}{\\partial a}\\right] \\in \\mathbb{R}^{m\\times p} \\quad\\quad H(a) := \\mathbb{E}_P\\left[ \\frac{\\partial^2 g(z, a)}{\\partial a^2}\\right] \\in \\mathbb{R}^{p\\times p\\times m}$$\n",
    "\n",
    "and so \n",
    "\n",
    "$$\\frac{\\partial L(P, a)}{\\partial a} = 2 D(a)' W \\mathbb{E}_P\\left[ g(z, a)\\right]\\in \\mathbb{R}^{p} \\quad \\quad \\frac{\\partial^2 L(P, a)}{\\partial a^2} = 2 H(a) W \\mathbb{E}_P\\left[ g(z, a)\\right] + 2 D(a)' W D(a) \\in \\mathbb{R}^{p\\times p}$$\n",
    "\n",
    "implying the approximation \n",
    "\n",
    "$$R_n(P, d) - L(P, \\bar d_n)\\approx \\int_{Z^n} (d(z^n) - \\bar d_n \\mathbf{1}(z^n))'\\left[ \\underbrace{H(\\bar d_n) W  g(z, \\bar d_n)}_{\\to 0 \\ \\text{as} \\ n\\to \\infty} + D(\\bar d_n)' W D(\\bar d_n)\\right](d(z^n) - \\bar d_n \\mathbf{1}(z^n)) \\mathrm{d}P(z^n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
