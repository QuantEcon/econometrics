{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coping with finite samples\n",
    "\n",
    "**January 2017**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From asymptotics to finite samples\n",
    "\n",
    "In notebook {reference asymptotics} we took an asymptotic approach by focusing on the behavior of the decision rule under the assumption that the sample size is ''almost infinity''. We identified consistency and fast rate of convergence as desirable features that any reasonable decision rule should satisfy.\n",
    "\n",
    "We drew the crucial conclustion that both consistency and rate of convergence are driven by the *complexity* of the composite function class $\\mathcal{L}_{\\mathcal A}$. In particular, we have seen that finiteness of model complexity is enough to ensure consistency and given consistency the magnitude of complexity is inversly related to the decision rule's rate of convergence: the lower the complexity, the faster the rate of convergence.  \n",
    "\n",
    "This notebook explores the implications of taking the finiteness of the sample size seriously. We investigate the extra issues that arise relative to the asymptotic case and outline some approaches meant to tackle these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompositions of the risk functional \n",
    "\n",
    "A consistent decision rule $d$ satisfies $L(P, d(z^n)) \\overset{P}{\\to} L(P, a^*_{L, P, \\mathcal{A}})$, so--by definition--the resulting loss functional has a degenerate asymptotic distribution centered around the loss of the best-in-class action. One might call this value \"asymptotic risk\". Being dependent on the action space $\\mathcal A$, however, this asymptotic risk is not necessarily zero. The global minimum of the loss function relates to the true feature of the DGP that we denoted by $\\gamma(P)=a^*_{L, P, \\mathcal{F}}$, with $\\mathcal{F}$ being the set of all admissible actions.  \n",
    "\n",
    "\n",
    "The central objects of finite sample analysis concern how the finite risk deviates from these two \"asymptotic\" values: \n",
    "\n",
    "* The deviation from the best-in-class loss is called **estimation error**:\n",
    "\n",
    "$$\\mathcal E_d(P, \\mathcal A, n) := R_n(P, d) - L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right) $$\n",
    "\n",
    "* The deviation from the global minimum is the so called **excess risk**: \n",
    "\n",
    "$$\\mathcal{ER}_d(P, n) :=  R_n(P, d) - L\\left(P, a^*_{L, P, \\mathcal{F}} \\right) $$\n",
    "\n",
    "\n",
    "### Estimation-misspecification decomposition\n",
    "\n",
    "Naturally, the two objects are closely connected. In fact, decomposing the excess risk using the estimation error highlights one of the most important tensions underlying finite sample inference problems. \n",
    "\n",
    "$$ R_n(P, d) - L\\left(P, a^*_{L, P, \\mathcal{F}}  \\right) =  \\underbrace{R_n(P, d) - L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)}_{\\substack{\\text{estimation error} \\\\ \\text{random}}} + \\underbrace{L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)- L\\left(P, a^*_{L, P, \\mathcal{F}}  \\right)}_{\\substack{\\text{misspecification error} \\\\ \\text{deterministic}}}. $$\n",
    "\n",
    "As we noted earlier the **misspecification error** is stemming from the fact that the true feature might lie outside of the action space. Intuitively, as we enlarge the action space $\\mathcal{A}$, the misspecification error gets weakly smaller, while the estimation error gets weakly larger. An ideal decision rule balances this trade-off.\n",
    "\n",
    "### Bias-volatility-misspecification decomposition \n",
    "\n",
    "To clarify why the estimation error might increase in the size of the action space, we study a slightly finer decomposition of excess risk bringing the \"standard\" bias-variance trade-off to bear. The starting point of this decomposition is the observation that by assigning members of $\\mathcal A$ to *random* samples $z^n$ the decision rule $d$ effectively induces a random variable on the action space. Having said that, for each sample size $n$, we can define the *mean action* $\\bar d_n$ by taking the expected value of $d(z^n)\\in\\mathcal A$ across the different ensemble samples:\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar d_n = \\int_{Z^n} d(z^n) \\mathrm{d}P_0(z^n)\n",
    "\\end{align*}\n",
    "\n",
    "Notice that the mean action need not be in the decision problem's action space $\\mathcal A$. Nonetheless, we can evaluate the loss at $\\bar d_n$ and use this value to decompose the excess risk's estimation error component     \n",
    "\n",
    "$$ R_n(P, d) - L\\left(P, a^*_{L, P, \\mathcal{F}} \\right) = \\underbrace{R_n\\left(P, d\\right) - L\\left(P, \\bar d_n \\right)}_{\\text{volatility}} + \\underbrace{L\\left(P, \\bar{d_n}\\right) - L\\left(P, a^{*}_{L,P,  \\mathcal{A}}\\right)}_{\\text{bias}} + \\underbrace{L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)- L\\left(P,a^*_{L, P, \\mathcal{F}}  \\right)}_{\\text{misspecification}} $$\n",
    "\n",
    "The only term that depends on the particular sample is the first one, hence the name volatility. We prefer to view this term as a measure of \"instability\" of the decision rule, which manifests in large sensitivity of the optimal action to small variations in the sample. Given this interpretation, it is relatively straightforward to see the volatility term's close connection with Rademacher complexity. **ADD 1-2 SENTENCES**\n",
    "\n",
    "Since the bias and misspecification terms are deterministic, it might be tempting to pull them together and define a \"total bias\" component as is typically done in the statistical learning literature (Abu-Mostafa et al., 2012 ). Notice, however, that there is a key difference between these two terms. While the bias term depends on the sample size $n$, the misspecification term remains constant given that the action space $\\mathcal{A}$ is kept fixed. In fact, we know from our analysis in lecture {asymptotic lecture} that\n",
    "\n",
    "* If the decision rule $d$ is consistent relative to $(\\mathcal{H}, \\mathcal{A})$, the *bias* converges to zero as $n$ goes to infinty \n",
    "* If the decision rule $d$ is consistent relative to $(\\mathcal{H}, \\mathcal{A})$, the *volatility* converges to zero as $n$ goes to infinty\n",
    "\n",
    "**MAYBE HINTING THAT LATER WE WILL SEE APPROACHES THAT TRIES TO MAKE $\\mathcal A$ SAMPLE DEPENDENT**\n",
    "\n",
    "**TOADD?**: stylized figure from the mostafa book (page 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Illustration of the bias-volatility-misspecification decomposition\n",
    "\n",
    "**1. Quadratic loss**\n",
    "\n",
    "The elements of the problem are\n",
    "\n",
    "* *Observable:* $Z = (Y,X)$\n",
    "* *Loss function:* $L(P, a) = \\int_Z (y - a(x))^2 \\mathrm{d}P(z)$ \n",
    "* *Admissible space:* $\\mathcal{F}\\equiv \\{ a: x \\mapsto y \\ |  \\ a \\ \\text{is measurable}\\}$ \n",
    "* *True feature:* $\\gamma(P) = \\mathbb{E}_P[Y|X] = \\inf_{a\\in \\mathcal{F}} \\ L(P, a)$ \n",
    "\n",
    "Then the minimal loss can be written as \n",
    "\n",
    "$$L(P, a^*_{P, \\mathcal{F}}) = \\int_Z \\underbrace{(y - \\mathbb{E}_P[Y|X](x))^2}_{=\\sigma^2_x \\ \\ \\text{(noise)}} \\mathrm{d}P(z) = \\mathbb{E}_P[\\sigma^2_x]$$\n",
    "\n",
    "the loss evaluated at the best-in-class action, $a^*_{P, \\mathcal{A}}(x)= \\inf_{a\\in\\mathcal{A}} \\ L(P, a)$, is\n",
    "\n",
    "$$L\\left(P, a^*_{P, \\mathcal{A}}\\right) = \\mathbb{E}_P[\\sigma^2_x] + \\int_Z \\underbrace{\\left[\\mathbb{E}_P[Y|X](x) - a^*_{P, \\mathcal{A}}(x)\\right]^2}_{= \\text{misspecification}^2_x} \\mathrm{d}P(z) = L(P,a^*_{P, \\mathcal{F}}) + \\mathbb{E}_P\\left[\\text{misspecification}^2_x\\right]$$\n",
    "\n",
    "and the loss evaluated at the average action $\\bar d_n(x) := \\int_{Z^n} a_{z^n}(x) \\mathrm{d}P(z^n)$ is\n",
    "\n",
    "$$L\\left(P, \\bar d_n\\right) = L\\left(P, a^*_{P, \\mathcal{A}}\\right)  + \\int_Z \\underbrace{\\left[a^*_{P, \\mathcal{A}}(x) - \\bar d_n(x)\\right]^2}_{= \\text{bias}_x^2} \\mathrm{d}P(z) = L\\left(P, a^*_{P, \\mathcal{A}}\\right)  + \\mathbb{E}_P\\left[ \\text{bias}_x^2 \\right]$$\n",
    "\n",
    "Moreover, since $\\frac{\\partial^2 L}{\\partial a^2} = 2$ and $O(\\lambda^3) = 0$, the volatility term is simply\n",
    "\n",
    "$$R_n(P, d) - L(P, \\bar d_n) = \\int_Z \\left[\\int_{Z^n} \\left(a_{z^n}(x) - \\bar d_n(x)\\mathbf{1}(z^n)\\right)^2\\mathrm{d}P(z^n)\\right]\\mathrm{d}P(z)  = \\mathbb{E}_P\\left[\\text{volatility}_x\\right]$$\n",
    "\n",
    "\n",
    "Therefore, the excess risk of a decision rule $d$ under the quadratic loss is \n",
    "\n",
    "$$R_n(P, d) - L(P, a^*_{P, \\mathcal{F}}) = \\mathbb{E}_P\\left[\\text{misspecification}^2_x\\right] + \\mathbb{E}_P\\left[\\text{bias}_x^2\\right] + \\mathbb{E}_P\\left[\\text{volatility}_x\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Relative entropy loss**\n",
    "\n",
    "The elements of the problem are\n",
    "\n",
    "* *Observable:* $Z \\sim P$, where $P$ has the density $p$\n",
    "* *Loss function:* $L(P, a) = \\int_Z p(z)\\log \\frac{p}{a}(z) \\mathrm{d}z$ \n",
    "* *Admissible space:* distributions on $Z$ for which density exists. Denote these densities by $a(z)$\n",
    "* *True feature:* $\\gamma(P) = p(z)$ \n",
    "\n",
    "Then the minimal loss is zero and it is reached by $a(z)=p(z)$, i.e. $L(P, a^*_{P, \\mathcal{F}})=0$. \n",
    "\n",
    "The loss evaluated at the best-in-class action $a^*_{P, \\mathcal{A}}(z)= \\inf_{a\\in\\mathcal{A}} \\ L(P, a)$, is\n",
    "\n",
    "$$L\\left(P, a^*_{P, \\mathcal{A}}\\right) = \\int_Z \\underbrace{\\log\\left(\\frac{p}{a^*_{P, \\mathcal{A}}}\\right)(z)}_{= \\text{misspecification}_z} \\mathrm{d}P(z) = \\mathbb{E}_P\\left[\\text{misspecification}_z\\right]$$\n",
    "\n",
    "and the loss evaluated at the average action $\\bar d_n(z) := \\int_{Z^n} a_{z^n}(z) \\mathrm{d}P(z^n)$ is\n",
    "\n",
    "\\begin{align*}\n",
    "L\\left(P, \\bar d_n\\right) &= \\int_Z \\left[\\log\\left(\\frac{p}{a^*_{P, \\mathcal{A}}}\\right)(z) + \\log\\left(\\frac{a^*_{P, \\mathcal{A}}}{\\bar d_n}\\right)(z)\\right] \\mathrm{d}P(z) \\\\\n",
    "&= L\\left(P, a^*_{P, \\mathcal{A}}\\right)  + \\int_Z \\underbrace{\\log\\left(\\frac{a^*_{P, \\mathcal{A}}}{\\bar d_n}\\right)(z)}_{= \\text{bias}_z}\\mathrm{d}P(z) = L\\left(P, a^*_{P, \\mathcal{A}}\\right) + \\mathbb{E}_P\\left[\\text{bias}_z\\right]\n",
    "\\end{align*}\n",
    "\n",
    "Note also that in this case the higher order terms in the Taylor expansion are not zeros. We might approximate the volatility of the decision rule with the second-order term defined above. However, relative entropy loss allows us to use an alternative (exact) measure for the variation in $d$, the so called **Theil's second entropy** (Theil (1967)), which captures all higher order moments of $d$. We can derive it by writing\n",
    "\n",
    "\\begin{align*}\n",
    "R_n(P, d) - L(P, \\bar d_n) &= \\mathbb{E}_P\\left[\\int_{Z^n} \\log \\left(\\frac{p}{d(z^n)}\\right)(z)\\mathrm{d}P(z^n)\\right]  - \\mathbb{E}_P\\left[\\log\\left(\\frac{p}{\\bar d_n}\\right)(z)\\right] \\\\\n",
    "&= \\mathbb{E}_P\\left[ \\underbrace{\\left(\\log \\bar d_n - \\mathbb{E}_{Z^n}[\\log d(z^n)]\\right)(z)}_{= \\nu(d)_z}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "The volatility term indeed captures the variability of $d(z^n)$ (as the sample varies). For example, $\\mathbb{V}[d(z^n)]=0$ implies $\\nu(d) = 0$. Furthermore, note that Theil's second entropy measure of an arbitrary (integrable) random variable $Y$ is\n",
    "\n",
    "$$\\nu(Y) := \\log \\mathbb{E}Y - \\mathbb{E}\\log Y$$\n",
    "\n",
    "This measure was utilized by Alvarez and Jermann (2006) and Backus et al. (2014) in the asset pricing literature. Essentially, it can be considered as a generalization of variance or more precisely, both variance and $\\nu$ are special cases of the general measure of volatiliy \n",
    "\n",
    "$$f(\\mathbb{E}Y) - \\mathbb{E}f(Y), \\quad\\quad\\text{where}\\quad f'' < 0$$\n",
    "\n",
    "The measure $\\nu$ is obtained by setting $f(y) = \\log(y)$, while the variance follows from $f(y)=-y^2$.\n",
    "\n",
    "Therefore, the excess risk of a decision rule $d$ under the relative entropy loss is \n",
    "\n",
    "$$R_n(P, d) - L(P, a^*_{P, \\mathcal{F}}) = \\mathbb{E}_P\\left[\\text{misspecification}_z\\right] + \\mathbb{E}_P\\left[\\text{bias}_z\\right] + \\mathbb{E}_P\\left[\\nu(d)_z\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------\n",
    "## Classical approach -- the analogy principle\n",
    "\n",
    "\n",
    "\n",
    "In econometrics, the classical approach to inference builds heavily on the empirical loss minimization principle, or as they often put it the *analogy principle*. The underlying justification behind this approach is the *belief* that the decision rule's induced finite sample distributions converge fast enough as the sample size grows to infinity, so we can approximate them well with the corresponding asymptotic distributions.(SEE THE FIRST FIGURE OF LECTURE 2). As we have seen before, this property of the decision rule crucially depends on the complexity of the composite function class $\\mathcal L_{\\mathcal A}$.\n",
    "\n",
    "Traditionally, the classical approach does not take into account the particular sample at hand while determining the decision problem's action space $\\mathcal A$. In fact, since it relies almost exclusively on large sample approximations, it would not make much sense to do so. Later, when we turn to more modern approaches, we will see that this feature of the classical approach can cause serious troubles. \n",
    "\n",
    "In this notebook we denote the decision rules obtained by empirical loss minimazation for a sample size $n$ as \n",
    "\n",
    "$$d^C(z^{n}):=\\min_{a\\in\\mathcal A} \\ L(P_n,a)$$.\n",
    "\n",
    "**Remark:** Notice that in general there is no formal justification for setting the finite sample distribution of the decision rule equal to the limiting distribution. Typically, more accurate estimates can be obtained via simulations, like Monte Carlo or bootstrap. We should add in fairness that researchers of the classical approach often extend their analysis with such techniques in order to assess the accuracy of the large sample approximations.\n",
    "\n",
    "\n",
    "Some well-known examples building on the classical approach are\n",
    "\n",
    "* **Non-linear Least Squares estimator:** In the case of regression function estimation, we can take \n",
    "\n",
    "\\begin{align*}\n",
    "&\\mathcal A \\subset \\mathcal{F} = Y^X \\quad\\quad \\text{and}\\quad\\quad L(P, \\mu) = \\int_{(Y,X)} (y - \\mu(x))^2P(\\mathrm{d}(y,x))\\quad\\quad\\text{then} \\\\\n",
    "&\\hspace{25mm}\\widehat{\\mu}^C(z^n) = \\text{arg}\\inf\\limits_{\\mu \\in \\mathcal{A}}\\hspace{2mm}  \\frac{1}{n}\\sum_{t=1}^n (y_t - \\mu(x_t))^2\n",
    "\\end{align*}\n",
    "\n",
    "* **Maximum Likelihood Estimator:** In the case of density function estimation, we can take \n",
    "\n",
    "\\begin{align*}\n",
    "&\\mathcal A \\subset \\mathcal{F} = \\{f: Z \\mapsto \\mathbb{R}_+ : \\int_Z f(z)\\mathrm{d}z =  1 \\}\\quad\\quad \\text{and}\\quad\\quad L(p, f) = \\int_{Z} p(z)\\log \\frac{p}{f}(z) \\mathrm{d}z\\quad\\quad\\text{then} \\\\\n",
    "&\\hspace{35mm}\\widehat{f}^C(z^{n}) = \\text{arg}\\inf\\limits_{f \\in \\mathcal{A}}\\hspace{2mm} - \\frac{1}{n}\\sum_{t=1}^n \\log f(z_t) + \\underbrace{H(p)}_{\\text{entropy of }p}\n",
    "\\end{align*}\n",
    "\n",
    "* **GMM estimator:** Having a set of moment restrictions and a positive semi-definite $W$, we can take \n",
    "\n",
    "\\begin{align*}\n",
    "&\\mathcal{A} = \\{g(\\cdot; \\theta) : \\theta \\in \\Theta\\}\\quad\\quad \\text{and}\\quad\\quad L(P, \\theta) = \\left[\\int g(z; \\theta)\\mathrm{d}P(z)\\right]' W \\left[\\int g(z; \\theta)\\mathrm{d}P(z)\\right] \\quad\\quad\\text{then} \\\\\n",
    "&\\hspace{35mm}\\widehat{\\theta}_{W}(z^n) = \\text{arg}\\inf\\limits_{\\theta \\in \\mathcal{A}}\\hspace{2mm} \\left[\\frac{1}{n}\\sum_{t=1}^n g(z_t; \\theta)\\right]' W \\left[\\frac{1}{n}\\sum_{t=1}^n g(z_t; \\theta)\\right]\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification\n",
    "\n",
    "**TOEDIT: Balint**\n",
    "\n",
    "The \"heuristic justification\" is that in large samples consistent decision rules become invariant implying that the risk simply boils down to its loss function evaluated at the best-in-class action. Good generalization ensures that the empirical loss and the true loss are approximately the same and the shrinking variance ensures that the risk aprroximates the loss.\n",
    "\n",
    "$$ L(P_n, d^C(z^n)) \\approx L(P, d^C(z^n)) \\approx R_n(P, d^C) $$ \n",
    "\n",
    "The main difficulty, of course, is to find the critical value for the \"sufficiently large\" $n$. This critical value naturally depends on the *rate of convergence* which itself hinges on the complexity properties of the DGP and the composite loss-action space.\n",
    "\n",
    "### Asymptotic standard errors\n",
    "\n",
    "**TOEDIT: Balint**\n",
    "\n",
    "Although the variation of the decision rule vanishes asymptotically, by employing the \"right\" scaling factor corresponding to the rate of convergence, we can derive a non-degenerate asymptotic distribution for $d^C$. Typically the goal is to establish a limiting distribution via the central limit theorem. For example, when the scaling factor is $\\sqrt{n}$ -- frequently encountered in practice -- $(d^C(z^n) - a^*_{\\mathcal{A}})$ must go to zero fast enough in order to ensure the existence of the following limit\n",
    "\n",
    "$$ \\sqrt{n}\\left(d^C(z^n) -  a^*_{\\mathcal{A}}\\right) \\to \\mathcal{N}(0, \\Sigma_L)\\quad\\quad\\text{as}\\quad n\\to \\infty .$$\n",
    "\n",
    "(Note that here we deal with convergence in the action space and not in the loss space.)\n",
    "\n",
    "To capture the variation stemming from the finiteness of the sample, the classical approach proceeds in the spirit of \"backward induction\": first, derive the asymptotic covariance matrix $\\Sigma_L$ of $\\sqrt{n}d^C$ and then scale it appropriately (by $n$) to obtain the finite sample approximation of the variance of $d^C$.\n",
    "\n",
    "As the notation reveals, alternative loss functions can lead to different estimators and corresponding asymptotic covariance matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example and analysis\n",
    "\n",
    "**TODO: Balint**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "\n",
    "**TOEDIT: Balint**\n",
    "\n",
    "This provides basis to rank decision rules by invoking the criterion of asymptotic efficiency, defined as the smallest possible asymptotic covariance matrix relative to some well-defined class of estimators. (Examples are maximum likelihood and Cramer-Rao lower bound, GMM can be made efficient by choosing the weighting matrix appropriately...) \n",
    "\n",
    "Of course, this type of \"minimization\" of asymptotic covariance matrices does not necessarily imply small finite sample variance of the decision rule. The estimation-approximation error decomposition helps understanding the inherent trade-off.\n",
    "\n",
    "Since the finite sample estimator variance can be linked with the second order term of the Taylor expansion, it might be tempting to view the quest for asymptotic efficiency as a device to minimize the risk of the decision rule. Notice, however, that this method does not take into account the possible *trade-off* between the different moments of the decision rule (i.e. the leading terms in the Taylor expansion of risk). In particular, the classical approach often restricts attention to unbiased estimators and looks for the minimum variance estimator *among this class*. The unbiasedness is gauged only relative to the---restricted---range of the decision rule $\\mathcal{A}$. Hence, even if the decision rule is unbiased we still have misspecification error. Now, it is difficult to defend the merits of an unbiased but misspecified  decision rule with large variance relative to a misspecified decision rule with some bias and smaller variance. \"Clever\" estimators tipically trade-off bias-misspecification and variance flexibly adjusting to the available sample size and the complexity of the estimation problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of the example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example reveals the drawbacks of using asymptotic theory to approximate a decision rule's finite sample performance. A potential measure of discrepancy between the finite sample behavior and the asymptotic one is the   **estimation error** defined as\n",
    "\n",
    "$$\\mathcal{E}_d(P, \\mathcal{A}, n) := R_n(P, d) - \\inf_{a\\in\\mathcal{A}} \\ L(P, a).$$\n",
    "\n",
    "While the risk $R_n(P, d)$ captures the performance of $d$ in samples of size $n$, $L(P, a^*_{P, L, \\mathcal{A}})$ essentially encodes its asymptotic properties and from the consistency of $d$ it follows that \n",
    "\n",
    "$$\\lim_{n\\to \\infty} \\ \\mathcal{E}_d(P, \\mathcal{A}, n) \\overset{P}{=} 0.$$\n",
    "\n",
    "In other words, even if $d$ is consistent relative to $(\\mathcal{A}, \\mathcal{H})$, its finite sample behavior still hinges on the range of $d$, i.e. the action space $\\mathcal{A}$.\n",
    "\n",
    "Recall that the origin of estimation error is the fact that we do not know $P$, instead we have to use the information in the (finite) sample to approximate the 'best' action in $\\mathcal{A}$. Intuitively, the smaller the estimation error the better this approximation. Given a decision rule and a finite sample at hand we would like to know how close the empirical loss and the true loss are. Making sure that these two quantities are close to each other ensures that the empirical loss is informative about the true loss. This property is usually referred to as **generalization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization\n",
    "\n",
    "For a fixed finite sample $z^n$, we say that an assigned action $d(z^n)\\in \\mathcal{A}$ *generalizes well*, if the quantity  \n",
    "\n",
    "$$\\left|L(P, d(z^n)) - L(P_n, d(z^n))\\right|\\quad \\text{is small}.$$\n",
    "\n",
    "Note that this property does *not* require that the empricial loss is itself small, which is the objective function of the classical ELM approach. It only requires that the empirical loss is close to the true loss.\n",
    "\n",
    "This sheds some light on what can go wrong with the ELM approach in finite samples. In practice, one of the worst situations is **overfitting**, that is, when the empirical loss is much smaller than the true loss, hence our assessment of the quality of $d(z^n)\\in\\mathcal{A}$ might be overly optimistic.\n",
    "\n",
    "\n",
    "**Roadmap**\n",
    "\n",
    "Note that the generalization property depends on the particular realization of the sample. The realized sample determines the chosen action, $d(z^n)\\in\\mathcal{A}$, and the empirical distribution, $P_n$. In order to give statements regarding the generalization property that extends to more than one paritcular realization of the sample the following steps are taken:\n",
    "\n",
    "* In order to resolve the uncertainty about the chosen action, $d(z^n)$, consider all the actions that are in the range of the decision rule, $\\mathcal{A}$.\n",
    "* In order to resolve the uncertainty about the empirical distribution either\n",
    "    * take expectations or\n",
    "    * characterize where the random variable concentrates via tail bounds.\n",
    "* Give statements which apply uniformly for data generating processes in a given class, $P\\in\\mathcal{H}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolving variation across actions\n",
    "\n",
    "Extending the generalization property to all actions in the range of $d$, $a \\in\\mathcal{A}$, leads to the notion of **generalization error**, defined as\n",
    "\n",
    "$$ \\Delta(P, z^n, \\mathcal{A}) := \\sup_{a\\in\\mathcal{A}} \\ \\left|L(P, a) - L(P_n, a)\\right|.$$\n",
    "\n",
    "When the loss functional takes the form $L(P, a) = \\int l(a, z)\\mathrm{d}P(z)$ then the generalization error is the supremum of a scaled empirical process indexed by the function class $\\mathcal{L}_\\mathcal{A}$. The finite sample techniques discussed in {reference asymptotic notebook} prove to be useful to characterize the behavior of $\\Delta(P, z^n, \\mathcal{A})$.\n",
    "\n",
    "\n",
    "#### Resolving uncertainty about the empirical distribution\n",
    "\n",
    "**Tail bounds**\n",
    "\n",
    "To draw uniform inference about the generalization properties of $d$, we can use probabilistic tail bounds for $\\Delta(P, z^n, \\mathcal{A})$.  One of the key defining element of the tail bounds is the complexity of the class $\\mathcal{L}_\\mathcal{A}$.\n",
    "\n",
    "We can apply {last theorem asymptotic notebook} in the current setting. For uniformly bounded functions $\\lvert l_a \\rvert_{\\infty} < 1 \\ \\forall l_a  \\in\\mathcal{L}_\\mathcal{A}$ if $\\delta - 2 \\mathcal{R}_n(\\mathcal{L}_\\mathcal{A}) > 0$ then\n",
    "\n",
    "$$ P\\Big\\{ \\Vert P_n - P \\Vert_{\\mathcal{L}_\\mathcal{A}} \\geq  \\delta \\Big\\} \\leq 2 \\exp\\Big\\{- n 2 \\big(\\delta - 2 \\mathcal{R}_n(\\mathcal{L}_\\mathcal{A})\\big)^2 \\Big\\}. $$\n",
    "\n",
    "\n",
    "Probabilistically, the lower the complexity of $\\mathcal{L}_\\mathcal{A}$ the better the generalization ability of the decision rule.\n",
    "\n",
    "**ADD:** rewriting concentration bound to hold with high probability and maybe replace the '1' bound factor by 'B'\n",
    "\n",
    "\n",
    "**Average generalization error**\n",
    "\n",
    "A somewhat less ambitious approach is to focus on the average generalization error, i.e. \n",
    "\n",
    "$$\\mathbb{E}_{Z^n}\\left[ \\Delta(P, z^n)\\right] = \\int_{Z^n} \\sup_{a\\in\\mathcal{A}} \\ \\left|L(P, a) - L(P_n, a)\\right| \\mathrm{d}P(z^n).$$\n",
    "\n",
    "Naturally, by bounding the tail probabilities of $\\Delta$ we control the mean as well. In fact ,with some technical care---using a symmetrization argument---one can bound the expectation of the generalization error using the Rademacher complexity of the class $\\mathcal{L}_\\mathcal{A}$,\n",
    "\n",
    "$$\\mathbb{E}_{Z^n}\\left[ \\Delta(P, z^n)\\right]\\leq 2\\mathcal{R}_n(\\mathcal{L}_\\mathcal{A}).$$\n",
    "\n",
    "The important message is that in order to control the generalization property of the decision rule we need to limit the complexity of $\\mathcal{L}_\\mathcal{A}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Digression -- estimation error and generalization** \n",
    "\n",
    "It turns out that the average generalization error of the ELM decision rule $d^C$ is tightly linked to its estimation error. To see this, consider the following decomposition of the excess loss for a given realization of the sample $z^n$   \n",
    "\n",
    "\\begin{align*}\n",
    "L(P, d^C(z^n)) - L(P, a^*_{\\mathcal{A}}) = & \\Big(L(P, d^C(z^n)) - L(P_n, d^C(z^n))\\Big) \\\\\n",
    "+ &\\underbrace{\\Big(L(P_n, d^C(z^n)) - L(P_n, a^*_{\\mathcal{A}}) \\Big)}_{\\leq 0} + \\Big(L(P_n,  a^*_{\\mathcal{A}}) - L(P,a^*_{\\mathcal{A}}) \\Big)\\quad\\quad  (1)\n",
    "\\end{align*}\n",
    "\n",
    "and take the expectations over $Z^n$ to arrive at \n",
    "\n",
    "$$\\mathcal{E}_{d^C}(P, \\mathcal{A}, n) =  R_n(P, d^C) - L(P, a^*_{\\mathcal{A}}) \\leq \\mathbb{E}_{Z^n}\\Big[\\sup_{a\\in\\mathcal{A}}\\{L(P, a) - L(P_n, a)\\} \\Big] =  \\mathbb{E}_{Z^n}\\left[ \\Delta(P, z^n)\\right]$$\n",
    "\n",
    "* The second term on the RHS is nonpositive, because the decision rule is based on ELM, so  $L(P_n, d(z^n)) \\leq L(P_n, a) \\ \\forall a\\in\\mathcal{A}$. \n",
    "* The last term disappears when we take the expectation, as we assumed the form $L(P,a) = \\int_Z l(a, z)\\mathrm{d}P(z)$ of the loss function and $\\mathbb{E}_{Z^n}[P_n]=P$.\n",
    "\n",
    "This suggests that by seeking good generalization performance of the ELM estimator the statistician can efficiently control the estimation error as well, thus making sure that the asymptotic analysis provides a relatively good approximation of the finite sample properties of $d^C$.\n",
    "\n",
    "It is easy to see that one could always make the estimation error and generalization error zero by choosing a constant decision rule -- that is, one which range is a singleton and hence assigns the same action to each possible realization of the sample. However, that decision rule would ignore all the information that is available in the data -- the source of information for statistical inference. The approach of statistical learning theory attempts to balance this trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Learning Theory -- controlling complexity\n",
    "\n",
    "A criticism of the classical approach is that it does not deal with the generalization problem arising in finite samples. Statistical learning theory takes a somewhat different approach and to balance good generalization and low estimation error with small approximation error.   \n",
    "\n",
    "Again, the objective is to minimize the excess risk of the decision rule. As seen earlier the estimation-approximation error decomposition highlights one of the main dilemmas the statistician is facing\n",
    "\n",
    "$$ \\underbrace{R_n(P, d) - L\\left(P, a^{*}_{L, P, \\mathcal{F}} \\right)}_{\\text{excess risk}} =  \\underbrace{R_n(P, d) - L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)}_{\\substack{\\text{estimation error} \\\\ \\text{random}}} + \\underbrace{L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)- L\\left(P, a^{*}_{L, P, \\mathcal{F}} \\right)}_{\\substack{\\text{approximation error} \\\\ \\text{deterministic}}}. $$\n",
    "\n",
    "* The approximation error captures the idea that the true feature of the DGP does not lie in the range of the decision rule, hence there is an inherent error due to this misspecification. Correspondingly, ceteris paribus enlarging the action space -- the range of the decision rule -- the approximation error gets smaller. As $\\mathcal{A}$ approaches $\\mathcal{F}$ the approximation error should vanish.\n",
    "\n",
    "* However, the range of the decision rule also plays a key role in the size of the estimation error and its ability to generalize. The non-asymptotic tail bounds teach us that in order to achieve low estimation error and good generalization the complexity of the class $\\mathcal{L}_\\mathcal{A}$ has to be small. The complexity is weakly increasing in the action space, $\\mathcal{A}$.\n",
    "\n",
    "The above trade-off---inherent in all statistical inference problems---can be visualized on the following graph.\n",
    "\n",
    "<img src=\"./decomp.png\", width=600, height=440>\n",
    "\n",
    "In terms of the action space of the decision rule,\n",
    "\n",
    "* whenever the gain from smaller approximation error exceeds the loss from greater estimation error one should increase the action space, there is **underfitting**.\n",
    "* whenever the gain from smaller estimation error exceeds the loss from greater approximation error one should decrease the action space, there is **overfitting**.\n",
    "\n",
    "An ideal decision rule traces the minimum of the U shaped excess risk. By controlling the range of the decision rule, the action space $\\mathcal{A}$, the approach of statistical learning theory can balance the trade-off between the estimation error and the approximation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling complexity through the action space  \n",
    "\n",
    "Based on the previous discussion on how the size of the action space affects both estimation and approximation errors the modern approaches to statistical inference explicitly control for complexity through the decision rule's action space. This ensures limited generalization and estimation errors in small samples while also reduces misspecification asymptotically.\n",
    "\n",
    "In this sense, we can think of the corresponding decision rule -- $d_{SLT}$ -- in terms of a sequence of action spaces.\n",
    "\n",
    "$$ \\mathcal{D}_{ELM}\\left(\\{\\mathcal{A}_k \\}_{k \\geq 1}\\right) := \\left\\{ d: \\mathcal{S} \\mapsto \\mathcal{F} \\ \\Big| \\ \\ \\forall z^{\\infty}, \\ \\ d(z^n) := \\inf_{a\\in\\mathcal{A}_{k(z^n)}} \\ L(P_n, a) ,\\ \\ \\text{s.t.} \\ \\  \\mathcal{A}_k\\subseteq \\mathcal{A}_{k+1}\\subseteq \\mathcal{F}, \\ \\ \\forall n \\geq 1\\right\\}, $$\n",
    "\n",
    "where $k: \\mathcal{S} \\mapsto \\mathbb{N}$ is a data-dependent subsequence of $k$.\n",
    "\n",
    "This approach, of course, nests the classical frequentist one by setting $\\mathcal{A}_n = \\mathcal{A}\\subseteq \\mathcal{F}$, $\\forall n\\geq 1$.\n",
    "\n",
    "For a given realization of the sample one would like to select the class of actions such that the corresponding empirical loss minimizer achieves almost optimal excess risk. Automating the sepcification of the subsequence---and hence the chosen action space---to depend solely on the realized sample is called model selection. One of the most common form of model selection is the so called penalized empirical loss minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalized empirical loss minimization\n",
    "\n",
    "**Add Oracle inequalities**\n",
    "\n",
    "Most often one encounters the above procedure in the following form. At each sample size $n$ the constrained optimization problem\n",
    "\n",
    "$$ d_{SLT}(z^n) := \\inf_{a\\in\\mathcal{A}_n} \\ L(P_n, a) $$\n",
    "\n",
    "is recast as an unconstrained optimization problem via the method of Lagrange multipliers,\n",
    "\n",
    "$$ d_{SLT}(z^n) := \\inf_{a\\in\\mathcal{F}} \\ L(P_n, a) + \\Gamma (a, n). $$\n",
    "\n",
    "The general function $\\Gamma$ is the penalty term determining the effective size of the action space -- and hence determining $\\mathcal{A}_n$ in the sequence.\n",
    "\n",
    "Often $\\Gamma(a, n)$ takes the specific form $\\Gamma(a, n) = \\kappa_n C(a)$ where C is a general measure of the complexity of a single action and $\\kappa_n$ is a tuning parameter determined flexibly with the sample size and the quality of the fit -- usuallyt through cross-validation. (**TODO**: here we should probably have a brief digression on how we can jump from the complexity of sets to the complexity of one single action.)\n",
    "\n",
    "By choosing the action space, the loss function and the penalty term many of the well known machine learning techniques can be treated simultaneously in the introduced framework. The lasso and ridge regressions, support vector machines or regularization networks can be seen as particular specifications of the defined objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example reveals the danger of using asymptotic theory to approximate a decision rule's finite sample performance. A possible measure that can inform us about how well the asymptotic distribution approximates the finite sample properties is the **estimation error** defined above as\n",
    "\n",
    "$$\\mathcal{E}_d(P, \\mathcal{A}, n) := R_n(P, d) - \\inf_{a\\in\\mathcal{A}} \\ L(P, a).$$\n",
    "\n",
    "While the risk $R_n(P, d)$ captures the performance of $d$ in samples of size $n$, $L(P, a^*_{P, L, \\mathcal{A}})$ essentially encodes its asymptotic properties and from the consistency of $d$ it follows that \n",
    "\n",
    "$$\\lim_{n\\to \\infty} \\ \\mathcal{E}_d(P, \\mathcal{A}, n) \\overset{P}{=} 0.$$\n",
    "\n",
    "In other words, even if $d$ is consistent relative to $(\\mathcal{A}, \\mathcal{H})$, its finite sample behavior hinges on the range of $d$, i.e. the action space $\\mathcal{A}$.\n",
    "\n",
    "Recall that the origin of estimation error is the fact that we do not know $P$, instead we have to use the information in the (finite) sample to approximate the 'best' action in $\\mathcal{A}$. Intuitively, the smaller the estimation error the better this approximation. This consideration is closely related to the idea of **generalization**: the key issue in learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization\n",
    "\n",
    "Endowed with a fixed finite sample $z^n$, we say that a particular action $a\\in\\mathcal{A}$ *generalizes well*, if the quantity  \n",
    "\n",
    "$$\\left|L(P, a) - L(P_n, a)\\right|\\quad \\text{is small}$$\n",
    "\n",
    "that is, if the discrepancy between the empricial and true loss is \"relatively small\". Note that this property does *not* require that the empricial loss is itself small, which is the objective function of the classical ELM approach.\n",
    "\n",
    "**ADD ROAD MAP: how to resolve $A$, $z^n$, $P$**\n",
    "\n",
    "This sheds some light on what can go wrong with the ELM approach in finite samples. In practice, one of the worst situations is **overfitting**, that is when the empirical loss is much smaller than the true loss, hence our assessment of the quality of $a\\in\\mathcal{A}$ might be overly optimistic.\n",
    "\n",
    "Extending the generalization property to $\\forall a \\in\\mathcal{A}$ leads to the notion of **generalization error**, defined as\n",
    "\n",
    "$$ \\Delta(P, z^n, \\mathcal{A}) := \\sup_{a\\in\\mathcal{A}} \\ \\left|L(P, a) - L(P_n, a)\\right|$$\n",
    "\n",
    "The decision rule derived through ELM will generalize well from the given sample $z^n$, if the associated $\\Delta$ is small.\n",
    "\n",
    "Notice that this error still hinges on the particular sample $z^n$. To draw uniform inference about the generalization properties of $d$, in notebook {reference asymptotics}, we constructed probabilistic tail bounds for $\\Delta(P, z^n, \\mathcal{A})$, for which the key building block was the complexity of the action space $\\mathcal{A}$.  \n",
    "\n",
    "A somewhat less ambitious approach is to focus on the average generalization error, i.e. \n",
    "\n",
    "$$\\bar \\Delta(P, \\mathcal{A}) := \\int_{Z^n} \\sup_{a\\in\\mathcal{A}} \\ \\left|L(P, a) - L(P_n, a)\\right| \\mathrm{d}P(z^n) = \\mathbb{E}_{Z^n}\\left[ \\Delta(P, z^n)\\right]$$\n",
    "\n",
    "Naturally, by bounding the tail probabilities of $\\Delta$ we control the mean as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation error and generalization\n",
    "\n",
    "It turns out that the average generalization error of the ELM decision rule $d^C$ is tightly linked to its estimation error. To see this, consider the following decomposition of the excess loss for a given realization of the sample $z^n$   \n",
    "\n",
    "\\begin{align*}\n",
    "L(P, d^C(z^n)) - L(P, a^*_{\\mathcal{A}}) = & \\Big(L(P, d^C(z^n)) - L(P_n, d^C(z^n))\\Big) \\\\\n",
    "+ &\\underbrace{\\Big(L(P_n, d^C(z^n)) - L(P_n, a^*_{\\mathcal{A}}) \\Big)}_{\\leq 0} + \\Big(L(P_n,  a^*_{\\mathcal{A}}) - L(P,a^*_{\\mathcal{A}}) \\Big)\\quad\\quad  (1)\n",
    "\\end{align*}\n",
    "\n",
    "and take the expectations over $Z^n$ to arrive at \n",
    "\n",
    "$$\\mathcal{E}_{d^C}(P, \\mathcal{A}, n) =  R_n(P, d^C) - L(P, a^*_{\\mathcal{A}}) \\leq \\mathbb{E}_{Z^n}\\Big[\\sup_{a\\in\\mathcal{A}}\\{L(P, a) - L(P_n, a)\\} \\Big] = \\bar \\Delta(P, \\mathcal{A})$$\n",
    "\n",
    "\n",
    " * In (1) the second term on the RHS is nonpositive, because the decision rule is based on ELM, so  $L(P, d(z^n)) \\leq L(P_n, a) \\ \\forall a\\in\\mathcal{A}$. \n",
    " * The last term disappears when we take the expectation, as we assumed the form $L(P,a) = \\int_Z l(a, z)\\mathrm{d}P(z)$ of the loss function and $\\mathbb{E}_{Z^n}[P_n]=P$.\n",
    "\n",
    "\n",
    "This suggests that by seeking good generalization performance of the ELM estimator the statistician can efficiently control the estimation error as well, thus making sure that the asymptotic analysis provides a relatively good approximation of the finite sample properties of $d^C$. As we saw in the last notebook, the key device to curb the average generalization error is to choose an action space $\\mathcal{A}$ with small complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**coming from bias-variance-misspecification decomposition...**\n",
    "\n",
    "From this somewhat naive perspective, if the \"initial\" action space is \"too small\", there is no way to get rid of misspecification even if the sample size becomes astronomical. The only reasoning that might justify this approach is the *belief* that the model was correctly specified in the first place.   \n",
    "\n",
    "Anticipating the key idea behind modern approaches to inference, it is worth contemplating the possibility that the action space itself depends on the sample size as a result of the statistician's desire to reduce misspecification at least asymptotically. This requires a slight adjustment in the definition of the ELM decision rules. In particular, define the class of decision rules indexed by the sequence $\\{\\mathcal{A}_n\\}_{n\\geq 1}$ as follows\n",
    "\n",
    "$$ \\mathcal{D}_{ELM}\\left(\\{\\mathcal{A}_n\\}_{n\\geq 1}\\right) := \\left\\{ d: \\mathcal{S} \\mapsto \\mathcal{F} \\ \\left| \\ \\ \\forall z^{\\infty}, \\ \\ d(z^n) := \\inf_{a\\in\\mathcal{A}_n} \\ L(P_n, a) ,\\ \\ \\text{s.t.} \\ \\  \\mathcal{A}_n\\subseteq \\mathcal{A}_{n+1}\\subseteq \\mathcal{F}, \\ \\ \\forall n \\geq 1\\right\\}\\right. $$\n",
    "\n",
    "A special case, of course, is the original scenario, when $\\mathcal{A}_n = \\mathcal{A}\\subseteq \\mathcal{F}$, $\\forall n\\geq 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "## Statistical Learning Theory -- the minimax approach\n",
    "\n",
    "\n",
    "As seen above the classical approach does not deal with the generalization problem arising in finite samples. Statistical learning theory takes a somewhat different approach controlling for the estimation error -- while still keeping an eye on the approximation error.   \n",
    "\n",
    "Again, the objective is to minimize the excess risk of the decision rule. As seen earlier the estimation-approximation error decomposition highlights one of the main dilemmas the statistician is facing\n",
    "\n",
    "$$ \\underbrace{R_n(P, d) - L\\left(P, a^{*}_{L, P, \\mathcal{F}} \\right)}_{\\text{excess risk}} =  \\underbrace{R_n(P, d) - L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)}_{\\substack{\\text{estimation error} \\\\ \\text{random}}} + \\underbrace{L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)- L\\left(P, a^{*}_{L, P, \\mathcal{F}} \\right)}_{\\substack{\\text{approximation error} \\\\ \\text{deterministic}}}. $$\n",
    "\n",
    "By controlling the decision rule -- or more precisely the range of the decision rule, the action space $\\mathcal{A}$ -- the statistician can balance the trade-off between the estimation error and the approximation error.\n",
    "\n",
    "* The approximation captures the idea that the true feature of the DGP does not lie in the range of the decision rule, hence there is an inherent error due to this misspecification. Correspondingly, ceteris paribus enlarging the action space -- the range of the decision rule -- the approximation error gets smaller. As $\\mathcal{A}$ approaches $\\mathcal{F}$ the approximation error should vanish.\n",
    "\n",
    "* However, the range of the decision rule also plays a key role in the size of the estimation error. Indeed, it is easy to see that one could always make the estimation error zero by choosing a constant decision rule -- that is, one which range is a singleton and hence assigns the same action to each possible realization of the sample. \n",
    "\n",
    "Typically, this trade-off inherent in all statistical inference problems can be visualized on the following graph.\n",
    "\n",
    "<img src=\"./decomp.png\", width=600, height=440>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theoretical trade-off when dealing with decision rules based on the empirical loss minimization priciple is the following. In terms of the action space,\n",
    "\n",
    "* whenever the gain from smaller approximation error exceeds the loss from greater estimation error one should increase the action space, there is **underfitting**.\n",
    "* whenever the gain from smaller estimation error exceeds the loss from greater approximation error one should decrease the action space, there is **overfitting**.\n",
    "\n",
    "An ideal decision rule based on the analogy principle traces the minimum of the U shaped excess risk.\n",
    "\n",
    "Next, we take a closer look at how the estimation error depends on the decision rule -- in particular its range.\n",
    "\n",
    "As shown above the estimation error and the generalization property are tightly connected.\n",
    "\n",
    "$$ \\underbrace{R_n(P, d) - L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)}_{\\text{estimation error}} \\leq \\mathbb{E}_{Z^n}\\Big[\\sup_{a\\in\\mathcal{A}}\\{L(P, a) - L(P_n, a)\\} \\Big] = \\bar \\Delta(P, \\mathcal{A}) $$\n",
    "\n",
    "The tail bounds for empirical processes discussed in notebook {reference asymptotics} are useful to get a grasp on the generalization error $\\bar \\Delta(P, \\mathcal{A})$. In fact, one can bound it in terms of the Rademacher complexity of the composite loss-action space, $\\mathcal{L}$. \n",
    "\n",
    "$$ \\mathbb{E}_{Z^n}\\Big[\\sup_{a\\in\\mathcal{A}}\\{L(P, a) - L(P_n, a)\\} \\Big] = \\bar \\Delta(P, \\mathcal{A}) \\leq 2 \\mathcal{R}_n\\left(\\mathcal{L}\\right) $$\n",
    "\n",
    "The Rademacher complexity of $\\mathcal{L}:= \\{l(a, \\cdot) : a \\in \\mathcal{A}\\}$ is increasing in the action space, $\\mathcal{A}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling the complexity of the action space\n",
    "\n",
    "Based on the previous discussion on how the size of the action space affects both estimation and approximation errors through the complexity of $\\mathcal{L}$ the approach of statistical learning theory is to explicitly control for complexity through the decision rule's action space.\n",
    "\n",
    "In this sense the corresponding decision rule -- $d_{SLT}$ -- can be defined(?) in terms of a sequence of action spaces\n",
    "\n",
    "$$ \\mathcal{D}_{ELM}\\left(\\{\\mathcal{A}_n\\}_{n\\geq 1}\\right) := \\left\\{ d: \\mathcal{S} \\mapsto \\mathcal{F} \\ \\left| \\ \\ \\forall z^{\\infty}, \\ \\ d(z^n) := \\inf_{a\\in\\mathcal{A}_n} \\ L(P_n, a) ,\\ \\ \\text{s.t.} \\ \\  \\mathcal{A}_n\\subseteq \\mathcal{A}_{n+1}\\subseteq \\mathcal{F}, \\ \\ \\forall n \\geq 1\\right\\}\\right. $$ \n",
    "\n",
    "The rationale behind this procedure is that one should increase the action space and fit more flexible and complex models if the quantity and quality of the data increase. As a result, the effective $\\mathcal{A}_n$ is defined by the quantity of data at hand -- the more the data the more flexible model one can afford to fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalized empirical loss minimization\n",
    "\n",
    "Most often one encounters the above procedure in the following form. At each sample size $n$ the constrained optimization problem\n",
    "\n",
    "$$ d_{SLT}(z^n) := \\inf_{a\\in\\mathcal{A}_n} \\ L(P_n, a) $$\n",
    "\n",
    "is recast as an unconstrained optimization problem via the method of Lagrange multipliers,\n",
    "\n",
    "$$ d_{SLT}(z^n) := \\inf_{a\\in\\mathcal{F}} \\ L(P_n, a) + \\Gamma (a, n). $$\n",
    "\n",
    "The general function $\\Gamma$ is the penalty term determining the effective size of the action space -- and hence determining $\\mathcal{A}_n$ in the sequence.\n",
    "\n",
    "Often $\\Gamma(a, n)$ takes the specific form $\\Gamma(a, n) = \\kappa_n C(a)$ where C is a general measure of the complexity of a single action and $\\kappa_n$ is a tuning parameter determined flexibly with the sample size and the quality of the fit -- usuallyt through cross-validation. (**TODO**: here we should probably have a brief digression on how we can jump from the complexity of sets to the complexity of one single action.)\n",
    "\n",
    "By choosing the action space, the loss function and the penalty term many of the well known machine learning techniques can be treated simultaneously in the introduced framework. The lasso and ridge regressions, support vector machines or regularization networks can be seen as particular specifications of the defined objects. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Examples for the SLT approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----------------------------------------------------\n",
    "# Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A) Taylor-expansion of the risk functional\n",
    "\n",
    "Remember that the risk of a decision rule $d$ is given by the following expression.\n",
    "\n",
    "$$ R_n(P, d) := \\int\\limits_{Z^n} L(P, d(z^n)) \\ \\mathrm{d} P(z^n) $$\n",
    "\n",
    "Consider the Taylor expansion of this functional with respect to the decision rule around a particular $d$. For any alternative decision rule $\\tilde{d}$, we can define the difference\n",
    "\n",
    "$$\\tilde{d} - d := \\lambda \\eta(z^n)\\quad \\quad \\text{where}\\quad \\eta: \\mathcal{S} \\mapsto \\mathcal{A}, \\quad \\lambda\\in\\mathbb{R}_+$$\n",
    "\n",
    "and then the second-order Taylor expansion of the risk functional around $d$ is\n",
    "\n",
    "$$ R_n\\left(P, \\tilde{d}\\right) = R_n\\left(P, d \\right) + \\int_{Z^n} \\frac{\\partial L(P, d(z^n))}{\\partial a}\\lambda\\eta(z^n)\\mathrm{d} P(z^n) + \\int_{Z^n} \\frac{\\partial^2 L(P, d(z^n))}{\\partial a^2}\\frac{\\lambda^2\\eta(z^n)^2}{2}\\mathrm{d} P(z^n) + O(\\lambda^{3})$$\n",
    "\n",
    "where we use the notion of Gateaux differential (generalization of directional derivate) to obtain the marginal change in the loss function as the abstract $a$ changes. \n",
    "\n",
    "An important reference point of any decision rule $d$ is the expected action that it provides for a given sample size $n$,\n",
    "\n",
    "$$\\bar{d}_n := \\int_{Z^n} d(z^n)\\ \\mathrm{d}P(z^n)$$ \n",
    "\n",
    "which does not necessarily belong to $\\mathcal{A}$. In what follows, we imagine a decision rule $\\bar{d}_n\\mathbf{1}(z^n)$ that assigns the value $\\bar{d}_n$ to all sample realization $z^n$ and use the Taylor approximation around this hypothetical decision rule to approximate the risk of $d$. In this case, $d - \\bar{d}_n\\mathbf{1} := \\lambda \\eta(z^n)$ and \n",
    "\n",
    "$$ R_n\\left(P, d\\right) = L\\left(P, \\bar d_n \\right) + \\int_{Z^n} \\frac{\\partial^2 L(P, \\bar d_n)}{\\partial a^2}\\frac{(d - \\bar{d}_n\\mathbf{1})^2}{2}\\mathrm{d} P(z^n) + O(\\lambda^{3})$$\n",
    "\n",
    "where the first-order term vanishes because the partial -- evaluated at $\\bar d_n\\mathbf{1}$ -- is a constant and $\\int_{Z^n}(d - \\bar d_n\\mathbf{1}) \\mathrm{d}P = 0$. Note that in this expression the second-order term encodes the theoretical variation of the action that $d$ assigns to random samples of size $n$. The regular variance formula is altered by (one half of) the second derivative of the loss function (evaluated at $\\bar d_n$), representing the role of the loss functions's curvature in determining the decision rule's volatility. As a result, a reasonable measure for the decision rule's volatility can be defined as $R_n\\left(P, d\\right) - L\\left(P, \\bar d_n \\right)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B) Bias-variance-misspecification decomposition of GMM\n",
    "\n",
    "The elements of the problem are\n",
    "\n",
    "* *Observable:* $Z \\sim P$, with given moment conditions $g: Z \\times \\mathbb{R}^{p+m} \\mapsto \\mathbb{R}^m$ \n",
    "* *Action space:* $\\mathcal{A} = \\Theta \\subseteq \\mathbb{R}^p$\n",
    "* *Admissible space:* $\\mathcal{F} = \\Theta'\\equiv \\Theta \\times \\mathbb{R}^m$, so that we can always set the expectation of $g$ equal to zero by means of the $m$ auxiliary parameters.  \n",
    "* *Loss function:* $L(P, a) = \\left(\\int_Z g(z, a) \\mathrm{d}P(z)\\right)'W\\left(\\int_Z g(z, a) \\mathrm{d}P(z)\\right)$\n",
    "\n",
    "Then the minimal loss is zero (by construction), i.e. $L(P, a^{*}_{P, \\mathcal{F}}) = 0$. \n",
    "\n",
    "The loss evaluated at the best-in-class action $a^*_{P, \\mathcal{A}} = \\inf_{a\\in\\mathcal{A}} \\ L(P, a)$, is\n",
    "\n",
    "$$L\\left(P, a^*_{P, \\mathcal{A}}\\right) = \\mathbb{E}_P\\left[ g\\left(z, a^{*}_{P, \\mathcal{A}}\\right) \\right]' W \\mathbb{E}_P\\left[ g\\left(z, a^{*}_{P, \\mathcal{A}}\\right) \\right] = \\text{misspecification}$$\n",
    "\n",
    "For the bias term we substract this quantity from the loss evaluated at the average action $\\bar d_n(z) := \\int_{Z^n} a_{z^n} \\mathrm{d}P(z^n)$ \n",
    "\n",
    "$$L\\left(P, \\bar d_n\\right) - L\\left(P, a^*_{P, \\mathcal{A}}\\right) = \\mathbb{E}_P\\left[ g\\left(z, \\bar d_n \\right) \\right]' W \\mathbb{E}_P\\left[ g\\left(z, \\bar d_n \\right) \\right]  - \\mathbb{E}_P\\left[ g\\left(z, a^{*}_{P, \\mathcal{A}}\\right) \\right]' W \\mathbb{E}_P\\left[ g\\left(z, a^{*}_{P, \\mathcal{A}}\\right) \\right]  = \\text{bias}$$\n",
    "\n",
    "We approximate the volatility term with the second-order term of the Taylor expansion. For simplicity, make use of the following notation \n",
    "\n",
    "$$D(a) := \\mathbb{E}_P\\left[ \\frac{\\partial g(z, a)}{\\partial a}\\right] \\in \\mathbb{R}^{m\\times p} \\quad\\quad H(a) := \\mathbb{E}_P\\left[ \\frac{\\partial^2 g(z, a)}{\\partial a^2}\\right] \\in \\mathbb{R}^{p\\times p\\times m}$$\n",
    "\n",
    "and so \n",
    "\n",
    "$$\\frac{\\partial L(P, a)}{\\partial a} = 2 D(a)' W \\mathbb{E}_P\\left[ g(z, a)\\right]\\in \\mathbb{R}^{p} \\quad \\quad \\frac{\\partial^2 L(P, a)}{\\partial a^2} = 2 H(a) W \\mathbb{E}_P\\left[ g(z, a)\\right] + 2 D(a)' W D(a) \\in \\mathbb{R}^{p\\times p}$$\n",
    "\n",
    "implying the approximation \n",
    "\n",
    "$$R_n(P, d) - L(P, \\bar d_n)\\approx \\int_{Z^n} (d(z^n) - \\bar d_n \\mathbf{1}(z^n))'\\left[ \\underbrace{H(\\bar d_n) W  g(z, \\bar d_n)}_{\\to 0 \\ \\text{as} \\ n\\to \\infty} + D(\\bar d_n)' W D(\\bar d_n)\\right](d(z^n) - \\bar d_n \\mathbf{1}(z^n)) \\mathrm{d}P(z^n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
