{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coping with finite samples\n",
    "\n",
    "#### December 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From asymptotics to finite samples\n",
    "\n",
    "In notebook {reference asymptotics} we took an asymptotic approach in that we focused on how the decision rule behaves as the sample size grows to infinity. The main features we set as good measures of behavior were consistency and fast rate of convergence.\n",
    "\n",
    "The important conclusion we draw was that both consistency and the rate of convergence are driven by the complexity of the composite loss-action space, $\\mathcal{L}$. Finiteness of model complexity ensures consistency and given consistency, the size of the complexity affects the rate of convergence.\n",
    "\n",
    "This notebook explores implications of dealing with a finite sample size relative to the infinite, asymptotic case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation-approximation decomposition  \n",
    "\n",
    "We saw that a consistent decision rule, $d: \\mathcal{S} \\mapsto \\mathcal{A}$, satisfies $L(P, d(z^n)) \\overset{P}{\\to} L(P, a^*_{L, P, \\mathcal{A}})$, hence, the associated loss has a degenerate asymptotic distribution centered around the loss of the best-in-class action (\"asymptotic risk\"). One of the central objects of the finite sample analysis concerns how the finite sample risk deviates from this (asymptotic) best-in-class loss, the so called **estimation error**\n",
    "\n",
    "$$ R_n(P, d) - L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right) $$\n",
    "\n",
    "More generally, one would like to assess the performance of decision rule $d$ relative to the true feature $\\gamma(P)=a^*_{L, P, \\mathcal{F}}$ of the DGP, with $\\mathcal{F}$ denoting the set of all admissible actions, in which case the variable of interest is the **excess risk** \n",
    "\n",
    "$$ R_n(P, d) - L\\left(P, a^*_{L, P, \\mathcal{F}} \\right) $$\n",
    "\n",
    "Decomposing this excess risk highlights one of the most important tensions underlying finite sample inference problems. We can write\n",
    "\n",
    "$$ R_n(P, d) - L\\left(P, a^*_{L, P, \\mathcal{F}}  \\right) =  \\underbrace{R_n(P, d) - L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)}_{\\substack{\\text{estimation error} \\\\ \\text{random}}} + \\underbrace{L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)- L\\left(P, a^*_{L, P, \\mathcal{F}}  \\right)}_{\\substack{\\text{approximation error} \\\\ \\text{deterministic}}}. $$\n",
    "\n",
    "As we noted earlier the **approximation error** usually referred to as *misspecification* is stemming from the fact that the true feature might lie outside of the action space. Intuitively, as we enlarge the action space $\\mathcal{A}$, the approximation error gets weakly smaller and the estimation error gets weakly larger. An ideal decision rule balances this trade-off.\n",
    "\n",
    "Much of what follows next concerns the behavior of the terms in the above decomposition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-volatility-misspecification decomposition \n",
    "\n",
    "A well known concept in statistics is the bias-variance trade-off concerning the estimation error of a decision rule. Here we give a slightly more general version which accommodates the possibility of misspecification.\n",
    "\n",
    "TAYLOR \n",
    "\n",
    "\n",
    "We arrive at the decomposition of excess risk\n",
    "\n",
    "$$ R_n(P, d) - L\\left(P, a^*_{L, P, \\mathcal{F}} \\right) = \\underbrace{R_n\\left(P, d\\right) - L\\left(P, \\bar d_n \\right)}_{\\text{volatility}} + \\underbrace{L\\left(P, \\bar{d_n}\\right) - L\\left(P, a^{*}_{L,P,  \\mathcal{A}}\\right)}_{\\text{bias}} + \\underbrace{L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)- L\\left(P,a^*_{L, P, \\mathcal{F}}  \\right)}_{\\text{misspecification}} $$\n",
    "\n",
    "Note that while the first two terms depend on the sample size $n$, the misspecification term remains constant given that the action space $\\mathcal{A}$ is kept fixed.\n",
    "* If the decision rule $d$ is consistent relative to $(\\mathcal{H}, \\mathcal{A})$, the *bias* converges to zero as $n$ goes to infinty \n",
    "* If the decision rule $d$ is consistent relative to $(\\mathcal{H}, \\mathcal{A})$, the *volatility* converges to zero as $n$ goes to infinty\n",
    "\n",
    "**TOADD?**: stylized figure from the mostafa book (after bias-variance trade-off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of the bias-volatility-misspecification trade-off\n",
    "\n",
    "### 1. Quadratic loss\n",
    "\n",
    "The elements of the problem are\n",
    "\n",
    "* *Observable:* $Z = (Y,X)$\n",
    "* *Loss function:* $L(P, a) = \\int_Z (y - a(x))^2 \\mathrm{d}P(z)$ \n",
    "* *Admissible space:* $\\mathcal{F}\\equiv \\{ a: x \\mapsto y \\ |  \\ a \\ \\text{is measurable}\\}$ \n",
    "* *True feature:* $\\gamma(P) = \\mathbb{E}_P[Y|X] = \\inf_{a\\in \\mathcal{F}} \\ L(P, a)$ \n",
    "\n",
    "Then the minimal loss can be written as \n",
    "\n",
    "$$L(P, a^*_{P, \\mathcal{F}}) = \\int_Z \\underbrace{(y - \\mathbb{E}_P[Y|X](x))^2}_{=\\sigma^2_x \\ \\ \\text{(noise)}} \\mathrm{d}P(z) = \\mathbb{E}_P[\\sigma^2_x]$$\n",
    "\n",
    "the loss evaluated at the best-in-class action, $a^*_{P, \\mathcal{A}}(x)= \\inf_{a\\in\\mathcal{A}} \\ L(P, a)$, is\n",
    "\n",
    "$$L\\left(P, a^*_{P, \\mathcal{A}}\\right) = \\mathbb{E}_P[\\sigma^2_x] + \\int_Z \\underbrace{\\left[\\mathbb{E}_P[Y|X](x) - a^*_{P, \\mathcal{A}}(x)\\right]^2}_{= \\text{misspecification}^2_x} \\mathrm{d}P(z) = L(P,a^*_{P, \\mathcal{F}}) + \\mathbb{E}_P\\left[\\text{misspecification}^2_x\\right]$$\n",
    "\n",
    "and the loss evaluated at the average action $\\bar d_n(x) := \\int_{Z^n} a_{z^n}(x) \\mathrm{d}P(z^n)$ is\n",
    "\n",
    "$$L\\left(P, \\bar d_n\\right) = L\\left(P, a^*_{P, \\mathcal{A}}\\right)  + \\int_Z \\underbrace{\\left[a^*_{P, \\mathcal{A}}(x) - \\bar d_n(x)\\right]^2}_{= \\text{bias}_x^2} \\mathrm{d}P(z) = L\\left(P, a^*_{P, \\mathcal{A}}\\right)  + \\mathbb{E}_P\\left[ \\text{bias}_x^2 \\right]$$\n",
    "\n",
    "Moreover, since $\\frac{\\partial^2 L}{\\partial a^2} = 2$ and $O(\\lambda^3) = 0$, the volatility term is simply\n",
    "\n",
    "$$R_n(P, d) - L(P, \\bar d_n) = \\int_Z \\left[\\int_{Z^n} \\left(a_{z^n}(x) - \\bar d_n(x)\\mathbf{1}(z^n)\\right)^2\\mathrm{d}P(z^n)\\right]\\mathrm{d}P(z)  = \\mathbb{E}_P\\left[\\text{volatility}_x\\right]$$\n",
    "\n",
    "\n",
    "Therefore, the excess risk of a decision rule $d$ under the quadratic loss is \n",
    "\n",
    "$$R_n(P, d) - L(P, a^*_{P, \\mathcal{F}}) = \\mathbb{E}_P\\left[\\text{misspecification}^2_x\\right] + \\mathbb{E}_P\\left[\\text{bias}_x^2\\right] + \\mathbb{E}_P\\left[\\text{volatility}_x\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Relative entropy loss\n",
    "\n",
    "\n",
    "\n",
    "The elements of the problem are\n",
    "\n",
    "* *Observable:* $Z \\sim P$, where $P$ has the density $p$\n",
    "* *Loss function:* $L(P, a) = \\int_Z p(z)\\log \\frac{p}{a}(z) \\mathrm{d}z$ \n",
    "* *Admissible space:* distributions on $Z$ for which density exists. Denote these densities by $a(z)$\n",
    "* *True feature:* $\\gamma(P) = p(z)$ \n",
    "\n",
    "Then the minimal loss is zero and it is reached by $a(z)=p(z)$, i.e. $L(P, a^*_{P, \\mathcal{F}})=0$. \n",
    "\n",
    "The loss evaluated at the best-in-class action $a^*_{P, \\mathcal{A}}(z)= \\inf_{a\\in\\mathcal{A}} \\ L(P, a)$, is\n",
    "\n",
    "$$L\\left(P, a^*_{P, \\mathcal{A}}\\right) = \\int_Z \\underbrace{\\log\\left(\\frac{p}{a^*_{P, \\mathcal{A}}}\\right)(z)}_{= \\text{misspecification}_z} \\mathrm{d}P(z) = \\mathbb{E}_P\\left[\\text{misspecification}_z\\right]$$\n",
    "\n",
    "and the loss evaluated at the average action $\\bar d_n(z) := \\int_{Z^n} a_{z^n}(z) \\mathrm{d}P(z^n)$ is\n",
    "\n",
    "\\begin{align*}\n",
    "L\\left(P, \\bar d_n\\right) &= \\int_Z \\left[\\log\\left(\\frac{p}{a^*_{P, \\mathcal{A}}}\\right)(z) + \\log\\left(\\frac{a^*_{P, \\mathcal{A}}}{\\bar d_n}\\right)(z)\\right] \\mathrm{d}P(z) \\\\\n",
    "&= L\\left(P, a^*_{P, \\mathcal{A}}\\right)  + \\int_Z \\underbrace{\\log\\left(\\frac{a^*_{P, \\mathcal{A}}}{\\bar d_n}\\right)(z)}_{= \\text{bias}_z}\\mathrm{d}P(z) = L\\left(P, a^*_{P, \\mathcal{A}}\\right) + \\mathbb{E}_P\\left[\\text{bias}_z\\right]\n",
    "\\end{align*}\n",
    "\n",
    "Note also that in this case the higher order terms in the Taylor expansion are not zeros. We might approximate the volatility of the decision rule with the second-order term defined above. However, relative entropy loss allows us to use an alternative (exact) measure for the variation in $d$, the so called **Theil's second entropy** (Theil (1967)), which captures all higher order moments of $d$. We can derive it by writing\n",
    "\n",
    "\\begin{align*}\n",
    "R_n(P, d) - L(P, \\bar d_n) &= \\mathbb{E}_P\\left[\\int_{Z^n} \\log \\left(\\frac{p}{d(z^n)}\\right)(z)\\mathrm{d}P(z^n)\\right]  - \\mathbb{E}_P\\left[\\log\\left(\\frac{p}{\\bar d_n}\\right)(z)\\right] \\\\\n",
    "&= \\mathbb{E}_P\\left[ \\underbrace{\\left(\\log \\bar d_n - \\mathbb{E}_{Z^n}[\\log d(z^n)]\\right)(z)}_{= \\nu(d)_z}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "The volatility term indeed captures the variability of $d(z^n)$ (as the sample varies). For example, $\\mathbb{V}[d(z^n)]=0$ implies $\\nu(d) = 0$. Furthermore, note that Theil's second entropy measure of an arbitrary (integrable) random variable $Y$ is\n",
    "\n",
    "$$\\nu(Y) := \\log \\mathbb{E}Y - \\mathbb{E}\\log Y$$\n",
    "\n",
    "This measure was utilized by Alvarez and Jermann (2006) and Backus et al. (2014) in the asset pricing literature. Essentially, it can be considered as a generalization of variance or more precisely, both variance and $\\nu$ are special cases of the general measure of volatiliy \n",
    "\n",
    "$$f(\\mathbb{E}Y) - \\mathbb{E}f(Y), \\quad\\quad\\text{where}\\quad f'' < 0$$\n",
    "\n",
    "The measure $\\nu$ is obtained by setting $f(y) = \\log(y)$, while the variance follows from $f(y)=-y^2$.\n",
    "\n",
    "Therefore, the excess risk of a decision rule $d$ under the relative entropy loss is \n",
    "\n",
    "$$R_n(P, d) - L(P, a^*_{P, \\mathcal{F}}) = \\mathbb{E}_P\\left[\\text{misspecification}_z\\right] + \\mathbb{E}_P\\left[\\text{bias}_z\\right] + \\mathbb{E}_P\\left[\\nu(d)_z\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------_\n",
    "## Classical approach -- the analogy principle\n",
    "\n",
    "\n",
    "The classical approach builds entirely on the empirical loss minimization principle.\n",
    "Briefly, the underlying justification is the *belief* that the decision rule's finite sample distributions converge fast enough (as $n\\to\\infty$) so that we can approximate them well with the limiting distribution.\n",
    "\n",
    "The decision rule itself is obtained by the empirical loss minimization procedure, with the action space being identical irrespective of the sample size. In terms of the general ELM decision rules we can nest this scenario by taking the index sequence to be constant. I.e. $\\{\\mathcal{A}_n\\}_{n\\geq1} \\equiv \\{\\mathcal{A}\\}_{n\\geq1}$ \n",
    "\n",
    "$$ \\mathcal{D}_{ELM} \\left(\\{\\mathcal{A}\\}_{n\\geq1} \\right) := \\left\\{ d: \\mathcal{S} \\mapsto \\mathcal{F} \\ \\left| \\ \\ \\forall z^{\\infty}, \\ \\ d(z^n) := \\inf_{a\\in\\mathcal{A}} \\ L(P_n, a) ,\\ \\ \\mathcal{A}\\subseteq \\mathcal{F}, \\ \\ \\forall n \\geq 1\\right\\}\\right. $$\n",
    "\n",
    "For ease of notation we denote the decision rules obtained via the classical aprroach as $d^C(z^{n})$.\n",
    "\n",
    "As we saw earlier this procedure leads to a consistent decision rule provided the composite loss-action space has a finite complexity measure. The complexity also plays a crucial role in determining the rate of convergence.\n",
    "\n",
    "However, in general there is no formal justification for the finite sample distribution of the decision rule based on the limiting distribution. Typically, the most accurate estimates can be obtained via simulations, like bootstrapping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Examples of decision rules built on the classical approach\n",
    "\n",
    "\n",
    "Maximum likelihood, (non-linear) least squares and GMM can be all considered as special cases. \n",
    "\n",
    "**Example 1.** In the case of regression function estimation take $\\mathcal{F} = Y^X$ and $L(P, \\mu) = \\int_{(Y,X)} (y - \\mu(x))^2P(\\mathrm{d}(y,x))$, then the **non-linear least squares** estimator can be written as\n",
    "\n",
    "$$\\widehat{\\mu}^C(z^n) = \\text{arg}\\inf\\limits_{\\mu \\in \\mathcal{A}}\\hspace{2mm}  \\frac{1}{n}\\sum_{t=1}^n (y_t - \\mu(x_t))^2. $$\n",
    "\n",
    "**Example 2.** In the case of density function estimation, we can take $\\mathcal{F} = \\{f: Z \\mapsto \\mathbb{R}_+ : \\int_Z f(z)\\mathrm{d}z =  1 \\}$ and $L(p, f) = \\int_{Z} p(z)\\log \\frac{p}{f}(z) \\mathrm{d}z$, then the **maximum likelihood** estimator can be written as\n",
    "\n",
    "$$ \\widehat{f}^C(z^{n}) = \\text{arg}\\inf\\limits_{f \\in \\mathcal{A}}\\hspace{2mm} - \\frac{1}{n}\\sum_{t=1}^n \\log f(z_t) + \\underbrace{H(p)}_{\\text{entropy of }p}. $$\n",
    "\n",
    "**Example 3.** In the case of moment restrictions, we can take $\\mathcal{A} = \\{g(\\cdot; \\theta) : \\theta \\in \\Theta\\}$ and $L(P, \\theta) = \\left[\\int g(z; \\theta)\\mathrm{d}P(z)\\right]' W \\left[\\int g(z; \\theta)\\mathrm{d}P(z)\\right]$ with a positive semi-definite $W$. Then the **GMM** estimator is\n",
    "\n",
    "$$ \\widehat{\\theta}_{W}(z^n) = \\text{arg}\\inf\\limits_{\\theta \\in \\mathcal{A}}\\hspace{2mm} \\left[\\frac{1}{n}\\sum_{t=1}^n g(z_t; \\theta)\\right]' W \\left[\\frac{1}{n}\\sum_{t=1}^n g(z_t; \\theta)\\right] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"heuristic justification\" is that in large samples consistent decision rules become invariant implying that the risk simply boils down to its loss function evaluated at the best-in-class action. Good generalization ensures that the empirical loss and the true loss are approximately the same and the shrinking variance ensures that the risk aprroximates the loss.\n",
    "\n",
    "$$ L(P_n, d^C(z^n)) \\approx L(P, d^C(z^n)) \\approx R_n(P, d^C) $$ \n",
    "\n",
    "The main difficulty, of course, is to find the critical value for the \"sufficiently large\" $n$. This critical value naturally depends on the *rate of convergence* which itself hinges on the complexity properties of the DGP and the composite loss-action space.\n",
    "\n",
    "Although the variation of the decision rule vanishes asymptotically, by employing the \"right\" scaling factor corresponding to the rate of convergence, we can derive a non-degenerate asymptotic distribution for $d^C$. Typically the goal is to establish a limiting distribution via the central limit theorem. For example, when the scaling factor is $\\sqrt{n}$ -- frequently encountered in practice -- $(d^C(z^n) - a^*_{\\mathcal{A}})$ must go to zero fast enough in order to ensure the existence of the following limit\n",
    "\n",
    "$$ \\sqrt{n}\\left(d^C(z^n) -  a^*_{\\mathcal{A}}\\right) \\to \\mathcal{N}(0, \\Sigma_L)\\quad\\quad\\text{as}\\quad n\\to \\infty .$$\n",
    "\n",
    "(Note that here we deal with convergence in the action space and not in the loss space.)\n",
    "\n",
    "To capture the variation stemming from the finiteness of the sample, the classical approach proceeds in the spirit of \"backward induction\": first, derive the asymptotic covariance matrix $\\Sigma_L$ of $\\sqrt{n}d^C$ and then scale it appropriately (by $n$) to obtain the finite sample approximation of the variance of $d^C$.\n",
    "\n",
    "As the notation reveals, alternative loss functions can lead to different estimators and corresponding asymptotic covariance matrices.\n",
    "\n",
    "This provides basis to rank decision rules by invoking the criterion of asymptotic efficiency, defined as the smallest possible asymptotic covariance matrix relative to some well-defined class of estimators. (Examples are maximum likelihood and Cramer-Rao lower bound, GMM can be made efficient by choosing the weighting matrix appropriately...) \n",
    "\n",
    "Of course, this type of \"minimization\" of asymptotic covariance matrices does not necessarily imply small finite sample variance of the decision rule. The estimation-approximation error decomposition helps understanding the inherent trade-off.\n",
    "\n",
    "Since the finite sample estimator variance can be linked with the second order term of the Taylor expansion, it might be tempting to view the quest for asymptotic efficiency as a device to minimize the risk of the decision rule. Notice, however, that this method does not take into account the possible *trade-off* between the different moments of the decision rule (i.e. the leading terms in the Taylor expansion of risk). In particular, the classical approach often restricts attention to unbiased estimators and looks for the minimum variance estimator *among this class*. The unbiasedness is gauged only relative to the---restricted---range of the decision rule $\\mathcal{A}$. Hence, even if the decision rule is unbiased we still have misspecification error. Now, it is difficult to defend the merits of an unbiased but misspecified  decision rule with large variance relative to a misspecified decision rule with some bias and smaller variance. \"Clever\" estimators tipically trade-off bias-misspecification and variance flexibly adjusting to the available sample size and the complexity of the estimation problem at hand.\n",
    "\n",
    "Summing up, in case of no misspecification:\n",
    "\n",
    "$$ \\underbrace{L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)- L\\left(P, a^*_{L, P, \\mathcal{F}} \\right)}_{\\text{misspecification}} = 0 $$\n",
    "\n",
    "In case of the decision rule being unbiased:\n",
    "\n",
    "$$ \\underbrace{L\\left(P, \\bar{d_n}\\right) - L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)}_{\\text{bias}} = 0 $$\n",
    "\n",
    "\n",
    "If both the bias and misspecification is zero for each sample size then the excess risk is determined by the volatility term. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example and analysis\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of the example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example reveals the drawbacks of using asymptotic theory to approximate a decision rule's finite sample performance. A potential measure of discrepancy between the finite sample behavior and the asymptotic one is the   **estimation error** defined as\n",
    "\n",
    "$$\\mathcal{E}_d(P, \\mathcal{A}, n) := R_n(P, d) - \\inf_{a\\in\\mathcal{A}} \\ L(P, a).$$\n",
    "\n",
    "While the risk $R_n(P, d)$ captures the performance of $d$ in samples of size $n$, $L(P, a^*_{P, L, \\mathcal{A}})$ essentially encodes its asymptotic properties and from the consistency of $d$ it follows that \n",
    "\n",
    "$$\\lim_{n\\to \\infty} \\ \\mathcal{E}_d(P, \\mathcal{A}, n) \\overset{P}{=} 0.$$\n",
    "\n",
    "In other words, even if $d$ is consistent relative to $(\\mathcal{A}, \\mathcal{H})$, its finite sample behavior still hinges on the range of $d$, i.e. the action space $\\mathcal{A}$.\n",
    "\n",
    "Recall that the origin of estimation error is the fact that we do not know $P$, instead we have to use the information in the (finite) sample to approximate the 'best' action in $\\mathcal{A}$. Intuitively, the smaller the estimation error the better this approximation. Given a decision rule and a finite sample at hand we would like to know how close the empirical loss and the true loss are. Making sure that these two quantities are close to each other ensures that the empirical loss is informative about the true loss. This property is usually referred to as **generalization**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization\n",
    "\n",
    "For a fixed finite sample $z^n$, we say that an assigned action $d(z^n)\\in \\mathcal{A}$ *generalizes well*, if the quantity  \n",
    "\n",
    "$$\\left|L(P, d(z^n)) - L(P_n, d(z^n))\\right|\\quad \\text{is small}.$$\n",
    "\n",
    "Note that this property does *not* require that the empricial loss is itself small, which is the objective function of the classical ELM approach. It only requires that the empirical loss is close to the true loss.\n",
    "\n",
    "This sheds some light on what can go wrong with the ELM approach in finite samples. In practice, one of the worst situations is **overfitting**, that is, when the empirical loss is much smaller than the true loss, hence our assessment of the quality of $d(z^n)\\in\\mathcal{A}$ might be overly optimistic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Roadmap**\n",
    "\n",
    "Note that the generalization property depends on the particular realization of the sample. The realized sample determines the chosen action, $d(z^n)\\in\\mathcal{A}$, and the empirical distribution, $P_n$. In order to give statements regarding the generalization property that extends to more than one paritcular realization of the sample the following steps are taken:\n",
    "\n",
    "* In order to resolve the uncertainty about the chosen action, $d(z^n)$, consider all the actions that are in the range of the decision rule, $\\mathcal{A}$.\n",
    "* In order to resolve the uncertainty about the empirical distribution either\n",
    "    * take expectations or\n",
    "    * characterize where the random variable concentrates via tail bounds.\n",
    "* Give statements which apply uniformly for data generating processes in a given class, $P\\in\\mathcal{H}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolving variation across actions\n",
    "\n",
    "Extending the generalization property to all actions in the range of $d$, $a \\in\\mathcal{A}$, leads to the notion of **generalization error**, defined as\n",
    "\n",
    "$$ \\Delta(P, z^n, \\mathcal{A}) := \\sup_{a\\in\\mathcal{A}} \\ \\left|L(P, a) - L(P_n, a)\\right|.$$\n",
    "\n",
    "When the loss functional takes the form $L(P, a) = \\int l(a, z)\\mathrm{d}P(z)$ then the generalization error is the supremum of a scaled empirical process indexed by the function class $\\mathcal{L}_\\mathcal{A}$. The finite sample techniques discussed in {reference asymptotic notebook} prove to be useful to characterize the behavior of $\\Delta(P, z^n, \\mathcal{A})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolving uncertainty about the empirical distribution\n",
    "\n",
    "**Tail bounds**\n",
    "\n",
    "To draw uniform inference about the generalization properties of $d$, we can use probabilistic tail bounds for $\\Delta(P, z^n, \\mathcal{A})$.  One of the key defining element of the tail bounds is the complexity of the class $\\mathcal{L}_\\mathcal{A}$.\n",
    "\n",
    "We can apply {last theorem asymptotic notebook} in the current setting. For uniformly bounded functions $\\lvert l_a \\rvert_{\\infty} < 1 \\ \\forall l_a  \\in\\mathcal{L}_\\mathcal{A}$ if $\\delta - 2 \\mathcal{R}_n(\\mathcal{L}_\\mathcal{A}) > 0$ then\n",
    "\n",
    "$$ P\\Big\\{ \\Vert P_n - P \\Vert_{\\mathcal{L}_\\mathcal{A}} \\geq  \\delta \\Big\\} \\leq 2 \\exp\\Big\\{- n 2 \\big(\\delta - 2 \\mathcal{R}_n(\\mathcal{L}_\\mathcal{A})\\big)^2 \\Big\\}. $$\n",
    "\n",
    "\n",
    "Probabilistically, the lower the complexity of $\\mathcal{L}_\\mathcal{A}$ the better the generalization ability of the decision rule.\n",
    "\n",
    "**ADD:** rewriting concentration bound to hold with high probability and maybe replace the '1' bound factor by 'B'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average generalization error**\n",
    "\n",
    "A somewhat less ambitious approach is to focus on the average generalization error, i.e. \n",
    "\n",
    "$$\\mathbb{E}_{Z^n}\\left[ \\Delta(P, z^n)\\right] = \\int_{Z^n} \\sup_{a\\in\\mathcal{A}} \\ \\left|L(P, a) - L(P_n, a)\\right| \\mathrm{d}P(z^n).$$\n",
    "\n",
    "Naturally, by bounding the tail probabilities of $\\Delta$ we control the mean as well. In fact ,with some technical care---using a symmetrization argument---one can bound the expectation of the generalization error using the Rademacher complexity of the class $\\mathcal{L}_\\mathcal{A}$,\n",
    "\n",
    "$$\\mathbb{E}_{Z^n}\\left[ \\Delta(P, z^n)\\right]\\leq 2\\mathcal{R}_n(\\mathcal{L}_\\mathcal{A}).$$\n",
    "\n",
    "The important message is that in order to control the generalization property of the decision rule we need to limit the complexity of $\\mathcal{L}_\\mathcal{A}$. \n",
    "\n",
    "**Digression -- estimation error and generalization** \n",
    "\n",
    "It turns out that the average generalization error of the ELM decision rule $d^C$ is tightly linked to its estimation error. To see this, consider the following decomposition of the excess loss for a given realization of the sample $z^n$   \n",
    "\n",
    "\\begin{align*}\n",
    "L(P, d^C(z^n)) - L(P, a^*_{\\mathcal{A}}) = & \\Big(L(P, d^C(z^n)) - L(P_n, d^C(z^n))\\Big) \\\\\n",
    "+ &\\underbrace{\\Big(L(P_n, d^C(z^n)) - L(P_n, a^*_{\\mathcal{A}}) \\Big)}_{\\leq 0} + \\Big(L(P_n,  a^*_{\\mathcal{A}}) - L(P,a^*_{\\mathcal{A}}) \\Big)\\quad\\quad  (1)\n",
    "\\end{align*}\n",
    "\n",
    "and take the expectations over $Z^n$ to arrive at \n",
    "\n",
    "$$\\mathcal{E}_{d^C}(P, \\mathcal{A}, n) =  R_n(P, d^C) - L(P, a^*_{\\mathcal{A}}) \\leq \\mathbb{E}_{Z^n}\\Big[\\sup_{a\\in\\mathcal{A}}\\{L(P, a) - L(P_n, a)\\} \\Big] =  \\mathbb{E}_{Z^n}\\left[ \\Delta(P, z^n)\\right]$$\n",
    "\n",
    "* The second term on the RHS is nonpositive, because the decision rule is based on ELM, so  $L(P_n, d(z^n)) \\leq L(P_n, a) \\ \\forall a\\in\\mathcal{A}$. \n",
    "* The last term disappears when we take the expectation, as we assumed the form $L(P,a) = \\int_Z l(a, z)\\mathrm{d}P(z)$ of the loss function and $\\mathbb{E}_{Z^n}[P_n]=P$.\n",
    "\n",
    "This suggests that by seeking good generalization performance of the ELM estimator the statistician can efficiently control the estimation error as well, thus making sure that the asymptotic analysis provides a relatively good approximation of the finite sample properties of $d^C$.\n",
    "\n",
    "It is easy to see that one could always make the estimation error and generalization error zero by choosing a constant decision rule -- that is, one which range is a singleton and hence assigns the same action to each possible realization of the sample. However, that decision rule would ignore all the information that is available in the data -- the source of information for statistical inference. The approach of statistical learning theory attempts to balance this trade-off.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Learning Theory -- controlling complexity\n",
    "\n",
    "A criticism of the classical approach is that it does not deal with the generalization problem arising in finite samples. Statistical learning theory takes a somewhat different approach and to balance good generalization and low estimation error with small approximation error.   \n",
    "\n",
    "Again, the objective is to minimize the excess risk of the decision rule. As seen earlier the estimation-approximation error decomposition highlights one of the main dilemmas the statistician is facing\n",
    "\n",
    "$$ \\underbrace{R_n(P, d) - L\\left(P, a^{*}_{L, P, \\mathcal{F}} \\right)}_{\\text{excess risk}} =  \\underbrace{R_n(P, d) - L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)}_{\\substack{\\text{estimation error} \\\\ \\text{random}}} + \\underbrace{L\\left(P, a^{*}_{L, P, \\mathcal{A}}\\right)- L\\left(P, a^{*}_{L, P, \\mathcal{F}} \\right)}_{\\substack{\\text{approximation error} \\\\ \\text{deterministic}}}. $$\n",
    "\n",
    "* The approximation error captures the idea that the true feature of the DGP does not lie in the range of the decision rule, hence there is an inherent error due to this misspecification. Correspondingly, ceteris paribus enlarging the action space -- the range of the decision rule -- the approximation error gets smaller. As $\\mathcal{A}$ approaches $\\mathcal{F}$ the approximation error should vanish.\n",
    "\n",
    "* However, the range of the decision rule also plays a key role in the size of the estimation error and its ability to generalize. The non-asymptotic tail bounds teach us that in order to achieve low estimation error and good generalization the complexity of the class $\\mathcal{L}_\\mathcal{A}$ has to be small. The complexity is weakly increasing in the action space, $\\mathcal{A}$.\n",
    "\n",
    "The above trade-off---inherent in all statistical inference problems---can be visualized on the following graph.\n",
    "\n",
    "<img src=\"./decomp.png\", width=600, height=440>\n",
    "\n",
    "In terms of the action space of the decision rule,\n",
    "\n",
    "* whenever the gain from smaller approximation error exceeds the loss from greater estimation error one should increase the action space, there is **underfitting**.\n",
    "* whenever the gain from smaller estimation error exceeds the loss from greater approximation error one should decrease the action space, there is **overfitting**.\n",
    "\n",
    "An ideal decision rule traces the minimum of the U shaped excess risk. By controlling the range of the decision rule, the action space $\\mathcal{A}$, the approach of statistical learning theory can balance the trade-off between the estimation error and the approximation error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling complexity through the action space  \n",
    "\n",
    "Based on the previous discussion on how the size of the action space affects both estimation and approximation errors the modern approaches to statistical inference explicitly control for complexity through the decision rule's action space. This ensures limited generalization and estimation errors in small samples while also reduces misspecification asymptotically.\n",
    "\n",
    "In this sense, we can think of the corresponding decision rule -- $d_{SLT}$ -- in terms of a sequence of action spaces.\n",
    "\n",
    "$$ \\mathcal{D}_{ELM}\\left(\\{\\mathcal{A}_k \\}_{k \\geq 1}\\right) := \\left\\{ d: \\mathcal{S} \\mapsto \\mathcal{F} \\ \\Big| \\ \\ \\forall z^{\\infty}, \\ \\ d(z^n) := \\inf_{a\\in\\mathcal{A}_{k(z^n)}} \\ L(P_n, a) ,\\ \\ \\text{s.t.} \\ \\  \\mathcal{A}_k\\subseteq \\mathcal{A}_{k+1}\\subseteq \\mathcal{F}, \\ \\ \\forall n \\geq 1\\right\\}, $$\n",
    "\n",
    "where $k: \\mathcal{S} \\mapsto \\mathbb{N}$ is a data-dependent subsequence of $k$.\n",
    "\n",
    "This approach, of course, nests the classical frequentist one by setting $\\mathcal{A}_n = \\mathcal{A}\\subseteq \\mathcal{F}$, $\\forall n\\geq 1$.\n",
    "\n",
    "For a given realization of the sample one would like to select the class of actions such that the corresponding empirical loss minimizer achieves almost optimal excess risk. Automating the sepcification of the subsequence---and hence the chosen action space---to depend solely on the realized sample is called model selection. One of the most common form of model selection is the so called penalized empirical loss minimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalized empirical loss minimization\n",
    "\n",
    "**Add Oracle inequalities**\n",
    "\n",
    "Most often one encounters the above procedure in the following form. At each sample size $n$ the constrained optimization problem\n",
    "\n",
    "$$ d_{SLT}(z^n) := \\inf_{a\\in\\mathcal{A}_n} \\ L(P_n, a) $$\n",
    "\n",
    "is recast as an unconstrained optimization problem via the method of Lagrange multipliers,\n",
    "\n",
    "$$ d_{SLT}(z^n) := \\inf_{a\\in\\mathcal{F}} \\ L(P_n, a) + \\Gamma (a, n). $$\n",
    "\n",
    "The general function $\\Gamma$ is the penalty term determining the effective size of the action space -- and hence determining $\\mathcal{A}_n$ in the sequence.\n",
    "\n",
    "Often $\\Gamma(a, n)$ takes the specific form $\\Gamma(a, n) = \\kappa_n C(a)$ where C is a general measure of the complexity of a single action and $\\kappa_n$ is a tuning parameter determined flexibly with the sample size and the quality of the fit -- usuallyt through cross-validation. (**TODO**: here we should probably have a brief digression on how we can jump from the complexity of sets to the complexity of one single action.)\n",
    "\n",
    "By choosing the action space, the loss function and the penalty term many of the well known machine learning techniques can be treated simultaneously in the introduced framework. The lasso and ridge regressions, support vector machines or regularization networks can be seen as particular specifications of the defined objects. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----------------------------------------------------\n",
    "# Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A) Taylor-expansion of the risk functional\n",
    "\n",
    "Remember that the risk of a decision rule $d$ is given by the following expression.\n",
    "\n",
    "$$ R_n(P, d) := \\int\\limits_{Z^n} L(P, d(z^n)) \\ \\mathrm{d} P(z^n) $$\n",
    "\n",
    "Consider the Taylor expansion of this functional with respect to the decision rule around a particular $d$. For any alternative decision rule $\\tilde{d}$, we can define the difference\n",
    "\n",
    "$$\\tilde{d} - d := \\lambda \\eta(z^n)\\quad \\quad \\text{where}\\quad \\eta: \\mathcal{S} \\mapsto \\mathcal{A}, \\quad \\lambda\\in\\mathbb{R}_+$$\n",
    "\n",
    "and then the second-order Taylor expansion of the risk functional around $d$ is\n",
    "\n",
    "$$ R_n\\left(P, \\tilde{d}\\right) = R_n\\left(P, d \\right) + \\int_{Z^n} \\frac{\\partial L(P, d(z^n))}{\\partial a}\\lambda\\eta(z^n)\\mathrm{d} P(z^n) + \\int_{Z^n} \\frac{\\partial^2 L(P, d(z^n))}{\\partial a^2}\\frac{\\lambda^2\\eta(z^n)^2}{2}\\mathrm{d} P(z^n) + O(\\lambda^{3})$$\n",
    "\n",
    "where we use the notion of Gateaux differential (generalization of directional derivate) to obtain the marginal change in the loss function as the abstract $a$ changes. \n",
    "\n",
    "An important reference point of any decision rule $d$ is the expected action that it provides for a given sample size $n$,\n",
    "\n",
    "$$\\bar{d}_n := \\int_{Z^n} d(z^n)\\ \\mathrm{d}P(z^n)$$ \n",
    "\n",
    "which does not necessarily belong to $\\mathcal{A}$. In what follows, we imagine a decision rule $\\bar{d}_n\\mathbf{1}(z^n)$ that assigns the value $\\bar{d}_n$ to all sample realization $z^n$ and use the Taylor approximation around this hypothetical decision rule to approximate the risk of $d$. In this case, $d - \\bar{d}_n\\mathbf{1} := \\lambda \\eta(z^n)$ and \n",
    "\n",
    "$$ R_n\\left(P, d\\right) = L\\left(P, \\bar d_n \\right) + \\int_{Z^n} \\frac{\\partial^2 L(P, \\bar d_n)}{\\partial a^2}\\frac{(d - \\bar{d}_n\\mathbf{1})^2}{2}\\mathrm{d} P(z^n) + O(\\lambda^{3})$$\n",
    "\n",
    "where the first-order term vanishes because the partial -- evaluated at $\\bar d_n\\mathbf{1}$ -- is a constant and $\\int_{Z^n}(d - \\bar d_n\\mathbf{1}) \\mathrm{d}P = 0$. Note that in this expression the second-order term encodes the theoretical variation of the action that $d$ assigns to random samples of size $n$. The regular variance formula is altered by (one half of) the second derivative of the loss function (evaluated at $\\bar d_n$), representing the role of the loss functions's curvature in determining the decision rule's volatility. As a result, a reasonable measure for the decision rule's volatility can be defined as $R_n\\left(P, d\\right) - L\\left(P, \\bar d_n \\right)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B) Bias-variance-misspecification decomposition of GMM\n",
    "\n",
    "The elements of the problem are\n",
    "\n",
    "* *Observable:* $Z \\sim P$, with given moment conditions $g: Z \\times \\mathbb{R}^{p+m} \\mapsto \\mathbb{R}^m$ \n",
    "* *Action space:* $\\mathcal{A} = \\Theta \\subseteq \\mathbb{R}^p$\n",
    "* *Admissible space:* $\\mathcal{F} = \\Theta'\\equiv \\Theta \\times \\mathbb{R}^m$, so that we can always set the expectation of $g$ equal to zero by means of the $m$ auxiliary parameters.  \n",
    "* *Loss function:* $L(P, a) = \\left(\\int_Z g(z, a) \\mathrm{d}P(z)\\right)'W\\left(\\int_Z g(z, a) \\mathrm{d}P(z)\\right)$\n",
    "\n",
    "Then the minimal loss is zero (by construction), i.e. $L(P, a^{*}_{P, \\mathcal{F}}) = 0$. \n",
    "\n",
    "The loss evaluated at the best-in-class action $a^*_{P, \\mathcal{A}} = \\inf_{a\\in\\mathcal{A}} \\ L(P, a)$, is\n",
    "\n",
    "$$L\\left(P, a^*_{P, \\mathcal{A}}\\right) = \\mathbb{E}_P\\left[ g\\left(z, a^{*}_{P, \\mathcal{A}}\\right) \\right]' W \\mathbb{E}_P\\left[ g\\left(z, a^{*}_{P, \\mathcal{A}}\\right) \\right] = \\text{misspecification}$$\n",
    "\n",
    "For the bias term we substract this quantity from the loss evaluated at the average action $\\bar d_n(z) := \\int_{Z^n} a_{z^n} \\mathrm{d}P(z^n)$ \n",
    "\n",
    "$$L\\left(P, \\bar d_n\\right) - L\\left(P, a^*_{P, \\mathcal{A}}\\right) = \\mathbb{E}_P\\left[ g\\left(z, \\bar d_n \\right) \\right]' W \\mathbb{E}_P\\left[ g\\left(z, \\bar d_n \\right) \\right]  - \\mathbb{E}_P\\left[ g\\left(z, a^{*}_{P, \\mathcal{A}}\\right) \\right]' W \\mathbb{E}_P\\left[ g\\left(z, a^{*}_{P, \\mathcal{A}}\\right) \\right]  = \\text{bias}$$\n",
    "\n",
    "We approximate the volatility term with the second-order term of the Taylor expansion. For simplicity, make use of the following notation \n",
    "\n",
    "$$D(a) := \\mathbb{E}_P\\left[ \\frac{\\partial g(z, a)}{\\partial a}\\right] \\in \\mathbb{R}^{m\\times p} \\quad\\quad H(a) := \\mathbb{E}_P\\left[ \\frac{\\partial^2 g(z, a)}{\\partial a^2}\\right] \\in \\mathbb{R}^{p\\times p\\times m}$$\n",
    "\n",
    "and so \n",
    "\n",
    "$$\\frac{\\partial L(P, a)}{\\partial a} = 2 D(a)' W \\mathbb{E}_P\\left[ g(z, a)\\right]\\in \\mathbb{R}^{p} \\quad \\quad \\frac{\\partial^2 L(P, a)}{\\partial a^2} = 2 H(a) W \\mathbb{E}_P\\left[ g(z, a)\\right] + 2 D(a)' W D(a) \\in \\mathbb{R}^{p\\times p}$$\n",
    "\n",
    "implying the approximation \n",
    "\n",
    "$$R_n(P, d) - L(P, \\bar d_n)\\approx \\int_{Z^n} (d(z^n) - \\bar d_n \\mathbf{1}(z^n))'\\left[ \\underbrace{H(\\bar d_n) W  g(z, \\bar d_n)}_{\\to 0 \\ \\text{as} \\ n\\to \\infty} + D(\\bar d_n)' W D(\\bar d_n)\\right](d(z^n) - \\bar d_n \\mathbf{1}(z^n)) \\mathrm{d}P(z^n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
